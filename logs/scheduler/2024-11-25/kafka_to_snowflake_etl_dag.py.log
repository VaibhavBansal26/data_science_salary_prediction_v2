[2024-11-25T15:40:12.641+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:40:12.641+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 21)
[2024-11-25T15:40:12.642+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:40:12.642+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=21
[2024-11-25T15:40:12.644+0000] {processor.py:153} INFO - Started process (PID=21) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:40:12.644+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T15:40:12.645+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:40:12.645+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:40:12.647+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:40:12.647+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:40:13.722+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:40:13.722+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T15:40:14.157+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:40:14.157+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T15:40:14.448+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T15:40:14.451+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:40:14.451+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T15:40:14.476+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:40:14.475+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T15:40:14.476+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:40:14.485+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:40:14.485+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T15:40:14.485+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:40:14.485+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T15:40:14.495+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:40:14.495+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T15:40:14.513+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:40:14.513+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-24T15:40:14.512267+00:00, run_after=2024-11-25T15:40:14.512267+00:00
[2024-11-25T15:40:14.534+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 1.893 seconds
[2024-11-25T15:40:45.198+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:40:45.197+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 27)
[2024-11-25T15:40:45.198+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:40:45.198+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=27
[2024-11-25T15:40:45.202+0000] {processor.py:153} INFO - Started process (PID=27) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:40:45.204+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T15:40:45.210+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:40:45.209+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:40:45.213+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:40:45.213+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:40:45.736+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:40:45.736+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T15:40:45.737+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:40:45.737+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T15:40:46.028+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T15:40:46.029+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:40:46.029+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T15:40:46.033+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:40:46.033+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T15:40:46.033+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:40:46.039+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:40:46.039+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T15:40:46.039+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:40:46.039+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T15:40:46.051+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:40:46.051+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T15:40:46.061+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:40:46.061+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T15:40:46.061+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:40:46.061+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T15:40:46.071+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:40:46.071+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-24T15:40:46.070745+00:00, run_after=2024-11-25T15:40:46.070745+00:00
[2024-11-25T15:40:46.078+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.880 seconds
[2024-11-25T15:41:16.607+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:41:16.607+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 33)
[2024-11-25T15:41:16.608+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:41:16.608+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=33
[2024-11-25T15:41:16.612+0000] {processor.py:153} INFO - Started process (PID=33) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:41:16.612+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T15:41:16.613+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:41:16.613+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:41:16.615+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:41:16.615+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:41:16.993+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:41:16.993+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T15:41:16.994+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:41:16.994+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T15:41:17.254+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T15:41:17.256+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:41:17.256+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T15:41:17.259+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:41:17.259+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T15:41:17.260+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:41:17.266+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:41:17.266+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T15:41:17.266+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:41:17.266+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T15:41:17.276+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:41:17.276+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T15:41:17.282+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:41:17.282+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T15:41:17.282+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:41:17.282+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T15:41:17.291+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:41:17.291+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-24T15:41:17.291139+00:00, run_after=2024-11-25T15:41:17.291139+00:00
[2024-11-25T15:41:17.299+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.691 seconds
[2024-11-25T15:41:47.841+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:41:47.841+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 39)
[2024-11-25T15:41:47.842+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:41:47.842+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=39
[2024-11-25T15:41:47.846+0000] {processor.py:153} INFO - Started process (PID=39) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:41:47.846+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T15:41:47.847+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:41:47.847+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:41:47.852+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:41:47.852+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:41:48.312+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:41:48.312+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T15:41:48.316+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:41:48.314+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T15:41:48.653+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T15:41:48.656+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:41:48.656+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T15:41:48.660+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:41:48.660+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T15:41:48.660+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:41:48.667+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:41:48.667+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T15:41:48.667+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:41:48.667+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T15:41:48.694+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:41:48.694+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T15:41:48.702+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:41:48.702+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T15:41:48.702+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:41:48.702+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T15:41:48.716+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:41:48.716+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-24T15:41:48.716416+00:00, run_after=2024-11-25T15:41:48.716416+00:00
[2024-11-25T15:41:48.725+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.885 seconds
[2024-11-25T15:42:19.224+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:42:19.223+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 45)
[2024-11-25T15:42:19.225+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:42:19.225+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=45
[2024-11-25T15:42:19.228+0000] {processor.py:153} INFO - Started process (PID=45) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:42:19.228+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T15:42:19.229+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:42:19.229+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:42:19.231+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:42:19.231+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:42:19.621+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:42:19.621+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T15:42:19.622+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:42:19.621+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T15:42:19.930+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T15:42:19.933+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:42:19.933+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T15:42:19.940+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:42:19.940+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T15:42:19.940+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:42:19.948+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:42:19.948+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T15:42:19.948+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:42:19.948+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T15:42:19.963+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:42:19.963+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T15:42:19.983+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:42:19.983+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T15:42:19.984+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:42:19.983+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T15:42:20.007+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:42:20.007+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-24T15:42:20.007326+00:00, run_after=2024-11-25T15:42:20.007326+00:00
[2024-11-25T15:42:20.030+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.807 seconds
[2024-11-25T15:42:50.517+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:42:50.516+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 51)
[2024-11-25T15:42:50.518+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:42:50.518+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=51
[2024-11-25T15:42:50.522+0000] {processor.py:153} INFO - Started process (PID=51) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:42:50.523+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T15:42:50.524+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:42:50.524+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:42:50.526+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:42:50.526+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:42:50.831+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:42:50.831+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T15:42:50.832+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:42:50.832+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T15:42:51.058+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T15:42:51.063+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:42:51.063+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T15:42:51.075+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:42:51.075+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T15:42:51.076+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:42:51.081+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:42:51.081+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T15:42:51.081+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:42:51.081+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T15:42:51.089+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:42:51.089+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T15:42:51.094+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:42:51.094+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T15:42:51.094+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:42:51.094+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T15:42:51.104+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:42:51.104+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-24T15:42:51.104061+00:00, run_after=2024-11-25T15:42:51.104061+00:00
[2024-11-25T15:42:51.112+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.596 seconds
[2024-11-25T15:43:21.542+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:43:21.542+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 57)
[2024-11-25T15:43:21.543+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:43:21.543+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=57
[2024-11-25T15:43:21.549+0000] {processor.py:153} INFO - Started process (PID=57) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:43:21.550+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T15:43:21.551+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:43:21.551+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:43:21.553+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:43:21.553+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:43:21.864+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:43:21.864+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T15:43:21.865+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:43:21.865+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T15:43:22.107+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T15:43:22.108+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:43:22.108+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T15:43:22.111+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:43:22.111+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T15:43:22.112+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:43:22.117+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:43:22.116+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T15:43:22.117+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:43:22.117+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T15:43:22.124+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:43:22.124+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T15:43:22.129+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:43:22.129+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T15:43:22.130+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:43:22.129+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T15:43:22.138+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:43:22.138+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-24T15:43:22.138573+00:00, run_after=2024-11-25T15:43:22.138573+00:00
[2024-11-25T15:43:22.144+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.603 seconds
[2024-11-25T15:43:52.427+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:43:52.427+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 63)
[2024-11-25T15:43:52.428+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:43:52.428+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=63
[2024-11-25T15:43:52.432+0000] {processor.py:153} INFO - Started process (PID=63) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:43:52.432+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T15:43:52.433+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:43:52.433+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:43:52.435+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:43:52.435+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:43:52.745+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:43:52.745+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T15:43:52.746+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:43:52.745+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T15:43:52.988+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T15:43:52.990+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:43:52.990+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T15:43:52.993+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:43:52.993+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T15:43:52.993+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:43:52.998+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:43:52.998+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T15:43:52.999+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:43:52.999+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T15:43:53.006+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:43:53.006+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T15:43:53.011+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:43:53.011+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T15:43:53.012+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:43:53.011+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T15:43:53.021+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:43:53.021+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-24T15:43:53.021016+00:00, run_after=2024-11-25T15:43:53.021016+00:00
[2024-11-25T15:43:53.028+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.601 seconds
[2024-11-25T15:44:23.400+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:44:23.400+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 69)
[2024-11-25T15:44:23.401+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:44:23.401+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=69
[2024-11-25T15:44:23.405+0000] {processor.py:153} INFO - Started process (PID=69) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:44:23.405+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T15:44:23.406+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:44:23.406+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:44:23.407+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:44:23.407+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:44:23.723+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:44:23.723+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T15:44:23.724+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:44:23.724+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T15:44:23.969+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T15:44:23.970+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:44:23.970+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T15:44:23.973+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:44:23.973+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T15:44:23.974+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:44:23.980+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:44:23.980+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T15:44:23.980+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:44:23.980+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T15:44:23.989+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:44:23.989+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T15:44:23.994+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:44:23.994+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T15:44:23.995+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:44:23.995+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T15:44:24.003+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:44:24.003+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-24T15:44:24.003320+00:00, run_after=2024-11-25T15:44:24.003320+00:00
[2024-11-25T15:44:24.010+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.610 seconds
[2024-11-25T15:44:54.062+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:44:54.062+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 75)
[2024-11-25T15:44:54.063+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:44:54.063+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=75
[2024-11-25T15:44:54.066+0000] {processor.py:153} INFO - Started process (PID=75) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:44:54.066+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T15:44:54.067+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:44:54.067+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:44:54.068+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:44:54.068+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:44:54.355+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:44:54.355+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T15:44:54.356+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:44:54.356+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T15:44:54.605+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T15:44:54.607+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:44:54.607+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T15:44:54.610+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:44:54.610+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T15:44:54.610+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:44:54.616+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:44:54.616+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T15:44:54.616+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:44:54.616+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T15:44:54.624+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:44:54.624+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T15:44:54.629+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:44:54.629+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T15:44:54.630+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:44:54.630+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T15:44:54.638+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:44:54.638+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-24T15:44:54.638372+00:00, run_after=2024-11-25T15:44:54.638372+00:00
[2024-11-25T15:44:54.645+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.583 seconds
[2024-11-25T15:45:24.811+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:45:24.811+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 81)
[2024-11-25T15:45:24.812+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:45:24.812+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=81
[2024-11-25T15:45:24.817+0000] {processor.py:153} INFO - Started process (PID=81) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:45:24.818+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T15:45:24.820+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:45:24.819+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:45:24.833+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:45:24.833+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:45:25.156+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:45:25.156+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T15:45:25.157+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:45:25.156+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T15:45:25.416+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T15:45:25.418+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:45:25.418+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T15:45:25.421+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:45:25.421+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T15:45:25.421+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:45:25.426+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:45:25.426+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T15:45:25.426+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:45:25.426+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T15:45:25.437+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:45:25.437+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T15:45:25.443+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:45:25.443+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T15:45:25.443+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:45:25.443+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T15:45:25.488+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:45:25.487+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-24T15:45:25.486184+00:00, run_after=2024-11-25T15:45:25.486184+00:00
[2024-11-25T15:45:25.496+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.686 seconds
[2024-11-25T15:45:55.925+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:45:55.923+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 87)
[2024-11-25T15:45:55.927+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:45:55.927+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=87
[2024-11-25T15:45:55.940+0000] {processor.py:153} INFO - Started process (PID=87) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:45:55.942+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T15:45:55.946+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:45:55.946+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:45:55.948+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:45:55.948+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:45:56.254+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:45:56.253+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T15:45:56.255+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:45:56.254+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T15:45:56.492+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T15:45:56.493+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:45:56.493+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T15:45:56.497+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:45:56.497+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T15:45:56.497+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:45:56.502+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:45:56.502+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T15:45:56.503+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:45:56.503+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T15:45:56.512+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:45:56.512+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T15:45:56.518+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:45:56.518+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T15:45:56.518+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:45:56.518+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T15:45:56.527+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:45:56.527+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-24T15:45:56.527282+00:00, run_after=2024-11-25T15:45:56.527282+00:00
[2024-11-25T15:45:56.533+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.610 seconds
[2024-11-25T15:46:26.894+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:46:26.893+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 93)
[2024-11-25T15:46:26.894+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:46:26.894+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=93
[2024-11-25T15:46:26.897+0000] {processor.py:153} INFO - Started process (PID=93) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:46:26.898+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T15:46:26.898+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:46:26.898+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:46:26.899+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:46:26.899+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:46:27.199+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:46:27.199+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T15:46:27.200+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:46:27.200+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T15:46:27.462+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T15:46:27.463+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:46:27.463+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T15:46:27.467+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:46:27.467+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T15:46:27.467+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:46:27.472+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:46:27.472+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T15:46:27.472+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:46:27.472+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T15:46:27.491+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:46:27.491+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T15:46:27.500+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:46:27.499+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T15:46:27.500+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:46:27.500+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T15:46:27.509+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:46:27.509+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-24T15:46:27.509040+00:00, run_after=2024-11-25T15:46:27.509040+00:00
[2024-11-25T15:46:27.516+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.623 seconds
[2024-11-25T15:46:57.999+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:46:57.999+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 99)
[2024-11-25T15:46:58.000+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:46:58.000+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=99
[2024-11-25T15:46:58.005+0000] {processor.py:153} INFO - Started process (PID=99) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:46:58.005+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T15:46:58.007+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:46:58.006+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:46:58.008+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:46:58.008+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:46:58.326+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:46:58.326+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T15:46:58.327+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:46:58.326+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T15:46:58.560+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T15:46:58.561+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:46:58.561+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T15:46:58.565+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:46:58.564+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T15:46:58.565+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:46:58.570+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:46:58.570+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T15:46:58.570+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:46:58.570+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T15:46:58.581+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:46:58.581+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T15:46:58.588+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:46:58.588+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T15:46:58.588+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:46:58.588+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T15:46:58.601+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:46:58.601+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-24T15:46:58.600579+00:00, run_after=2024-11-25T15:46:58.600579+00:00
[2024-11-25T15:46:58.611+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.613 seconds
[2024-11-25T15:47:29.046+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:47:29.046+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 105)
[2024-11-25T15:47:29.047+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:47:29.047+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=105
[2024-11-25T15:47:29.052+0000] {processor.py:153} INFO - Started process (PID=105) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:47:29.052+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T15:47:29.053+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:47:29.053+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:47:29.054+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:47:29.054+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:47:29.475+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:47:29.475+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T15:47:29.476+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:47:29.475+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T15:47:29.721+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T15:47:29.723+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:47:29.723+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T15:47:29.726+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:47:29.726+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T15:47:29.726+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:47:29.732+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:47:29.732+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T15:47:29.732+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:47:29.732+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T15:47:29.740+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:47:29.740+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T15:47:29.745+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:47:29.745+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T15:47:29.746+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:47:29.745+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T15:47:29.754+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:47:29.754+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-24T15:47:29.754537+00:00, run_after=2024-11-25T15:47:29.754537+00:00
[2024-11-25T15:47:29.761+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.715 seconds
[2024-11-25T15:47:59.933+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:47:59.933+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 111)
[2024-11-25T15:47:59.934+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:47:59.934+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=111
[2024-11-25T15:47:59.940+0000] {processor.py:153} INFO - Started process (PID=111) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:47:59.941+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T15:47:59.942+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:47:59.942+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:47:59.943+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:47:59.943+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:48:00.393+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:48:00.392+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T15:48:00.394+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:48:00.393+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T15:48:00.638+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T15:48:00.640+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:48:00.640+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T15:48:00.643+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:48:00.643+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T15:48:00.643+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:48:00.649+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:48:00.649+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T15:48:00.649+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:48:00.649+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T15:48:00.672+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:48:00.672+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T15:48:00.678+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:48:00.678+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T15:48:00.678+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:48:00.678+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T15:48:00.687+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:48:00.687+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-24T15:48:00.687554+00:00, run_after=2024-11-25T15:48:00.687554+00:00
[2024-11-25T15:48:00.695+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.762 seconds
[2024-11-25T15:48:31.173+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:48:31.172+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 117)
[2024-11-25T15:48:31.176+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:48:31.176+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=117
[2024-11-25T15:48:31.191+0000] {processor.py:153} INFO - Started process (PID=117) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:48:31.192+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T15:48:31.194+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:48:31.194+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:48:31.196+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:48:31.196+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:48:31.500+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:48:31.500+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T15:48:31.501+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:48:31.500+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T15:48:31.735+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T15:48:31.737+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:48:31.737+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T15:48:31.740+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:48:31.740+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T15:48:31.741+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:48:31.745+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:48:31.745+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T15:48:31.746+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:48:31.746+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T15:48:31.755+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:48:31.755+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T15:48:31.760+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:48:31.760+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T15:48:31.761+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:48:31.760+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T15:48:31.770+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:48:31.770+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-24T15:48:31.770311+00:00, run_after=2024-11-25T15:48:31.770311+00:00
[2024-11-25T15:48:31.777+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.605 seconds
[2024-11-25T15:49:01.897+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:49:01.897+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 123)
[2024-11-25T15:49:01.898+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:49:01.898+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=123
[2024-11-25T15:49:01.905+0000] {processor.py:153} INFO - Started process (PID=123) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:49:01.906+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T15:49:01.907+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:49:01.907+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:49:01.909+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:49:01.909+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:49:02.223+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:49:02.223+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T15:49:02.224+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:49:02.223+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T15:49:02.468+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T15:49:02.470+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:49:02.470+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T15:49:02.473+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:49:02.473+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T15:49:02.473+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:49:02.478+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:49:02.478+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T15:49:02.478+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:49:02.478+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T15:49:02.487+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:49:02.487+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T15:49:02.493+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:49:02.493+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T15:49:02.493+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:49:02.493+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T15:49:02.502+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:49:02.502+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-24T15:49:02.502396+00:00, run_after=2024-11-25T15:49:02.502396+00:00
[2024-11-25T15:49:02.508+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.612 seconds
[2024-11-25T15:49:32.577+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:49:32.576+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 129)
[2024-11-25T15:49:32.579+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:49:32.579+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=129
[2024-11-25T15:49:32.585+0000] {processor.py:153} INFO - Started process (PID=129) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:49:32.585+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T15:49:32.586+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:49:32.586+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:49:32.589+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:49:32.588+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:49:32.910+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:49:32.910+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T15:49:32.911+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:49:32.910+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T15:49:33.154+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T15:49:33.155+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:49:33.155+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T15:49:33.159+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:49:33.159+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T15:49:33.159+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:49:33.164+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:49:33.164+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T15:49:33.164+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:49:33.164+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T15:49:33.171+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:49:33.171+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T15:49:33.177+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:49:33.177+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T15:49:33.177+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:49:33.177+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T15:49:33.192+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:49:33.192+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-24T15:49:33.192454+00:00, run_after=2024-11-25T15:49:33.192454+00:00
[2024-11-25T15:49:33.200+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.624 seconds
[2024-11-25T15:50:03.274+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:50:03.273+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 135)
[2024-11-25T15:50:03.274+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:50:03.274+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=135
[2024-11-25T15:50:03.292+0000] {processor.py:153} INFO - Started process (PID=135) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:50:03.292+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T15:50:03.293+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:50:03.293+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:50:03.295+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:50:03.295+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:50:03.631+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:50:03.631+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T15:50:03.632+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:50:03.631+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T15:50:03.881+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T15:50:03.882+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:50:03.882+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T15:50:03.886+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:50:03.886+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T15:50:03.886+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:50:03.891+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:50:03.891+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T15:50:03.892+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:50:03.891+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T15:50:03.902+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:50:03.902+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T15:50:03.917+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:50:03.917+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T15:50:03.917+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:50:03.917+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T15:50:03.926+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:50:03.926+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-24T15:50:03.926771+00:00, run_after=2024-11-25T15:50:03.926771+00:00
[2024-11-25T15:50:03.933+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.660 seconds
[2024-11-25T15:50:34.354+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:50:34.354+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 141)
[2024-11-25T15:50:34.354+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:50:34.354+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=141
[2024-11-25T15:50:34.357+0000] {processor.py:153} INFO - Started process (PID=141) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:50:34.357+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T15:50:34.358+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:50:34.358+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:50:34.359+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:50:34.359+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:50:34.656+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:50:34.655+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T15:50:34.657+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:50:34.656+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T15:50:34.897+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T15:50:34.898+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:50:34.898+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T15:50:34.902+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:50:34.902+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T15:50:34.902+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:50:34.907+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:50:34.907+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T15:50:34.908+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:50:34.908+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T15:50:34.915+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:50:34.915+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T15:50:34.921+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:50:34.921+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T15:50:34.921+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:50:34.921+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T15:50:34.930+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:50:34.930+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-24T15:50:34.930595+00:00, run_after=2024-11-25T15:50:34.930595+00:00
[2024-11-25T15:50:34.937+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.584 seconds
[2024-11-25T15:51:05.494+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:51:05.493+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 147)
[2024-11-25T15:51:05.494+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:51:05.494+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=147
[2024-11-25T15:51:05.498+0000] {processor.py:153} INFO - Started process (PID=147) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:51:05.499+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T15:51:05.500+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:51:05.500+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:51:05.501+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:51:05.501+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:51:05.800+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:51:05.800+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T15:51:05.801+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:51:05.801+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T15:51:06.034+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T15:51:06.035+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:51:06.035+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T15:51:06.039+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:51:06.038+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T15:51:06.039+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:51:06.044+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:51:06.044+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T15:51:06.045+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:51:06.044+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T15:51:06.052+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:51:06.052+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T15:51:06.057+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:51:06.057+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T15:51:06.058+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:51:06.058+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T15:51:06.066+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:51:06.066+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-24T15:51:06.066654+00:00, run_after=2024-11-25T15:51:06.066654+00:00
[2024-11-25T15:51:06.074+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.581 seconds
[2024-11-25T15:51:36.478+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:51:36.478+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 153)
[2024-11-25T15:51:36.479+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:51:36.479+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=153
[2024-11-25T15:51:36.484+0000] {processor.py:153} INFO - Started process (PID=153) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:51:36.484+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T15:51:36.485+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:51:36.485+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:51:36.488+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:51:36.488+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:51:36.804+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:51:36.804+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T15:51:36.805+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:51:36.804+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T15:51:37.057+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T15:51:37.059+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:51:37.059+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T15:51:37.062+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:51:37.062+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T15:51:37.062+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:51:37.068+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:51:37.068+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T15:51:37.068+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:51:37.068+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T15:51:37.075+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:51:37.075+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T15:51:37.081+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:51:37.081+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T15:51:37.081+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:51:37.081+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T15:51:37.090+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:51:37.090+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-24T15:51:37.090078+00:00, run_after=2024-11-25T15:51:37.090078+00:00
[2024-11-25T15:51:37.098+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.620 seconds
[2024-11-25T15:52:07.342+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:52:07.342+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 159)
[2024-11-25T15:52:07.344+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:52:07.343+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=159
[2024-11-25T15:52:07.349+0000] {processor.py:153} INFO - Started process (PID=159) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:52:07.349+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T15:52:07.350+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:52:07.350+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:52:07.352+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:52:07.352+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:52:07.696+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:52:07.696+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T15:52:07.697+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:52:07.697+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T15:52:07.951+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T15:52:07.953+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:52:07.953+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T15:52:07.960+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:52:07.960+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T15:52:07.961+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:52:07.966+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:52:07.966+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T15:52:07.966+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:52:07.966+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T15:52:07.978+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:52:07.978+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T15:52:07.983+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:52:07.983+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T15:52:07.984+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:52:07.984+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T15:52:07.994+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:52:07.994+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-24T15:52:07.994546+00:00, run_after=2024-11-25T15:52:07.994546+00:00
[2024-11-25T15:52:08.001+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.660 seconds
[2024-11-25T15:52:38.375+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:52:38.375+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 165)
[2024-11-25T15:52:38.376+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:52:38.376+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=165
[2024-11-25T15:52:38.379+0000] {processor.py:153} INFO - Started process (PID=165) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:52:38.379+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T15:52:38.380+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:52:38.380+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:52:38.381+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:52:38.381+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:52:38.668+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:52:38.668+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T15:52:38.669+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:52:38.668+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T15:52:38.914+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T15:52:38.916+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:52:38.916+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T15:52:38.920+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:52:38.920+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T15:52:38.920+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:52:38.925+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:52:38.925+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T15:52:38.925+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:52:38.925+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T15:52:38.933+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:52:38.933+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T15:52:38.938+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:52:38.938+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T15:52:38.939+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:52:38.939+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T15:52:38.948+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:52:38.948+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-24T15:52:38.948369+00:00, run_after=2024-11-25T15:52:38.948369+00:00
[2024-11-25T15:52:38.956+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.581 seconds
[2024-11-25T15:53:09.231+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:53:09.230+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 171)
[2024-11-25T15:53:09.232+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:53:09.232+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=171
[2024-11-25T15:53:09.239+0000] {processor.py:153} INFO - Started process (PID=171) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:53:09.240+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T15:53:09.241+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:53:09.241+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:53:09.243+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:53:09.243+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:53:09.586+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:53:09.586+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T15:53:09.587+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:53:09.586+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T15:53:09.874+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T15:53:09.876+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:53:09.876+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T15:53:09.880+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:53:09.879+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T15:53:09.880+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:53:09.885+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:53:09.885+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T15:53:09.886+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:53:09.886+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T15:53:09.915+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:53:09.915+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T15:53:09.922+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:53:09.921+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T15:53:09.922+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:53:09.922+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T15:53:09.933+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:53:09.933+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-24T15:53:09.933007+00:00, run_after=2024-11-25T15:53:09.933007+00:00
[2024-11-25T15:53:09.940+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.710 seconds
[2024-11-25T15:53:40.338+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:53:40.337+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 177)
[2024-11-25T15:53:40.339+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:53:40.339+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=177
[2024-11-25T15:53:40.345+0000] {processor.py:153} INFO - Started process (PID=177) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:53:40.346+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T15:53:40.347+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:53:40.347+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:53:40.348+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:53:40.348+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:53:40.675+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:53:40.674+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T15:53:40.675+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:53:40.675+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T15:53:40.916+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T15:53:40.917+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:53:40.917+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T15:53:40.921+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:53:40.921+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T15:53:40.921+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:53:40.926+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:53:40.926+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T15:53:40.926+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:53:40.926+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T15:53:40.936+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:53:40.936+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T15:53:40.947+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:53:40.947+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T15:53:40.949+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:53:40.949+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T15:53:40.962+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:53:40.962+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-24T15:53:40.962095+00:00, run_after=2024-11-25T15:53:40.962095+00:00
[2024-11-25T15:53:40.968+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.631 seconds
[2024-11-25T15:54:11.153+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:54:11.152+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 183)
[2024-11-25T15:54:11.154+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:54:11.153+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=183
[2024-11-25T15:54:11.159+0000] {processor.py:153} INFO - Started process (PID=183) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:54:11.160+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T15:54:11.161+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:54:11.161+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:54:11.163+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:54:11.163+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:54:11.499+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:54:11.499+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T15:54:11.501+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:54:11.500+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T15:54:11.730+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T15:54:11.731+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:54:11.731+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T15:54:11.734+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:54:11.734+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T15:54:11.735+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:54:11.740+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:54:11.740+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T15:54:11.740+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:54:11.740+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T15:54:11.750+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:54:11.750+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T15:54:11.755+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:54:11.755+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T15:54:11.755+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:54:11.755+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T15:54:11.766+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:54:11.766+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-24T15:54:11.766169+00:00, run_after=2024-11-25T15:54:11.766169+00:00
[2024-11-25T15:54:11.772+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.620 seconds
[2024-11-25T15:54:42.229+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:54:42.229+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 189)
[2024-11-25T15:54:42.230+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:54:42.230+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=189
[2024-11-25T15:54:42.235+0000] {processor.py:153} INFO - Started process (PID=189) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:54:42.236+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T15:54:42.237+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:54:42.237+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:54:42.238+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:54:42.238+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:54:42.582+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:54:42.581+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T15:54:42.583+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:54:42.582+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T15:54:42.858+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T15:54:42.859+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:54:42.859+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T15:54:42.862+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:54:42.862+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T15:54:42.862+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:54:42.868+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:54:42.868+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T15:54:42.868+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:54:42.868+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T15:54:42.878+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:54:42.878+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T15:54:42.884+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:54:42.884+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T15:54:42.884+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:54:42.884+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T15:54:42.893+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:54:42.893+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-24T15:54:42.893219+00:00, run_after=2024-11-25T15:54:42.893219+00:00
[2024-11-25T15:54:42.900+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.671 seconds
[2024-11-25T15:55:13.172+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:55:13.172+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 195)
[2024-11-25T15:55:13.174+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:55:13.174+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=195
[2024-11-25T15:55:13.176+0000] {processor.py:153} INFO - Started process (PID=195) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:55:13.177+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T15:55:13.177+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:55:13.177+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:55:13.178+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:55:13.178+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:55:13.775+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:55:13.774+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T15:55:13.777+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:55:13.775+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T15:55:14.946+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T15:55:14.948+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:55:14.948+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T15:55:14.952+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:55:14.952+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T15:55:14.953+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:55:14.960+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:55:14.960+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T15:55:14.960+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:55:14.960+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T15:55:14.974+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:55:14.973+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T15:55:14.986+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:55:14.986+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T15:55:14.986+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:55:14.986+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T15:55:15.028+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:55:15.027+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T15:55:15.043+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 1.872 seconds
[2024-11-25T15:55:45.653+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:55:45.652+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 206)
[2024-11-25T15:55:45.655+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:55:45.654+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=206
[2024-11-25T15:55:45.662+0000] {processor.py:153} INFO - Started process (PID=206) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:55:45.663+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T15:55:45.664+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:55:45.664+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:55:45.666+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:55:45.666+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:55:46.113+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:55:46.113+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T15:55:46.114+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:55:46.113+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T15:55:46.417+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T15:55:46.419+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:55:46.419+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T15:55:46.424+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:55:46.424+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T15:55:46.425+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:55:46.435+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:55:46.435+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T15:55:46.435+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:55:46.435+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T15:55:46.444+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:55:46.444+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T15:55:46.451+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:55:46.451+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T15:55:46.451+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:55:46.451+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T15:55:46.460+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:55:46.460+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T15:55:46.467+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.815 seconds
[2024-11-25T15:56:17.007+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:56:17.007+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 212)
[2024-11-25T15:56:17.008+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:56:17.008+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=212
[2024-11-25T15:56:17.013+0000] {processor.py:153} INFO - Started process (PID=212) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:56:17.014+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T15:56:17.015+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:56:17.015+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:56:17.017+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:56:17.016+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:56:17.572+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:56:17.572+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T15:56:17.573+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:56:17.572+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T15:56:17.680+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T15:56:17.682+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:56:17.682+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T15:56:17.686+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:56:17.686+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T15:56:17.686+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:56:17.692+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:56:17.692+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T15:56:17.692+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:56:17.692+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T15:56:17.702+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:56:17.702+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T15:56:17.708+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:56:17.708+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T15:56:17.709+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:56:17.708+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T15:56:17.718+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:56:17.718+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T15:56:17.725+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.718 seconds
[2024-11-25T15:56:48.262+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:56:48.259+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 218)
[2024-11-25T15:56:48.267+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:56:48.267+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=218
[2024-11-25T15:56:48.300+0000] {processor.py:153} INFO - Started process (PID=218) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:56:48.305+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T15:56:48.312+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:56:48.312+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:56:48.357+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:56:48.356+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:56:49.063+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:56:49.062+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T15:56:49.064+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:56:49.063+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T15:56:49.328+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T15:56:49.329+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:56:49.329+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T15:56:49.333+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:56:49.332+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T15:56:49.333+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:56:49.338+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:56:49.338+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T15:56:49.338+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:56:49.338+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T15:56:49.346+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:56:49.346+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T15:56:49.353+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:56:49.353+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T15:56:49.354+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:56:49.354+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T15:56:49.365+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:56:49.365+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T15:56:49.372+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 1.114 seconds
[2024-11-25T15:57:19.702+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:57:19.702+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 224)
[2024-11-25T15:57:19.703+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:57:19.703+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=224
[2024-11-25T15:57:19.710+0000] {processor.py:153} INFO - Started process (PID=224) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:57:19.710+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T15:57:19.711+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:57:19.711+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:57:19.713+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:57:19.713+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:57:20.076+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:57:20.076+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T15:57:20.077+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:57:20.076+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T15:57:20.308+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T15:57:20.309+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:57:20.309+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T15:57:20.312+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:57:20.312+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T15:57:20.313+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:57:20.337+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:57:20.337+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T15:57:20.337+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:57:20.337+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T15:57:20.358+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:57:20.358+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T15:57:20.365+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:57:20.365+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T15:57:20.365+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:57:20.365+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T15:57:20.375+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:57:20.375+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T15:57:20.382+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.680 seconds
[2024-11-25T15:57:50.926+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:57:50.926+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 230)
[2024-11-25T15:57:50.927+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:57:50.927+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=230
[2024-11-25T15:57:50.932+0000] {processor.py:153} INFO - Started process (PID=230) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:57:50.933+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T15:57:50.934+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:57:50.934+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:57:50.936+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:57:50.936+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:57:51.288+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:57:51.288+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T15:57:51.289+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:57:51.288+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T15:57:51.551+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T15:57:51.552+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:57:51.552+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T15:57:51.556+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:57:51.556+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T15:57:51.556+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:57:51.562+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:57:51.562+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T15:57:51.562+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:57:51.562+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T15:57:51.570+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:57:51.570+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T15:57:51.575+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:57:51.575+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T15:57:51.576+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:57:51.576+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T15:57:51.586+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:57:51.586+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T15:57:51.593+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.667 seconds
[2024-11-25T15:58:21.966+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:58:21.966+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 236)
[2024-11-25T15:58:21.966+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:58:21.966+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=236
[2024-11-25T15:58:21.968+0000] {processor.py:153} INFO - Started process (PID=236) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:58:21.969+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T15:58:21.969+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:58:21.969+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:58:21.970+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:58:21.970+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:58:22.345+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:58:22.345+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T15:58:22.346+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:58:22.346+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T15:58:22.754+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T15:58:22.756+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:58:22.756+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T15:58:22.760+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:58:22.760+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T15:58:22.761+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:58:22.767+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:58:22.767+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T15:58:22.768+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:58:22.768+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T15:58:22.783+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:58:22.783+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T15:58:22.791+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:58:22.791+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T15:58:22.792+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:58:22.791+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T15:58:22.802+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:58:22.802+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T15:58:22.811+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.845 seconds
[2024-11-25T15:58:53.221+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:58:53.220+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 242)
[2024-11-25T15:58:53.222+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:58:53.222+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=242
[2024-11-25T15:58:53.227+0000] {processor.py:153} INFO - Started process (PID=242) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:58:53.228+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T15:58:53.229+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:58:53.229+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:58:53.231+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:58:53.231+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:58:53.579+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:58:53.579+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T15:58:53.580+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:58:53.580+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T15:58:53.834+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T15:58:53.835+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:58:53.835+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T15:58:53.841+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:58:53.841+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T15:58:53.841+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:58:53.847+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:58:53.847+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T15:58:53.848+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:58:53.848+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T15:58:53.855+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:58:53.855+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T15:58:53.861+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:58:53.861+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T15:58:53.861+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:58:53.861+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T15:58:53.871+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:58:53.870+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T15:58:53.877+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.657 seconds
[2024-11-25T15:59:24.438+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:59:24.437+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 248)
[2024-11-25T15:59:24.440+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:59:24.439+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=248
[2024-11-25T15:59:24.446+0000] {processor.py:153} INFO - Started process (PID=248) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:59:24.447+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T15:59:24.448+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:59:24.448+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:59:24.449+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:59:24.449+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:59:24.793+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:59:24.793+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T15:59:24.794+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:59:24.794+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T15:59:25.056+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T15:59:25.058+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:59:25.058+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T15:59:25.061+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:59:25.061+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T15:59:25.062+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:59:25.068+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:59:25.068+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T15:59:25.068+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:59:25.068+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T15:59:25.078+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:59:25.078+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T15:59:25.084+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:59:25.084+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T15:59:25.084+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:59:25.084+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T15:59:25.096+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:59:25.095+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T15:59:25.102+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.665 seconds
[2024-11-25T15:59:55.533+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:59:55.533+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 254)
[2024-11-25T15:59:55.535+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:59:55.535+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=254
[2024-11-25T15:59:55.540+0000] {processor.py:153} INFO - Started process (PID=254) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:59:55.540+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T15:59:55.541+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:59:55.541+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:59:55.543+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:59:55.543+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:59:55.888+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:59:55.888+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T15:59:55.889+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:59:55.889+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T15:59:56.157+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T15:59:56.158+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:59:56.158+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T15:59:56.162+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:59:56.162+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T15:59:56.162+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T15:59:56.167+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:59:56.167+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T15:59:56.168+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:59:56.168+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T15:59:56.176+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:59:56.176+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T15:59:56.182+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:59:56.182+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T15:59:56.182+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:59:56.182+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T15:59:56.191+0000] {logging_mixin.py:137} INFO - [2024-11-25T15:59:56.191+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T15:59:56.197+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.664 seconds
[2024-11-25T16:00:26.629+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:00:26.628+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 260)
[2024-11-25T16:00:26.631+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:00:26.630+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=260
[2024-11-25T16:00:26.638+0000] {processor.py:153} INFO - Started process (PID=260) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:00:26.639+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:00:26.640+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:00:26.640+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:00:26.642+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:00:26.642+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:00:26.991+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:00:26.991+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:00:26.992+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:00:26.991+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:00:27.264+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:00:27.265+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:00:27.265+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:00:27.269+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:00:27.269+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:00:27.269+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:00:27.274+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:00:27.274+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:00:27.274+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:00:27.274+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:00:27.283+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:00:27.283+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:00:27.289+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:00:27.289+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:00:27.290+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:00:27.289+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:00:27.298+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:00:27.298+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:00:27.305+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.677 seconds
[2024-11-25T16:00:57.649+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:00:57.648+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 271)
[2024-11-25T16:00:57.650+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:00:57.650+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=271
[2024-11-25T16:00:57.658+0000] {processor.py:153} INFO - Started process (PID=271) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:00:57.659+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:00:57.659+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:00:57.659+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:00:57.661+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:00:57.661+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:00:58.011+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:00:58.011+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:00:58.013+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:00:58.012+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:00:58.264+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:00:58.266+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:00:58.266+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:00:58.269+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:00:58.269+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:00:58.269+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:00:58.274+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:00:58.274+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:00:58.274+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:00:58.274+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:00:58.287+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:00:58.287+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:00:58.293+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:00:58.293+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:00:58.293+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:00:58.293+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:00:58.302+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:00:58.302+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:00:58.309+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.661 seconds
[2024-11-25T16:01:28.828+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:01:28.827+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 277)
[2024-11-25T16:01:28.829+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:01:28.829+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=277
[2024-11-25T16:01:28.848+0000] {processor.py:153} INFO - Started process (PID=277) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:01:28.849+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:01:28.850+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:01:28.850+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:01:28.852+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:01:28.852+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:01:29.207+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:01:29.207+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:01:29.208+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:01:29.207+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:01:29.458+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:01:29.459+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:01:29.459+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:01:29.463+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:01:29.463+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:01:29.463+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:01:29.469+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:01:29.469+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:01:29.469+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:01:29.469+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:01:29.480+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:01:29.479+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:01:29.488+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:01:29.488+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:01:29.489+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:01:29.489+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:01:29.499+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:01:29.499+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:01:29.506+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.679 seconds
[2024-11-25T16:02:00.313+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:02:00.312+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 283)
[2024-11-25T16:02:00.314+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:02:00.314+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=283
[2024-11-25T16:02:00.322+0000] {processor.py:153} INFO - Started process (PID=283) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:02:00.322+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:02:00.323+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:02:00.323+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:02:00.325+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:02:00.325+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:02:00.702+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:02:00.702+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:02:00.703+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:02:00.702+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:02:00.952+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:02:00.954+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:02:00.954+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:02:00.957+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:02:00.957+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:02:00.957+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:02:00.963+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:02:00.963+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:02:00.963+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:02:00.963+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:02:00.973+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:02:00.973+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:02:00.979+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:02:00.979+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:02:00.979+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:02:00.979+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:02:00.988+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:02:00.988+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:02:00.995+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.674 seconds
[2024-11-25T16:02:31.462+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:02:31.461+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 289)
[2024-11-25T16:02:31.463+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:02:31.463+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=289
[2024-11-25T16:02:31.472+0000] {processor.py:153} INFO - Started process (PID=289) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:02:31.473+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:02:31.474+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:02:31.474+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:02:31.476+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:02:31.476+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:02:31.829+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:02:31.828+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:02:31.830+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:02:31.829+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:02:32.078+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:02:32.080+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:02:32.080+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:02:32.084+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:02:32.083+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:02:32.084+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:02:32.089+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:02:32.089+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:02:32.089+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:02:32.089+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:02:32.097+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:02:32.097+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:02:32.103+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:02:32.103+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:02:32.104+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:02:32.103+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:02:32.115+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:02:32.115+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:02:32.123+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.661 seconds
[2024-11-25T16:03:02.497+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:03:02.497+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 295)
[2024-11-25T16:03:02.497+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:03:02.497+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=295
[2024-11-25T16:03:02.500+0000] {processor.py:153} INFO - Started process (PID=295) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:03:02.501+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:03:02.502+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:03:02.502+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:03:02.504+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:03:02.504+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:03:02.854+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:03:02.854+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:03:02.855+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:03:02.854+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:03:03.123+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:03:03.125+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:03:03.125+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:03:03.130+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:03:03.129+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:03:03.130+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:03:03.138+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:03:03.137+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:03:03.138+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:03:03.138+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:03:03.156+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:03:03.156+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:03:03.164+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:03:03.164+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:03:03.164+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:03:03.164+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:03:03.176+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:03:03.176+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:03:03.183+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.686 seconds
[2024-11-25T16:03:33.567+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:03:33.567+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 301)
[2024-11-25T16:03:33.568+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:03:33.567+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=301
[2024-11-25T16:03:33.570+0000] {processor.py:153} INFO - Started process (PID=301) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:03:33.571+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:03:33.571+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:03:33.571+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:03:33.572+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:03:33.572+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:03:33.963+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:03:33.963+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:03:33.966+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:03:33.963+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:03:34.273+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:03:34.275+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:03:34.274+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:03:34.278+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:03:34.278+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:03:34.278+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:03:34.284+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:03:34.284+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:03:34.284+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:03:34.284+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:03:34.293+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:03:34.293+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:03:34.301+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:03:34.301+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:03:34.302+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:03:34.302+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:03:34.314+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:03:34.314+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:03:34.320+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.754 seconds
[2024-11-25T16:04:04.875+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:04:04.874+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 307)
[2024-11-25T16:04:04.878+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:04:04.878+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=307
[2024-11-25T16:04:04.883+0000] {processor.py:153} INFO - Started process (PID=307) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:04:04.883+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:04:04.884+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:04:04.884+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:04:04.886+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:04:04.886+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:04:05.269+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:04:05.269+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:04:05.270+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:04:05.269+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:04:05.639+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:04:05.641+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:04:05.641+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:04:05.648+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:04:05.648+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:04:05.648+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:04:05.656+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:04:05.656+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:04:05.656+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:04:05.656+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:04:05.674+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:04:05.674+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:04:05.681+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:04:05.681+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:04:05.682+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:04:05.681+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:04:05.692+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:04:05.692+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:04:05.700+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.826 seconds
[2024-11-25T16:04:36.204+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:04:36.204+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 313)
[2024-11-25T16:04:36.205+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:04:36.205+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=313
[2024-11-25T16:04:36.211+0000] {processor.py:153} INFO - Started process (PID=313) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:04:36.212+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:04:36.213+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:04:36.213+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:04:36.215+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:04:36.215+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:04:36.544+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:04:36.544+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:04:36.545+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:04:36.545+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:04:36.798+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:04:36.799+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:04:36.799+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:04:36.803+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:04:36.803+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:04:36.803+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:04:36.809+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:04:36.809+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:04:36.809+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:04:36.809+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:04:36.816+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:04:36.816+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:04:36.822+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:04:36.822+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:04:36.822+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:04:36.822+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:04:36.831+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:04:36.831+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:04:36.838+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.634 seconds
[2024-11-25T16:05:07.292+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:05:07.291+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 319)
[2024-11-25T16:05:07.292+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:05:07.292+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=319
[2024-11-25T16:05:07.295+0000] {processor.py:153} INFO - Started process (PID=319) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:05:07.296+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:05:07.296+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:05:07.296+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:05:07.297+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:05:07.297+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:05:07.640+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:05:07.640+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:05:07.641+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:05:07.640+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:05:07.910+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:05:07.911+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:05:07.911+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:05:07.915+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:05:07.915+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:05:07.915+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:05:07.924+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:05:07.924+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:05:07.924+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:05:07.924+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:05:07.935+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:05:07.935+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:05:07.949+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:05:07.949+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:05:07.949+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:05:07.949+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:05:07.973+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:05:07.972+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:05:07.984+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.693 seconds
[2024-11-25T16:05:38.168+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:05:38.167+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 325)
[2024-11-25T16:05:38.169+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:05:38.169+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=325
[2024-11-25T16:05:38.175+0000] {processor.py:153} INFO - Started process (PID=325) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:05:38.176+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:05:38.177+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:05:38.177+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:05:38.179+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:05:38.179+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:05:38.586+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:05:38.585+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:05:38.587+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:05:38.586+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:05:38.817+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:05:38.818+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:05:38.818+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:05:38.822+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:05:38.822+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:05:38.822+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:05:38.828+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:05:38.828+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:05:38.828+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:05:38.828+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:05:38.838+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:05:38.838+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:05:38.847+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:05:38.847+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:05:38.848+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:05:38.847+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:05:38.859+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:05:38.859+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:05:38.866+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.699 seconds
[2024-11-25T16:06:09.289+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:06:09.289+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 331)
[2024-11-25T16:06:09.290+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:06:09.290+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=331
[2024-11-25T16:06:09.294+0000] {processor.py:153} INFO - Started process (PID=331) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:06:09.295+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:06:09.296+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:06:09.295+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:06:09.297+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:06:09.297+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:06:09.780+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:06:09.780+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:06:09.781+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:06:09.780+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:06:09.884+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:06:09.887+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:06:09.887+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:06:09.893+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:06:09.893+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:06:09.894+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:06:09.899+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:06:09.899+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:06:09.899+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:06:09.899+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:06:09.909+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:06:09.909+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:06:09.917+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:06:09.917+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:06:09.917+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:06:09.917+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:06:09.928+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:06:09.928+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:06:09.943+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.655 seconds
[2024-11-25T16:06:40.391+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:06:40.391+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 337)
[2024-11-25T16:06:40.392+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:06:40.392+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=337
[2024-11-25T16:06:40.412+0000] {processor.py:153} INFO - Started process (PID=337) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:06:40.413+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:06:40.420+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:06:40.420+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:06:40.427+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:06:40.427+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:06:40.987+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:06:40.987+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:06:40.989+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:06:40.988+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:06:41.283+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:06:41.286+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:06:41.285+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:06:41.289+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:06:41.289+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:06:41.289+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:06:41.298+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:06:41.297+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:06:41.298+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:06:41.298+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:06:41.318+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:06:41.317+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:06:41.328+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:06:41.328+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:06:41.329+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:06:41.328+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:06:41.340+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:06:41.340+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:06:41.347+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.956 seconds
[2024-11-25T16:07:12.053+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:07:12.051+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 343)
[2024-11-25T16:07:12.054+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:07:12.054+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=343
[2024-11-25T16:07:12.060+0000] {processor.py:153} INFO - Started process (PID=343) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:07:12.061+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:07:12.061+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:07:12.061+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:07:12.063+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:07:12.063+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:07:12.437+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:07:12.437+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:07:12.438+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:07:12.437+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:07:12.833+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:07:12.836+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:07:12.836+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:07:12.840+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:07:12.840+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:07:12.840+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:07:12.847+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:07:12.847+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:07:12.847+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:07:12.847+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:07:12.859+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:07:12.859+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:07:12.872+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:07:12.872+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:07:12.873+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:07:12.873+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:07:12.890+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:07:12.890+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:07:12.899+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.849 seconds
[2024-11-25T16:07:43.497+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:07:43.496+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 349)
[2024-11-25T16:07:43.497+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:07:43.497+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=349
[2024-11-25T16:07:43.502+0000] {processor.py:153} INFO - Started process (PID=349) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:07:43.503+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:07:43.505+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:07:43.505+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:07:43.507+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:07:43.507+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:07:44.090+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:07:44.090+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:07:44.091+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:07:44.090+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:07:44.347+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:07:44.348+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:07:44.348+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:07:44.352+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:07:44.352+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:07:44.352+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:07:44.357+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:07:44.357+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:07:44.358+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:07:44.358+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:07:44.366+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:07:44.366+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:07:44.374+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:07:44.374+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:07:44.374+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:07:44.374+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:07:44.390+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:07:44.390+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:07:44.402+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.905 seconds
[2024-11-25T16:08:14.878+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:08:14.877+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 355)
[2024-11-25T16:08:14.879+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:08:14.879+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=355
[2024-11-25T16:08:14.883+0000] {processor.py:153} INFO - Started process (PID=355) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:08:14.883+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:08:14.884+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:08:14.884+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:08:14.885+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:08:14.885+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:08:15.224+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:08:15.224+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:08:15.225+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:08:15.224+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:08:15.478+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:08:15.480+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:08:15.480+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:08:15.483+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:08:15.483+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:08:15.483+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:08:15.490+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:08:15.490+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:08:15.490+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:08:15.490+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:08:15.500+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:08:15.500+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:08:15.506+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:08:15.506+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:08:15.506+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:08:15.506+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:08:15.524+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:08:15.524+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:08:15.536+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.660 seconds
[2024-11-25T16:08:46.026+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:08:46.026+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 361)
[2024-11-25T16:08:46.027+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:08:46.027+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=361
[2024-11-25T16:08:46.036+0000] {processor.py:153} INFO - Started process (PID=361) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:08:46.037+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:08:46.038+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:08:46.038+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:08:46.040+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:08:46.040+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:08:46.461+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:08:46.461+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:08:46.462+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:08:46.461+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:08:46.712+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:08:46.714+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:08:46.714+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:08:46.717+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:08:46.717+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:08:46.717+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:08:46.723+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:08:46.723+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:08:46.723+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:08:46.723+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:08:46.730+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:08:46.730+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:08:46.736+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:08:46.736+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:08:46.736+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:08:46.736+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:08:46.746+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:08:46.746+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:08:46.753+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.728 seconds
[2024-11-25T16:09:17.195+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:09:17.194+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 367)
[2024-11-25T16:09:17.196+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:09:17.196+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=367
[2024-11-25T16:09:17.201+0000] {processor.py:153} INFO - Started process (PID=367) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:09:17.202+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:09:17.203+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:09:17.203+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:09:17.205+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:09:17.205+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:09:17.565+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:09:17.565+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:09:17.566+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:09:17.565+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:09:17.830+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:09:17.833+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:09:17.833+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:09:17.841+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:09:17.840+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:09:17.841+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:09:17.854+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:09:17.854+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:09:17.854+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:09:17.854+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:09:17.868+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:09:17.868+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:09:17.882+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:09:17.882+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:09:17.883+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:09:17.882+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:09:17.912+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:09:17.912+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:09:17.926+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.732 seconds
[2024-11-25T16:09:48.169+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:09:48.168+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 373)
[2024-11-25T16:09:48.169+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:09:48.169+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=373
[2024-11-25T16:09:48.175+0000] {processor.py:153} INFO - Started process (PID=373) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:09:48.176+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:09:48.177+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:09:48.177+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:09:48.178+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:09:48.178+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:09:48.689+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:09:48.689+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:09:48.694+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:09:48.693+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:09:48.788+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:09:48.789+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:09:48.789+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:09:48.793+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:09:48.792+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:09:48.793+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:09:48.798+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:09:48.798+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:09:48.798+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:09:48.798+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:09:48.809+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:09:48.809+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:09:48.815+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:09:48.815+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:09:48.815+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:09:48.815+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:09:48.824+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:09:48.824+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:09:48.830+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.662 seconds
[2024-11-25T16:10:19.258+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:10:19.257+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 379)
[2024-11-25T16:10:19.259+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:10:19.259+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=379
[2024-11-25T16:10:19.265+0000] {processor.py:153} INFO - Started process (PID=379) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:10:19.266+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:10:19.267+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:10:19.267+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:10:19.269+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:10:19.269+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:10:19.641+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:10:19.641+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:10:19.642+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:10:19.641+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:10:19.884+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:10:19.886+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:10:19.886+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:10:19.889+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:10:19.889+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:10:19.889+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:10:19.894+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:10:19.894+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:10:19.894+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:10:19.894+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:10:19.923+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:10:19.923+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:10:19.929+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:10:19.929+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:10:19.929+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:10:19.929+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:10:19.938+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:10:19.938+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:10:19.946+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.689 seconds
[2024-11-25T16:10:50.287+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:10:50.287+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 385)
[2024-11-25T16:10:50.288+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:10:50.288+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=385
[2024-11-25T16:10:50.292+0000] {processor.py:153} INFO - Started process (PID=385) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:10:50.293+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:10:50.293+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:10:50.293+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:10:50.295+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:10:50.294+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:10:50.642+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:10:50.642+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:10:50.644+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:10:50.643+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:10:50.878+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:10:50.879+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:10:50.879+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:10:50.883+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:10:50.883+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:10:50.883+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:10:50.888+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:10:50.888+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:10:50.888+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:10:50.888+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:10:50.898+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:10:50.898+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:10:50.906+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:10:50.906+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:10:50.906+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:10:50.906+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:10:50.916+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:10:50.916+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:10:50.923+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.636 seconds
[2024-11-25T16:11:21.343+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:11:21.342+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 391)
[2024-11-25T16:11:21.344+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:11:21.344+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=391
[2024-11-25T16:11:21.350+0000] {processor.py:153} INFO - Started process (PID=391) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:11:21.351+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:11:21.352+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:11:21.352+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:11:21.353+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:11:21.353+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:11:21.724+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:11:21.723+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:11:21.725+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:11:21.724+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:11:21.973+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:11:21.974+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:11:21.974+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:11:21.977+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:11:21.977+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:11:21.978+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:11:21.983+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:11:21.983+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:11:21.983+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:11:21.983+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:11:21.991+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:11:21.991+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:11:21.997+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:11:21.997+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:11:21.997+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:11:21.997+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:11:22.006+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:11:22.006+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:11:22.012+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.670 seconds
[2024-11-25T16:11:52.070+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:11:52.069+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 397)
[2024-11-25T16:11:52.072+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:11:52.072+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=397
[2024-11-25T16:11:52.077+0000] {processor.py:153} INFO - Started process (PID=397) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:11:52.078+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:11:52.079+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:11:52.079+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:11:52.081+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:11:52.081+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:11:52.453+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:11:52.453+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:11:52.454+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:11:52.453+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:11:52.698+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:11:52.699+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:11:52.699+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:11:52.702+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:11:52.702+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:11:52.703+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:11:52.708+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:11:52.708+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:11:52.708+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:11:52.708+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:11:52.719+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:11:52.719+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:11:52.724+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:11:52.724+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:11:52.725+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:11:52.725+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:11:52.750+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:11:52.750+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:11:52.770+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.701 seconds
[2024-11-25T16:12:22.893+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:12:22.893+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 403)
[2024-11-25T16:12:22.894+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:12:22.894+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=403
[2024-11-25T16:12:22.900+0000] {processor.py:153} INFO - Started process (PID=403) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:12:22.901+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:12:22.902+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:12:22.902+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:12:22.904+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:12:22.903+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:12:23.251+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:12:23.251+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:12:23.252+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:12:23.252+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:12:23.502+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:12:23.504+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:12:23.504+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:12:23.511+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:12:23.511+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:12:23.512+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:12:23.519+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:12:23.519+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:12:23.519+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:12:23.519+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:12:23.529+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:12:23.529+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:12:23.534+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:12:23.534+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:12:23.535+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:12:23.535+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:12:23.545+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:12:23.544+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:12:23.556+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.663 seconds
[2024-11-25T16:12:53.663+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:12:53.663+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 409)
[2024-11-25T16:12:53.664+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:12:53.664+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=409
[2024-11-25T16:12:53.668+0000] {processor.py:153} INFO - Started process (PID=409) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:12:53.668+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:12:53.669+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:12:53.669+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:12:53.670+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:12:53.670+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:12:54.272+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:12:54.272+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:12:54.273+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:12:54.273+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:12:54.364+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:12:54.366+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:12:54.366+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:12:54.369+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:12:54.369+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:12:54.370+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:12:54.375+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:12:54.375+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:12:54.375+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:12:54.375+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:12:54.384+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:12:54.384+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:12:54.390+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:12:54.390+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:12:54.390+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:12:54.390+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:12:54.399+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:12:54.399+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:12:54.406+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.743 seconds
[2024-11-25T16:13:24.836+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:13:24.835+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 415)
[2024-11-25T16:13:24.837+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:13:24.837+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=415
[2024-11-25T16:13:24.841+0000] {processor.py:153} INFO - Started process (PID=415) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:13:24.842+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:13:24.843+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:13:24.843+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:13:24.846+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:13:24.846+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:13:25.208+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:13:25.207+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:13:25.209+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:13:25.208+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:13:25.472+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:13:25.474+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:13:25.474+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:13:25.478+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:13:25.478+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:13:25.478+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:13:25.484+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:13:25.484+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:13:25.484+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:13:25.484+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:13:25.493+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:13:25.493+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:13:25.499+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:13:25.499+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:13:25.499+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:13:25.499+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:13:25.509+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:13:25.509+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:13:25.516+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.681 seconds
[2024-11-25T16:13:55.848+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:13:55.847+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 421)
[2024-11-25T16:13:55.849+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:13:55.849+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=421
[2024-11-25T16:13:55.856+0000] {processor.py:153} INFO - Started process (PID=421) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:13:55.857+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:13:55.859+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:13:55.859+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:13:55.861+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:13:55.861+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:13:56.268+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:13:56.267+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:13:56.272+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:13:56.269+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:13:56.511+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:13:56.512+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:13:56.512+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:13:56.516+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:13:56.515+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:13:56.516+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:13:56.521+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:13:56.521+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:13:56.521+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:13:56.521+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:13:56.531+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:13:56.531+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:13:56.537+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:13:56.537+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:13:56.538+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:13:56.537+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:13:56.546+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:13:56.546+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:13:56.553+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.705 seconds
[2024-11-25T16:14:27.054+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:14:27.054+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 427)
[2024-11-25T16:14:27.055+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:14:27.055+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=427
[2024-11-25T16:14:27.061+0000] {processor.py:153} INFO - Started process (PID=427) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:14:27.062+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:14:27.063+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:14:27.063+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:14:27.064+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:14:27.064+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:14:27.418+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:14:27.418+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:14:27.419+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:14:27.419+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:14:27.684+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:14:27.686+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:14:27.686+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:14:27.689+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:14:27.689+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:14:27.690+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:14:27.695+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:14:27.695+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:14:27.695+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:14:27.695+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:14:27.707+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:14:27.707+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:14:27.713+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:14:27.713+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:14:27.713+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:14:27.713+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:14:27.722+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:14:27.722+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:14:27.729+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.676 seconds
[2024-11-25T16:14:58.154+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:14:58.153+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 433)
[2024-11-25T16:14:58.155+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:14:58.155+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=433
[2024-11-25T16:14:58.160+0000] {processor.py:153} INFO - Started process (PID=433) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:14:58.161+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:14:58.162+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:14:58.162+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:14:58.163+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:14:58.163+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:14:58.505+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:14:58.505+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:14:58.507+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:14:58.506+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:14:58.765+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:14:58.766+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:14:58.766+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:14:58.770+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:14:58.769+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:14:58.770+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:14:58.775+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:14:58.775+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:14:58.775+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:14:58.775+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:14:58.785+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:14:58.785+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:14:58.791+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:14:58.791+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:14:58.791+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:14:58.791+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:14:58.800+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:14:58.800+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:14:58.807+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.655 seconds
[2024-11-25T16:15:29.234+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:15:29.233+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 439)
[2024-11-25T16:15:29.235+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:15:29.235+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=439
[2024-11-25T16:15:29.238+0000] {processor.py:153} INFO - Started process (PID=439) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:15:29.238+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:15:29.239+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:15:29.239+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:15:29.240+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:15:29.240+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:15:29.615+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:15:29.615+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:15:29.616+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:15:29.616+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:15:29.861+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:15:29.862+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:15:29.862+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:15:29.866+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:15:29.865+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:15:29.866+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:15:29.871+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:15:29.871+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:15:29.871+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:15:29.871+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:15:29.881+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:15:29.881+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:15:29.891+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:15:29.891+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:15:29.893+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:15:29.892+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:15:29.906+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:15:29.906+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:15:29.914+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.681 seconds
[2024-11-25T16:16:00.474+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:16:00.473+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 445)
[2024-11-25T16:16:00.475+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:16:00.475+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=445
[2024-11-25T16:16:00.482+0000] {processor.py:153} INFO - Started process (PID=445) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:16:00.483+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:16:00.485+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:16:00.485+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:16:00.486+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:16:00.486+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:16:00.897+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:16:00.897+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:16:00.898+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:16:00.898+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:16:01.153+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:16:01.154+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:16:01.154+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:16:01.158+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:16:01.158+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:16:01.158+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:16:01.163+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:16:01.163+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:16:01.163+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:16:01.163+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:16:01.170+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:16:01.170+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:16:01.176+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:16:01.175+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:16:01.176+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:16:01.176+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:16:01.185+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:16:01.185+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:16:01.192+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.715 seconds
[2024-11-25T16:16:31.654+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:16:31.653+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 451)
[2024-11-25T16:16:31.656+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:16:31.655+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=451
[2024-11-25T16:16:31.661+0000] {processor.py:153} INFO - Started process (PID=451) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:16:31.662+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:16:31.663+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:16:31.663+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:16:31.665+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:16:31.665+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:16:32.196+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:16:32.196+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:16:32.197+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:16:32.197+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:16:32.291+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:16:32.293+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:16:32.293+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:16:32.296+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:16:32.296+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:16:32.296+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:16:32.301+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:16:32.301+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:16:32.301+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:16:32.301+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:16:32.309+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:16:32.309+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:16:32.314+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:16:32.314+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:16:32.315+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:16:32.314+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:16:32.323+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:16:32.323+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:16:32.330+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.677 seconds
[2024-11-25T16:17:02.639+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:17:02.639+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 457)
[2024-11-25T16:17:02.640+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:17:02.640+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=457
[2024-11-25T16:17:02.643+0000] {processor.py:153} INFO - Started process (PID=457) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:17:02.643+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:17:02.644+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:17:02.644+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:17:02.645+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:17:02.645+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:17:03.156+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:17:03.155+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:17:03.156+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:17:03.156+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:17:03.250+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:17:03.255+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:17:03.255+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:17:03.264+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:17:03.263+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:17:03.264+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:17:03.269+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:17:03.269+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:17:03.270+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:17:03.269+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:17:03.280+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:17:03.280+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:17:03.287+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:17:03.286+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:17:03.287+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:17:03.287+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:17:03.296+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:17:03.296+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:17:03.303+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.663 seconds
[2024-11-25T16:17:33.359+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:17:33.358+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 463)
[2024-11-25T16:17:33.360+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:17:33.360+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=463
[2024-11-25T16:17:33.365+0000] {processor.py:153} INFO - Started process (PID=463) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:17:33.365+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:17:33.367+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:17:33.366+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:17:33.368+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:17:33.368+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:17:33.716+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:17:33.716+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:17:33.717+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:17:33.717+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:17:33.951+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:17:33.957+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:17:33.957+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:17:33.963+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:17:33.963+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:17:33.963+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:17:33.969+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:17:33.969+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:17:33.969+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:17:33.969+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:17:33.978+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:17:33.978+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:17:33.984+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:17:33.984+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:17:33.984+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:17:33.984+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:17:33.993+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:17:33.993+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:17:33.999+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.641 seconds
[2024-11-25T16:18:04.321+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:18:04.320+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 469)
[2024-11-25T16:18:04.321+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:18:04.321+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=469
[2024-11-25T16:18:04.326+0000] {processor.py:153} INFO - Started process (PID=469) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:18:04.326+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:18:04.327+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:18:04.327+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:18:04.329+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:18:04.329+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:18:04.666+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:18:04.666+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:18:04.667+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:18:04.666+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:18:04.917+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:18:04.918+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:18:04.918+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:18:04.921+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:18:04.921+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:18:04.922+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:18:04.927+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:18:04.927+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:18:04.927+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:18:04.927+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:18:04.935+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:18:04.935+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:18:04.941+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:18:04.941+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:18:04.941+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:18:04.941+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:18:04.950+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:18:04.950+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:18:04.957+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.637 seconds
[2024-11-25T16:18:35.463+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:18:35.463+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 475)
[2024-11-25T16:18:35.465+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:18:35.464+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=475
[2024-11-25T16:18:35.469+0000] {processor.py:153} INFO - Started process (PID=475) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:18:35.470+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:18:35.471+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:18:35.471+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:18:35.474+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:18:35.474+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:18:35.829+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:18:35.829+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:18:35.830+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:18:35.829+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:18:36.067+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:18:36.069+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:18:36.069+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:18:36.072+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:18:36.072+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:18:36.072+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:18:36.078+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:18:36.078+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:18:36.078+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:18:36.078+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:18:36.087+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:18:36.087+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:18:36.093+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:18:36.093+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:18:36.093+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:18:36.093+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:18:36.102+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:18:36.102+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:18:36.109+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.646 seconds
[2024-11-25T16:19:06.259+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:19:06.258+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 481)
[2024-11-25T16:19:06.261+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:19:06.260+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=481
[2024-11-25T16:19:06.290+0000] {processor.py:153} INFO - Started process (PID=481) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:19:06.291+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:19:06.292+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:19:06.292+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:19:06.295+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:19:06.295+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:19:06.646+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:19:06.646+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:19:06.648+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:19:06.647+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:19:06.917+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:19:06.919+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:19:06.919+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:19:06.922+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:19:06.922+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:19:06.923+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:19:06.928+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:19:06.928+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:19:06.928+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:19:06.928+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:19:06.936+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:19:06.936+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:19:06.942+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:19:06.942+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:19:06.942+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:19:06.942+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:19:06.952+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:19:06.952+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:19:06.958+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.700 seconds
[2024-11-25T16:19:37.390+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:19:37.389+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 487)
[2024-11-25T16:19:37.391+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:19:37.391+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=487
[2024-11-25T16:19:37.396+0000] {processor.py:153} INFO - Started process (PID=487) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:19:37.398+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:19:37.399+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:19:37.399+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:19:37.401+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:19:37.401+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:19:37.997+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:19:37.996+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:19:37.997+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:19:37.997+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:19:38.164+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:19:38.167+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:19:38.167+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:19:38.173+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:19:38.173+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:19:38.173+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:19:38.185+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:19:38.185+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:19:38.185+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:19:38.185+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:19:38.201+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:19:38.201+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:19:38.212+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:19:38.212+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:19:38.214+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:19:38.213+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:19:38.237+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:19:38.237+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:19:38.251+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.862 seconds
[2024-11-25T16:20:08.348+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:20:08.347+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 493)
[2024-11-25T16:20:08.349+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:20:08.349+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=493
[2024-11-25T16:20:08.358+0000] {processor.py:153} INFO - Started process (PID=493) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:20:08.359+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:20:08.361+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:20:08.360+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:20:08.362+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:20:08.362+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:20:08.908+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:20:08.908+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:20:08.911+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:20:08.910+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:20:09.021+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:20:09.024+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:20:09.024+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:20:09.033+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:20:09.033+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:20:09.035+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:20:09.046+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:20:09.046+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:20:09.046+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:20:09.046+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:20:09.072+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:20:09.072+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:20:09.084+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:20:09.084+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:20:09.085+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:20:09.084+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:20:09.101+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:20:09.101+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:20:09.122+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.775 seconds
[2024-11-25T16:20:39.218+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:20:39.217+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 499)
[2024-11-25T16:20:39.218+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:20:39.218+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=499
[2024-11-25T16:20:39.224+0000] {processor.py:153} INFO - Started process (PID=499) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:20:39.225+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:20:39.226+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:20:39.226+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:20:39.227+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:20:39.227+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:20:39.613+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:20:39.613+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:20:39.614+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:20:39.614+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:20:39.860+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:20:39.861+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:20:39.861+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:20:39.864+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:20:39.864+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:20:39.864+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:20:39.870+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:20:39.870+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:20:39.870+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:20:39.870+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:20:39.886+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:20:39.886+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:20:39.895+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:20:39.895+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:20:39.896+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:20:39.895+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:20:39.920+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:20:39.920+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:20:39.928+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.711 seconds
[2024-11-25T16:21:10.089+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:21:10.088+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 505)
[2024-11-25T16:21:10.089+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:21:10.089+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=505
[2024-11-25T16:21:10.094+0000] {processor.py:153} INFO - Started process (PID=505) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:21:10.095+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:21:10.096+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:21:10.096+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:21:10.097+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:21:10.097+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:21:10.463+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:21:10.463+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:21:10.465+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:21:10.464+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:21:10.726+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:21:10.727+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:21:10.727+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:21:10.731+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:21:10.731+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:21:10.731+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:21:10.736+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:21:10.736+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:21:10.737+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:21:10.737+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:21:10.746+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:21:10.746+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:21:10.752+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:21:10.752+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:21:10.752+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:21:10.752+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:21:10.763+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:21:10.763+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:21:10.771+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.683 seconds
[2024-11-25T16:21:41.181+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:21:41.180+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 511)
[2024-11-25T16:21:41.182+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:21:41.182+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=511
[2024-11-25T16:21:41.189+0000] {processor.py:153} INFO - Started process (PID=511) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:21:41.190+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:21:41.191+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:21:41.191+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:21:41.192+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:21:41.192+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:21:41.612+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:21:41.612+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:21:41.613+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:21:41.613+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:21:41.878+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:21:41.879+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:21:41.879+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:21:41.884+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:21:41.884+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:21:41.884+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:21:41.890+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:21:41.890+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:21:41.891+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:21:41.891+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:21:41.913+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:21:41.913+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:21:41.920+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:21:41.920+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:21:41.921+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:21:41.920+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:21:41.930+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:21:41.930+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:21:41.938+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.758 seconds
[2024-11-25T16:22:12.274+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:22:12.273+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 517)
[2024-11-25T16:22:12.275+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:22:12.275+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=517
[2024-11-25T16:22:12.285+0000] {processor.py:153} INFO - Started process (PID=517) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:22:12.286+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:22:12.288+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:22:12.288+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:22:12.290+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:22:12.290+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:22:12.684+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:22:12.684+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:22:12.685+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:22:12.684+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:22:12.996+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:22:12.998+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:22:12.998+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:22:13.003+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:22:13.002+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:22:13.003+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:22:13.010+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:22:13.010+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:22:13.010+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:22:13.010+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:22:13.020+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:22:13.020+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:22:13.030+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:22:13.030+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:22:13.031+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:22:13.030+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:22:13.042+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:22:13.042+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:22:13.049+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.776 seconds
[2024-11-25T16:22:43.702+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:22:43.702+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 523)
[2024-11-25T16:22:43.704+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:22:43.704+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=523
[2024-11-25T16:22:43.729+0000] {processor.py:153} INFO - Started process (PID=523) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:22:43.729+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:22:43.730+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:22:43.730+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:22:43.732+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:22:43.732+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:22:44.090+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:22:44.090+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:22:44.091+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:22:44.090+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:22:44.323+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:22:44.325+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:22:44.325+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:22:44.330+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:22:44.330+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:22:44.331+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:22:44.336+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:22:44.335+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:22:44.336+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:22:44.336+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:22:44.344+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:22:44.344+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:22:44.350+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:22:44.350+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:22:44.350+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:22:44.350+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:22:44.360+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:22:44.360+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:22:44.366+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.665 seconds
[2024-11-25T16:23:14.770+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:23:14.769+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 529)
[2024-11-25T16:23:14.771+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:23:14.771+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=529
[2024-11-25T16:23:14.776+0000] {processor.py:153} INFO - Started process (PID=529) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:23:14.777+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:23:14.778+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:23:14.778+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:23:14.780+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:23:14.779+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:23:15.300+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:23:15.300+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:23:15.301+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:23:15.301+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:23:15.393+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:23:15.394+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:23:15.394+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:23:15.398+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:23:15.398+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:23:15.398+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:23:15.403+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:23:15.403+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:23:15.403+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:23:15.403+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:23:15.412+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:23:15.412+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:23:15.428+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:23:15.427+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:23:15.431+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:23:15.430+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:23:15.458+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:23:15.458+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:23:15.468+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.699 seconds
[2024-11-25T16:23:45.824+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:23:45.824+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 535)
[2024-11-25T16:23:45.826+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:23:45.826+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=535
[2024-11-25T16:23:45.831+0000] {processor.py:153} INFO - Started process (PID=535) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:23:45.832+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:23:45.834+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:23:45.834+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:23:45.837+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:23:45.837+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:23:46.317+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:23:46.317+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:23:46.318+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:23:46.318+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:23:46.409+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:23:46.411+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:23:46.410+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:23:46.414+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:23:46.414+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:23:46.414+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:23:46.419+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:23:46.419+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:23:46.419+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:23:46.419+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:23:46.426+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:23:46.426+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:23:46.433+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:23:46.432+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:23:46.433+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:23:46.433+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:23:46.441+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:23:46.441+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:23:46.448+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.624 seconds
[2024-11-25T16:24:16.954+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:24:16.954+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 541)
[2024-11-25T16:24:16.957+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:24:16.956+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=541
[2024-11-25T16:24:16.962+0000] {processor.py:153} INFO - Started process (PID=541) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:24:16.963+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:24:16.964+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:24:16.964+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:24:16.966+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:24:16.966+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:24:17.348+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:24:17.348+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:24:17.349+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:24:17.348+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:24:17.607+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:24:17.609+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:24:17.609+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:24:17.612+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:24:17.612+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:24:17.613+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:24:17.618+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:24:17.618+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:24:17.618+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:24:17.618+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:24:17.627+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:24:17.627+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:24:17.633+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:24:17.633+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:24:17.633+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:24:17.633+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:24:17.642+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:24:17.642+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:24:17.648+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.694 seconds
[2024-11-25T16:24:48.078+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:24:48.077+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 547)
[2024-11-25T16:24:48.080+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:24:48.080+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=547
[2024-11-25T16:24:48.086+0000] {processor.py:153} INFO - Started process (PID=547) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:24:48.087+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:24:48.088+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:24:48.088+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:24:48.090+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:24:48.090+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:24:48.460+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:24:48.460+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:24:48.461+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:24:48.460+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:24:48.719+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:24:48.720+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:24:48.720+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:24:48.724+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:24:48.724+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:24:48.724+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:24:48.729+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:24:48.729+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:24:48.730+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:24:48.730+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:24:48.739+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:24:48.739+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:24:48.745+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:24:48.745+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:24:48.745+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:24:48.745+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:24:48.754+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:24:48.754+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:24:48.761+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.683 seconds
[2024-11-25T16:25:19.268+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:25:19.267+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 553)
[2024-11-25T16:25:19.269+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:25:19.269+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=553
[2024-11-25T16:25:19.276+0000] {processor.py:153} INFO - Started process (PID=553) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:25:19.277+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:25:19.278+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:25:19.278+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:25:19.280+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:25:19.280+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:25:19.658+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:25:19.658+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:25:19.659+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:25:19.659+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:25:19.996+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:25:19.998+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:25:19.998+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:25:20.002+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:25:20.002+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:25:20.003+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:25:20.009+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:25:20.009+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:25:20.009+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:25:20.009+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:25:20.027+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:25:20.026+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:25:20.034+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:25:20.034+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:25:20.034+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:25:20.034+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:25:20.044+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:25:20.044+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:25:20.052+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.785 seconds
[2024-11-25T16:25:50.697+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:25:50.697+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 559)
[2024-11-25T16:25:50.698+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:25:50.698+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=559
[2024-11-25T16:25:50.704+0000] {processor.py:153} INFO - Started process (PID=559) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:25:50.705+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:25:50.706+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:25:50.705+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:25:50.707+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:25:50.707+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:25:51.078+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:25:51.078+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:25:51.080+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:25:51.079+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:25:51.325+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:25:51.327+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:25:51.327+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:25:51.331+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:25:51.330+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:25:51.331+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:25:51.336+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:25:51.336+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:25:51.336+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:25:51.336+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:25:51.345+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:25:51.345+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:25:51.351+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:25:51.351+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:25:51.351+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:25:51.351+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:25:51.361+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:25:51.361+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:25:51.368+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.672 seconds
[2024-11-25T16:26:21.931+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:26:21.930+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 565)
[2024-11-25T16:26:21.932+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:26:21.932+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=565
[2024-11-25T16:26:21.940+0000] {processor.py:153} INFO - Started process (PID=565) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:26:21.941+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:26:21.942+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:26:21.942+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:26:21.944+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:26:21.944+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:26:22.455+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:26:22.454+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:26:22.455+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:26:22.455+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:26:22.555+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:26:22.557+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:26:22.557+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:26:22.561+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:26:22.561+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:26:22.561+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:26:22.566+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:26:22.566+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:26:22.567+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:26:22.566+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:26:22.576+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:26:22.575+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:26:22.581+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:26:22.581+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:26:22.582+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:26:22.581+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:26:22.591+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:26:22.591+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:26:22.598+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.668 seconds
[2024-11-25T16:26:53.022+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:26:53.021+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 571)
[2024-11-25T16:26:53.023+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:26:53.023+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=571
[2024-11-25T16:26:53.028+0000] {processor.py:153} INFO - Started process (PID=571) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:26:53.029+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:26:53.030+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:26:53.030+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:26:53.031+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:26:53.031+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:26:53.710+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:26:53.709+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:26:53.712+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:26:53.711+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:26:53.830+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:26:53.832+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:26:53.832+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:26:53.838+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:26:53.837+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:26:53.838+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:26:53.849+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:26:53.849+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:26:53.850+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:26:53.850+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:26:53.862+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:26:53.862+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:26:53.871+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:26:53.871+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:26:53.871+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:26:53.871+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:26:53.882+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:26:53.882+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:26:53.891+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.870 seconds
[2024-11-25T16:27:24.319+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:27:24.319+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 577)
[2024-11-25T16:27:24.320+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:27:24.320+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=577
[2024-11-25T16:27:24.343+0000] {processor.py:153} INFO - Started process (PID=577) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:27:24.345+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:27:24.348+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:27:24.348+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:27:24.351+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:27:24.351+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:27:24.706+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:27:24.706+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:27:24.707+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:27:24.706+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:27:24.960+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:27:24.961+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:27:24.961+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:27:24.965+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:27:24.964+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:27:24.965+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:27:24.971+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:27:24.971+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:27:24.972+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:27:24.972+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:27:24.984+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:27:24.984+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:27:24.992+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:27:24.992+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:27:24.992+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:27:24.992+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:27:25.004+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:27:25.004+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:27:25.011+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.693 seconds
[2024-11-25T16:27:55.542+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:27:55.542+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 583)
[2024-11-25T16:27:55.544+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:27:55.544+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=583
[2024-11-25T16:27:55.574+0000] {processor.py:153} INFO - Started process (PID=583) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:27:55.578+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:27:55.582+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:27:55.582+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:27:55.584+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:27:55.583+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:27:55.937+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:27:55.937+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:27:55.939+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:27:55.938+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:27:56.200+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:27:56.202+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:27:56.202+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:27:56.206+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:27:56.206+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:27:56.206+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:27:56.212+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:27:56.212+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:27:56.212+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:27:56.212+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:27:56.222+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:27:56.222+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:27:56.228+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:27:56.228+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:27:56.228+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:27:56.228+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:27:56.237+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:27:56.237+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:27:56.245+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.703 seconds
[2024-11-25T16:28:26.722+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:28:26.722+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 589)
[2024-11-25T16:28:26.724+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:28:26.724+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=589
[2024-11-25T16:28:26.730+0000] {processor.py:153} INFO - Started process (PID=589) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:28:26.731+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:28:26.732+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:28:26.732+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:28:26.734+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:28:26.734+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:28:27.190+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:28:27.189+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:28:27.191+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:28:27.190+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:28:27.621+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:28:27.624+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:28:27.624+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:28:27.629+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:28:27.629+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:28:27.629+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:28:27.639+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:28:27.639+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:28:27.639+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:28:27.639+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:28:27.654+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:28:27.654+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:28:27.664+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:28:27.664+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:28:27.664+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:28:27.664+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:28:27.684+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:28:27.684+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:28:27.692+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.971 seconds
[2024-11-25T16:28:58.154+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:28:58.154+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 595)
[2024-11-25T16:28:58.156+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:28:58.156+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=595
[2024-11-25T16:28:58.185+0000] {processor.py:153} INFO - Started process (PID=595) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:28:58.187+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:28:58.189+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:28:58.189+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:28:58.191+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:28:58.190+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:28:58.523+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:28:58.523+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:28:58.524+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:28:58.523+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:28:58.794+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:28:58.797+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:28:58.797+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:28:58.801+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:28:58.800+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:28:58.801+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:28:58.816+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:28:58.816+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:28:58.816+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:28:58.816+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:28:58.827+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:28:58.827+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:28:58.836+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:28:58.836+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:28:58.836+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:28:58.836+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:28:58.848+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:28:58.848+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:28:58.856+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.702 seconds
[2024-11-25T16:29:29.295+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:29:29.293+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 601)
[2024-11-25T16:29:29.297+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:29:29.296+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=601
[2024-11-25T16:29:29.316+0000] {processor.py:153} INFO - Started process (PID=601) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:29:29.317+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:29:29.318+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:29:29.318+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:29:29.319+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:29:29.319+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:29:29.723+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:29:29.723+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:29:29.724+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:29:29.723+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:29:30.017+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:29:30.020+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:29:30.020+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:29:30.026+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:29:30.025+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:29:30.026+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:29:30.032+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:29:30.032+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:29:30.032+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:29:30.032+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:29:30.043+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:29:30.043+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:29:30.051+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:29:30.051+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:29:30.051+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:29:30.051+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:29:30.063+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:29:30.062+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:29:30.069+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.777 seconds
[2024-11-25T16:30:00.470+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:30:00.470+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 607)
[2024-11-25T16:30:00.472+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:30:00.472+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=607
[2024-11-25T16:30:00.477+0000] {processor.py:153} INFO - Started process (PID=607) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:30:00.477+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:30:00.479+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:30:00.478+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:30:00.481+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:30:00.480+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:30:01.018+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:30:01.018+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:30:01.019+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:30:01.019+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:30:01.140+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:30:01.142+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:30:01.142+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:30:01.146+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:30:01.146+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:30:01.146+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:30:01.152+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:30:01.152+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:30:01.152+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:30:01.152+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:30:01.161+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:30:01.161+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:30:01.167+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:30:01.167+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:30:01.167+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:30:01.167+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:30:01.177+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:30:01.177+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:30:01.183+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.712 seconds
[2024-11-25T16:30:31.746+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:30:31.746+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 613)
[2024-11-25T16:30:31.747+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:30:31.747+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=613
[2024-11-25T16:30:31.751+0000] {processor.py:153} INFO - Started process (PID=613) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:30:31.752+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:30:31.752+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:30:31.752+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:30:31.754+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:30:31.753+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:30:33.041+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:30:33.041+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:30:33.042+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:30:33.042+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:30:33.174+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:30:33.176+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:30:33.176+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:30:33.180+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:30:33.179+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:30:33.180+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:30:33.189+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:30:33.189+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:30:33.189+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:30:33.189+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:30:33.206+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:30:33.206+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:30:33.217+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:30:33.217+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:30:33.218+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:30:33.217+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:30:33.229+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:30:33.229+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:30:33.236+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 1.490 seconds
[2024-11-25T16:31:03.369+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:31:03.369+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 619)
[2024-11-25T16:31:03.370+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:31:03.370+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=619
[2024-11-25T16:31:03.376+0000] {processor.py:153} INFO - Started process (PID=619) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:31:03.377+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:31:03.378+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:31:03.378+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:31:03.380+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:31:03.380+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:31:03.750+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:31:03.750+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:31:03.751+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:31:03.750+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:31:04.047+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:31:04.049+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:31:04.049+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:31:04.052+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:31:04.052+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:31:04.053+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:31:04.058+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:31:04.058+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:31:04.059+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:31:04.058+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:31:04.074+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:31:04.073+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:31:04.080+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:31:04.080+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:31:04.081+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:31:04.080+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:31:04.091+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:31:04.091+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:31:04.098+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.729 seconds
[2024-11-25T16:31:34.148+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:31:34.147+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 625)
[2024-11-25T16:31:34.149+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:31:34.149+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=625
[2024-11-25T16:31:34.157+0000] {processor.py:153} INFO - Started process (PID=625) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:31:34.158+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:31:34.159+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:31:34.159+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:31:34.161+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:31:34.161+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:31:34.508+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:31:34.508+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:31:34.511+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:31:34.508+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:31:34.764+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:31:34.765+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:31:34.765+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:31:34.770+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:31:34.770+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:31:34.771+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:31:34.782+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:31:34.782+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:31:34.782+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:31:34.782+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:31:34.795+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:31:34.795+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:31:34.802+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:31:34.802+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:31:34.802+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:31:34.802+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:31:34.811+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:31:34.811+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:31:34.818+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.671 seconds
[2024-11-25T16:32:05.217+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:32:05.217+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 631)
[2024-11-25T16:32:05.219+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:32:05.219+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=631
[2024-11-25T16:32:05.224+0000] {processor.py:153} INFO - Started process (PID=631) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:32:05.225+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:32:05.226+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:32:05.225+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:32:05.227+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:32:05.227+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:32:05.625+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:32:05.625+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:32:05.626+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:32:05.625+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:32:05.884+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:32:05.886+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:32:05.886+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:32:05.890+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:32:05.890+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:32:05.890+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:32:05.897+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:32:05.897+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:32:05.898+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:32:05.897+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:32:05.916+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:32:05.916+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:32:05.924+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:32:05.924+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:32:05.924+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:32:05.924+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:32:05.935+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:32:05.935+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:32:05.943+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.727 seconds
[2024-11-25T16:32:36.654+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:32:36.653+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 637)
[2024-11-25T16:32:36.654+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:32:36.654+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=637
[2024-11-25T16:32:36.660+0000] {processor.py:153} INFO - Started process (PID=637) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:32:36.661+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:32:36.662+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:32:36.662+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:32:36.664+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:32:36.664+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:32:37.010+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:32:37.010+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:32:37.011+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:32:37.010+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:32:37.255+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:32:37.257+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:32:37.257+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:32:37.261+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:32:37.260+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:32:37.261+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:32:37.266+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:32:37.266+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:32:37.266+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:32:37.266+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:32:37.277+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:32:37.277+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:32:37.283+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:32:37.283+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:32:37.283+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:32:37.283+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:32:37.294+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:32:37.294+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:32:37.302+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.649 seconds
[2024-11-25T16:33:07.440+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:33:07.439+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 643)
[2024-11-25T16:33:07.440+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:33:07.440+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=643
[2024-11-25T16:33:07.445+0000] {processor.py:153} INFO - Started process (PID=643) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:33:07.445+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:33:07.446+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:33:07.446+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:33:07.447+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:33:07.447+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:33:08.184+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:33:08.184+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:33:08.185+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:33:08.185+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:33:08.347+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:33:08.353+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:33:08.353+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:33:08.362+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:33:08.362+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:33:08.363+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:33:08.369+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:33:08.369+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:33:08.369+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:33:08.369+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:33:08.384+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:33:08.384+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:33:08.391+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:33:08.391+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:33:08.391+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:33:08.391+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:33:08.402+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:33:08.402+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:33:08.438+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.999 seconds
[2024-11-25T16:33:39.124+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:33:39.121+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 649)
[2024-11-25T16:33:39.126+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:33:39.126+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=649
[2024-11-25T16:33:39.137+0000] {processor.py:153} INFO - Started process (PID=649) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:33:39.138+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:33:39.140+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:33:39.140+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:33:39.143+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:33:39.143+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:33:39.623+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:33:39.623+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:33:39.624+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:33:39.624+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:33:39.716+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:33:39.717+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:33:39.717+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:33:39.721+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:33:39.720+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:33:39.721+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:33:39.727+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:33:39.727+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:33:39.727+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:33:39.727+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:33:39.734+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:33:39.734+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:33:39.741+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:33:39.741+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:33:39.741+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:33:39.741+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:33:39.750+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:33:39.750+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:33:39.757+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.636 seconds
[2024-11-25T16:34:09.919+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:34:09.918+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 655)
[2024-11-25T16:34:09.925+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:34:09.925+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=655
[2024-11-25T16:34:09.953+0000] {processor.py:153} INFO - Started process (PID=655) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:34:09.954+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:34:09.956+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:34:09.955+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:34:09.957+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:34:09.957+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:34:10.481+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:34:10.480+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:34:10.481+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:34:10.481+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:34:10.572+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:34:10.573+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:34:10.573+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:34:10.576+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:34:10.576+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:34:10.576+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:34:10.587+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:34:10.587+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:34:10.587+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:34:10.587+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:34:10.599+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:34:10.599+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:34:10.606+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:34:10.606+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:34:10.607+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:34:10.607+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:34:10.618+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:34:10.618+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:34:10.626+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.708 seconds
[2024-11-25T16:34:40.700+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:34:40.695+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 661)
[2024-11-25T16:34:40.705+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:34:40.705+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=661
[2024-11-25T16:34:40.713+0000] {processor.py:153} INFO - Started process (PID=661) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:34:40.714+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:34:40.715+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:34:40.715+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:34:40.717+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:34:40.717+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:34:41.196+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:34:41.196+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:34:41.197+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:34:41.197+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:34:41.296+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:34:41.297+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:34:41.297+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:34:41.300+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:34:41.300+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:34:41.300+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:34:41.306+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:34:41.306+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:34:41.306+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:34:41.306+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:34:41.314+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:34:41.314+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:34:41.320+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:34:41.320+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:34:41.320+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:34:41.320+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:34:41.331+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:34:41.331+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:34:41.339+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.645 seconds
[2024-11-25T16:35:11.437+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:35:11.437+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 667)
[2024-11-25T16:35:11.438+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:35:11.438+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=667
[2024-11-25T16:35:11.444+0000] {processor.py:153} INFO - Started process (PID=667) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:35:11.445+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:35:11.447+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:35:11.447+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:35:11.449+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:35:11.449+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:35:11.795+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:35:11.795+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:35:11.796+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:35:11.795+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:35:12.056+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:35:12.057+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:35:12.057+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:35:12.060+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:35:12.060+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:35:12.061+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:35:12.066+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:35:12.066+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:35:12.066+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:35:12.066+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:35:12.075+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:35:12.075+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:35:12.081+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:35:12.080+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:35:12.081+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:35:12.081+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:35:12.090+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:35:12.090+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:35:12.096+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.660 seconds
[2024-11-25T16:35:42.200+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:35:42.199+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 673)
[2024-11-25T16:35:42.201+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:35:42.201+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=673
[2024-11-25T16:35:42.235+0000] {processor.py:153} INFO - Started process (PID=673) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:35:42.237+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:35:42.238+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:35:42.238+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:35:42.240+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:35:42.240+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:35:42.585+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:35:42.585+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:35:42.586+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:35:42.586+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:35:42.833+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:35:42.834+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:35:42.834+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:35:42.838+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:35:42.838+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:35:42.838+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:35:42.844+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:35:42.844+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:35:42.845+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:35:42.845+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:35:42.854+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:35:42.854+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:35:42.859+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:35:42.859+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:35:42.860+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:35:42.859+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:35:42.869+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:35:42.869+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:35:42.875+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.676 seconds
[2024-11-25T16:36:12.947+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:36:12.946+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 679)
[2024-11-25T16:36:12.948+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:36:12.948+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=679
[2024-11-25T16:36:12.957+0000] {processor.py:153} INFO - Started process (PID=679) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:36:12.958+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:36:12.959+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:36:12.959+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:36:12.961+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:36:12.961+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:36:13.323+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:36:13.323+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:36:13.325+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:36:13.324+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:36:13.559+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:36:13.561+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:36:13.561+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:36:13.564+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:36:13.564+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:36:13.564+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:36:13.570+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:36:13.570+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:36:13.570+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:36:13.570+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:36:13.580+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:36:13.580+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:36:13.594+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:36:13.594+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:36:13.594+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:36:13.594+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:36:13.606+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:36:13.606+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:36:13.615+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.669 seconds
[2024-11-25T16:36:44.023+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:36:44.023+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 685)
[2024-11-25T16:36:44.025+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:36:44.024+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=685
[2024-11-25T16:36:44.033+0000] {processor.py:153} INFO - Started process (PID=685) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:36:44.035+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:36:44.038+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:36:44.038+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:36:44.043+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:36:44.042+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:36:44.565+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:36:44.565+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:36:44.566+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:36:44.566+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:36:44.671+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:36:44.673+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:36:44.673+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:36:44.676+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:36:44.676+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:36:44.676+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:36:44.681+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:36:44.681+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:36:44.681+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:36:44.681+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:36:44.692+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:36:44.692+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:36:44.699+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:36:44.699+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:36:44.699+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:36:44.699+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:36:44.708+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:36:44.708+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:36:44.714+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.692 seconds
[2024-11-25T16:37:15.074+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:37:15.073+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 691)
[2024-11-25T16:37:15.076+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:37:15.076+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=691
[2024-11-25T16:37:15.082+0000] {processor.py:153} INFO - Started process (PID=691) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:37:15.083+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:37:15.085+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:37:15.085+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:37:15.089+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:37:15.089+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:37:15.600+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:37:15.600+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:37:15.601+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:37:15.601+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:37:15.697+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:37:15.699+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:37:15.699+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:37:15.702+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:37:15.702+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:37:15.702+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:37:15.707+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:37:15.707+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:37:15.708+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:37:15.708+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:37:15.715+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:37:15.715+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:37:15.721+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:37:15.721+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:37:15.722+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:37:15.722+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:37:15.730+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:37:15.730+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:37:15.737+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.664 seconds
[2024-11-25T16:37:46.206+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:37:46.206+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 697)
[2024-11-25T16:37:46.207+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:37:46.207+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=697
[2024-11-25T16:37:46.211+0000] {processor.py:153} INFO - Started process (PID=697) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:37:46.211+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:37:46.212+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:37:46.212+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:37:46.214+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:37:46.214+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:37:46.585+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:37:46.585+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:37:46.587+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:37:46.586+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:37:46.835+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:37:46.837+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:37:46.836+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:37:46.840+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:37:46.840+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:37:46.840+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:37:46.845+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:37:46.845+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:37:46.845+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:37:46.845+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:37:46.853+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:37:46.853+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:37:46.859+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:37:46.859+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:37:46.859+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:37:46.859+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:37:46.868+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:37:46.868+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:37:46.874+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.668 seconds
[2024-11-25T16:38:16.937+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:38:16.937+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 703)
[2024-11-25T16:38:16.938+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:38:16.938+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=703
[2024-11-25T16:38:16.943+0000] {processor.py:153} INFO - Started process (PID=703) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:38:16.944+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:38:16.945+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:38:16.945+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:38:16.947+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:38:16.947+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:38:17.335+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:38:17.334+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:38:17.336+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:38:17.335+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:38:17.589+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:38:17.590+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:38:17.590+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:38:17.594+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:38:17.593+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:38:17.594+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:38:17.600+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:38:17.600+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:38:17.600+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:38:17.600+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:38:17.608+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:38:17.608+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:38:17.614+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:38:17.614+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:38:17.614+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:38:17.614+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:38:17.623+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:38:17.623+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:38:17.629+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.693 seconds
[2024-11-25T16:38:47.713+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:38:47.712+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 709)
[2024-11-25T16:38:47.714+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:38:47.714+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=709
[2024-11-25T16:38:47.721+0000] {processor.py:153} INFO - Started process (PID=709) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:38:47.724+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:38:47.729+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:38:47.729+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:38:47.734+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:38:47.734+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:38:48.101+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:38:48.101+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:38:48.102+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:38:48.102+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:38:48.359+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:38:48.361+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:38:48.361+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:38:48.366+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:38:48.365+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:38:48.366+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:38:48.373+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:38:48.373+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:38:48.373+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:38:48.373+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:38:48.384+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:38:48.384+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:38:48.390+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:38:48.390+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:38:48.391+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:38:48.390+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:38:48.400+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:38:48.400+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:38:48.407+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.695 seconds
[2024-11-25T16:39:18.770+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:39:18.770+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 715)
[2024-11-25T16:39:18.771+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:39:18.771+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=715
[2024-11-25T16:39:18.778+0000] {processor.py:153} INFO - Started process (PID=715) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:39:18.779+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:39:18.780+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:39:18.780+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:39:18.782+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:39:18.782+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:39:19.140+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:39:19.140+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:39:19.141+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:39:19.140+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:39:19.412+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:39:19.414+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:39:19.414+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:39:19.424+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:39:19.424+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:39:19.424+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:39:19.436+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:39:19.436+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:39:19.436+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:39:19.436+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:39:19.449+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:39:19.449+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:39:19.456+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:39:19.456+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:39:19.457+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:39:19.456+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:39:19.467+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:39:19.467+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:39:19.479+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.709 seconds
[2024-11-25T16:39:49.853+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:39:49.852+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 721)
[2024-11-25T16:39:49.854+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:39:49.854+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=721
[2024-11-25T16:39:49.858+0000] {processor.py:153} INFO - Started process (PID=721) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:39:49.858+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:39:49.859+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:39:49.859+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:39:49.860+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:39:49.860+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:39:50.437+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:39:50.436+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:39:50.438+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:39:50.437+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:39:50.587+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:39:50.589+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:39:50.589+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:39:50.595+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:39:50.595+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:39:50.595+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:39:50.603+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:39:50.603+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:39:50.603+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:39:50.603+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:39:50.628+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:39:50.628+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:39:50.640+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:39:50.640+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:39:50.640+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:39:50.640+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:39:50.653+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:39:50.653+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:39:50.660+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.808 seconds
[2024-11-25T16:40:21.183+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:40:21.183+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 727)
[2024-11-25T16:40:21.184+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:40:21.184+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=727
[2024-11-25T16:40:21.190+0000] {processor.py:153} INFO - Started process (PID=727) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:40:21.191+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:40:21.192+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:40:21.192+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:40:21.193+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:40:21.193+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:40:21.717+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:40:21.717+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:40:21.718+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:40:21.717+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:40:21.829+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:40:21.831+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:40:21.831+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:40:21.834+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:40:21.834+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:40:21.834+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:40:21.841+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:40:21.841+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:40:21.841+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:40:21.841+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:40:21.852+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:40:21.852+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:40:21.859+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:40:21.859+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:40:21.859+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:40:21.859+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:40:21.886+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:40:21.885+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:40:21.893+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.711 seconds
[2024-11-25T16:40:52.380+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:40:52.380+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 733)
[2024-11-25T16:40:52.381+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:40:52.381+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=733
[2024-11-25T16:40:52.387+0000] {processor.py:153} INFO - Started process (PID=733) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:40:52.388+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:40:52.388+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:40:52.388+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:40:52.390+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:40:52.390+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:40:52.877+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:40:52.877+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:40:52.878+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:40:52.878+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:40:52.976+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:40:52.977+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:40:52.977+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:40:52.981+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:40:52.980+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:40:52.981+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:40:52.986+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:40:52.986+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:40:52.986+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:40:52.986+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:40:53.008+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:40:53.008+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:40:53.014+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:40:53.014+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:40:53.014+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:40:53.014+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:40:53.024+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:40:53.024+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:40:53.031+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.651 seconds
[2024-11-25T16:41:23.101+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:41:23.101+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 739)
[2024-11-25T16:41:23.102+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:41:23.102+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=739
[2024-11-25T16:41:23.107+0000] {processor.py:153} INFO - Started process (PID=739) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:41:23.108+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:41:23.109+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:41:23.109+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:41:23.111+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:41:23.111+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:41:23.627+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:41:23.627+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:41:23.628+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:41:23.627+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:41:23.753+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:41:23.755+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:41:23.755+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:41:23.758+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:41:23.758+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:41:23.758+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:41:23.764+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:41:23.764+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:41:23.764+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:41:23.764+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:41:23.774+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:41:23.774+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:41:23.780+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:41:23.780+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:41:23.780+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:41:23.780+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:41:23.791+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:41:23.791+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:41:23.801+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.700 seconds
[2024-11-25T16:41:53.950+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:41:53.949+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 745)
[2024-11-25T16:41:53.952+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:41:53.952+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=745
[2024-11-25T16:41:53.959+0000] {processor.py:153} INFO - Started process (PID=745) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:41:53.960+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:41:53.961+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:41:53.961+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:41:53.963+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:41:53.963+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:41:54.307+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:41:54.306+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:41:54.308+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:41:54.307+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:41:54.601+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:41:54.602+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:41:54.602+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:41:54.606+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:41:54.606+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:41:54.606+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:41:54.612+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:41:54.612+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:41:54.613+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:41:54.613+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:41:54.623+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:41:54.623+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:41:54.630+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:41:54.630+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:41:54.630+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:41:54.630+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:41:54.640+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:41:54.640+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:41:54.647+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.698 seconds
[2024-11-25T16:42:24.726+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:42:24.724+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 751)
[2024-11-25T16:42:24.727+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:42:24.727+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=751
[2024-11-25T16:42:24.734+0000] {processor.py:153} INFO - Started process (PID=751) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:42:24.735+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:42:24.736+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:42:24.736+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:42:24.740+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:42:24.740+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:42:25.230+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:42:25.230+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:42:25.231+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:42:25.230+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:42:25.512+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:42:25.514+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:42:25.514+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:42:25.517+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:42:25.517+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:42:25.518+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:42:25.523+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:42:25.523+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:42:25.523+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:42:25.523+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:42:25.538+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:42:25.538+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:42:25.552+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:42:25.552+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:42:25.552+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:42:25.552+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:42:25.567+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:42:25.567+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:42:25.575+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.851 seconds
[2024-11-25T16:42:56.129+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:42:56.129+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 757)
[2024-11-25T16:42:56.131+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:42:56.130+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=757
[2024-11-25T16:42:56.137+0000] {processor.py:153} INFO - Started process (PID=757) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:42:56.138+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:42:56.139+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:42:56.139+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:42:56.141+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:42:56.141+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:42:56.480+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:42:56.480+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:42:56.482+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:42:56.481+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:42:56.721+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:42:56.723+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:42:56.723+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:42:56.726+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:42:56.726+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:42:56.727+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:42:56.731+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:42:56.731+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:42:56.732+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:42:56.732+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:42:56.740+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:42:56.740+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:42:56.746+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:42:56.746+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:42:56.746+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:42:56.746+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:42:56.757+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:42:56.757+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:42:56.764+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.635 seconds
[2024-11-25T16:43:26.957+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:43:26.956+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 763)
[2024-11-25T16:43:26.958+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:43:26.958+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=763
[2024-11-25T16:43:26.968+0000] {processor.py:153} INFO - Started process (PID=763) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:43:26.969+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:43:26.970+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:43:26.970+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:43:26.972+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:43:26.972+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:43:27.456+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:43:27.455+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:43:27.457+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:43:27.456+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:43:27.556+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:43:27.557+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:43:27.557+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:43:27.560+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:43:27.560+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:43:27.561+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:43:27.566+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:43:27.566+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:43:27.566+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:43:27.566+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:43:27.575+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:43:27.575+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:43:27.581+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:43:27.581+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:43:27.581+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:43:27.581+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:43:27.590+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:43:27.590+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:43:27.598+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.642 seconds
[2024-11-25T16:43:58.042+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:43:58.041+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 769)
[2024-11-25T16:43:58.043+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:43:58.043+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=769
[2024-11-25T16:43:58.049+0000] {processor.py:153} INFO - Started process (PID=769) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:43:58.050+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:43:58.051+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:43:58.051+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:43:58.053+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:43:58.053+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:43:58.558+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:43:58.558+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:43:58.560+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:43:58.560+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:43:58.656+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:43:58.657+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:43:58.657+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:43:58.660+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:43:58.660+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:43:58.661+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:43:58.665+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:43:58.665+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:43:58.666+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:43:58.666+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:43:58.674+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:43:58.674+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:43:58.681+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:43:58.681+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:43:58.681+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:43:58.681+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:43:58.690+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:43:58.690+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:43:58.698+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.657 seconds
[2024-11-25T16:44:29.121+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:44:29.120+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 775)
[2024-11-25T16:44:29.122+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:44:29.122+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=775
[2024-11-25T16:44:29.128+0000] {processor.py:153} INFO - Started process (PID=775) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:44:29.129+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:44:29.130+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:44:29.130+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:44:29.132+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:44:29.132+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:44:29.681+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:44:29.681+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:44:29.681+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:44:29.681+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:44:29.773+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:44:29.775+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:44:29.775+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:44:29.778+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:44:29.778+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:44:29.778+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:44:29.784+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:44:29.784+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:44:29.784+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:44:29.784+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:44:29.793+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:44:29.793+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:44:29.799+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:44:29.799+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:44:29.799+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:44:29.799+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:44:29.808+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:44:29.808+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:44:29.815+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.695 seconds
[2024-11-25T16:45:00.398+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:45:00.398+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 781)
[2024-11-25T16:45:00.400+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:45:00.399+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=781
[2024-11-25T16:45:00.407+0000] {processor.py:153} INFO - Started process (PID=781) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:45:00.408+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:45:00.410+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:45:00.409+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:45:00.412+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:45:00.411+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:45:00.759+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:45:00.759+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:45:00.764+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:45:00.760+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:45:00.997+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:45:00.999+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:45:00.998+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:45:01.002+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:45:01.002+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:45:01.002+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:45:01.008+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:45:01.008+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:45:01.008+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:45:01.008+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:45:01.020+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:45:01.019+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:45:01.030+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:45:01.030+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:45:01.032+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:45:01.031+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:45:01.046+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:45:01.046+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:45:01.053+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.655 seconds
[2024-11-25T16:45:31.548+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:45:31.547+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 787)
[2024-11-25T16:45:31.549+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:45:31.549+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=787
[2024-11-25T16:45:31.554+0000] {processor.py:153} INFO - Started process (PID=787) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:45:31.555+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:45:31.556+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:45:31.556+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:45:31.558+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:45:31.558+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:45:31.926+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:45:31.926+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:45:31.927+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:45:31.926+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:45:32.177+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:45:32.179+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:45:32.179+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:45:32.183+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:45:32.182+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:45:32.183+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:45:32.189+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:45:32.189+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:45:32.189+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:45:32.189+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:45:32.198+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:45:32.198+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:45:32.204+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:45:32.204+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:45:32.204+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:45:32.204+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:45:32.213+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:45:32.213+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:45:32.221+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.674 seconds
[2024-11-25T16:46:02.310+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:46:02.310+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 793)
[2024-11-25T16:46:02.311+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:46:02.311+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=793
[2024-11-25T16:46:02.317+0000] {processor.py:153} INFO - Started process (PID=793) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:46:02.318+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:46:02.319+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:46:02.319+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:46:02.320+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:46:02.320+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:46:02.664+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:46:02.664+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:46:02.665+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:46:02.664+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:46:02.916+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:46:02.917+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:46:02.917+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:46:02.921+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:46:02.921+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:46:02.921+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:46:02.926+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:46:02.926+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:46:02.926+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:46:02.926+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:46:02.934+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:46:02.934+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:46:02.940+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:46:02.940+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:46:02.941+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:46:02.940+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:46:02.964+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:46:02.964+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:46:02.971+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.662 seconds
[2024-11-25T16:46:33.184+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:46:33.184+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 799)
[2024-11-25T16:46:33.186+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:46:33.186+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=799
[2024-11-25T16:46:33.192+0000] {processor.py:153} INFO - Started process (PID=799) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:46:33.193+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:46:33.194+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:46:33.194+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:46:33.196+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:46:33.196+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:46:33.683+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:46:33.683+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:46:33.683+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:46:33.683+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:46:33.776+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:46:33.778+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:46:33.778+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:46:33.781+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:46:33.781+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:46:33.781+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:46:33.786+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:46:33.786+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:46:33.787+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:46:33.787+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:46:33.796+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:46:33.795+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:46:33.801+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:46:33.801+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:46:33.802+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:46:33.801+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:46:33.814+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:46:33.814+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:46:33.828+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.644 seconds
[2024-11-25T16:47:04.311+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:47:04.311+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 805)
[2024-11-25T16:47:04.314+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:47:04.314+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=805
[2024-11-25T16:47:04.319+0000] {processor.py:153} INFO - Started process (PID=805) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:47:04.320+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:47:04.321+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:47:04.321+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:47:04.323+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:47:04.323+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:47:04.868+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:47:04.868+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:47:04.869+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:47:04.869+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:47:04.965+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:47:04.967+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:47:04.967+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:47:04.970+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:47:04.970+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:47:04.970+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:47:04.975+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:47:04.975+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:47:04.976+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:47:04.976+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:47:04.987+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:47:04.987+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:47:05.000+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:47:05.000+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:47:05.000+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:47:05.000+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:47:05.010+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:47:05.010+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:47:05.016+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.705 seconds
[2024-11-25T16:47:35.350+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:47:35.350+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 811)
[2024-11-25T16:47:35.352+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:47:35.352+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=811
[2024-11-25T16:47:35.358+0000] {processor.py:153} INFO - Started process (PID=811) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:47:35.359+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:47:35.360+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:47:35.360+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:47:35.362+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:47:35.362+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:47:35.850+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:47:35.850+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:47:35.851+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:47:35.851+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:47:35.948+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:47:35.950+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:47:35.950+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:47:35.953+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:47:35.953+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:47:35.954+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:47:35.959+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:47:35.959+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:47:35.959+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:47:35.959+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:47:35.968+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:47:35.968+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:47:35.974+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:47:35.974+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:47:35.975+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:47:35.975+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:47:35.984+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:47:35.984+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:47:35.991+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.641 seconds
[2024-11-25T16:48:06.455+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:48:06.454+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 817)
[2024-11-25T16:48:06.456+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:48:06.456+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=817
[2024-11-25T16:48:06.462+0000] {processor.py:153} INFO - Started process (PID=817) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:48:06.463+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:48:06.464+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:48:06.464+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:48:06.465+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:48:06.465+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:48:06.935+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:48:06.935+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:48:06.936+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:48:06.935+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:48:07.037+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:48:07.038+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:48:07.038+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:48:07.042+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:48:07.042+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:48:07.042+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:48:07.047+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:48:07.047+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:48:07.048+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:48:07.048+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:48:07.055+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:48:07.055+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:48:07.061+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:48:07.061+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:48:07.061+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:48:07.061+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:48:07.070+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:48:07.070+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:48:07.078+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.624 seconds
[2024-11-25T16:48:37.382+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:48:37.381+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 823)
[2024-11-25T16:48:37.384+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:48:37.384+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=823
[2024-11-25T16:48:37.391+0000] {processor.py:153} INFO - Started process (PID=823) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:48:37.391+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:48:37.392+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:48:37.392+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:48:37.394+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:48:37.394+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:48:37.974+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:48:37.973+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:48:37.974+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:48:37.974+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:48:38.098+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:48:38.099+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:48:38.099+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:48:38.104+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:48:38.103+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:48:38.104+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:48:38.109+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:48:38.109+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:48:38.109+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:48:38.109+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:48:38.141+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:48:38.141+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:48:38.148+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:48:38.148+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:48:38.148+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:48:38.148+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:48:38.158+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:48:38.158+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:48:38.166+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.784 seconds
[2024-11-25T16:49:08.628+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:49:08.628+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 829)
[2024-11-25T16:49:08.630+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:49:08.630+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=829
[2024-11-25T16:49:08.638+0000] {processor.py:153} INFO - Started process (PID=829) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:49:08.639+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:49:08.640+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:49:08.640+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:49:08.642+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:49:08.642+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:49:08.979+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:49:08.979+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:49:08.980+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:49:08.979+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:49:09.235+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:49:09.236+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:49:09.236+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:49:09.240+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:49:09.240+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:49:09.240+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:49:09.248+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:49:09.248+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:49:09.248+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:49:09.248+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:49:09.256+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:49:09.256+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:49:09.262+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:49:09.262+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:49:09.263+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:49:09.262+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:49:09.273+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:49:09.273+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:49:09.278+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.651 seconds
[2024-11-25T16:49:39.412+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:49:39.412+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 835)
[2024-11-25T16:49:39.413+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:49:39.413+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=835
[2024-11-25T16:49:39.420+0000] {processor.py:153} INFO - Started process (PID=835) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:49:39.421+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:49:39.422+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:49:39.422+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:49:39.424+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:49:39.424+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:49:39.759+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:49:39.759+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:49:39.760+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:49:39.759+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:49:40.014+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:49:40.017+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:49:40.017+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:49:40.021+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:49:40.020+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:49:40.021+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:49:40.026+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:49:40.026+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:49:40.026+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:49:40.026+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:49:40.033+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:49:40.033+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:49:40.039+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:49:40.039+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:49:40.039+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:49:40.039+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:49:40.048+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:49:40.048+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:49:40.055+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.643 seconds
[2024-11-25T16:50:10.538+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:50:10.538+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 841)
[2024-11-25T16:50:10.539+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:50:10.539+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=841
[2024-11-25T16:50:10.548+0000] {processor.py:153} INFO - Started process (PID=841) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:50:10.549+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:50:10.550+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:50:10.550+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:50:10.552+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:50:10.552+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:50:11.048+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:50:11.048+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:50:11.048+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:50:11.048+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:50:11.151+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:50:11.153+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:50:11.153+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:50:11.156+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:50:11.156+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:50:11.156+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:50:11.161+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:50:11.161+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:50:11.161+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:50:11.161+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:50:11.169+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:50:11.169+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:50:11.174+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:50:11.174+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:50:11.175+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:50:11.175+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:50:11.184+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:50:11.184+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:50:11.192+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.654 seconds
[2024-11-25T16:50:41.530+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:50:41.529+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 847)
[2024-11-25T16:50:41.531+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:50:41.531+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=847
[2024-11-25T16:50:41.542+0000] {processor.py:153} INFO - Started process (PID=847) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:50:41.543+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:50:41.544+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:50:41.544+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:50:41.546+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:50:41.546+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:50:42.047+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:50:42.047+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:50:42.048+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:50:42.047+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:50:42.142+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:50:42.144+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:50:42.144+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:50:42.147+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:50:42.147+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:50:42.147+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:50:42.152+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:50:42.152+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:50:42.152+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:50:42.152+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:50:42.160+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:50:42.160+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:50:42.166+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:50:42.166+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:50:42.167+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:50:42.167+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:50:42.175+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:50:42.175+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:50:42.183+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.654 seconds
[2024-11-25T16:51:12.614+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:51:12.612+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 853)
[2024-11-25T16:51:12.614+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:51:12.614+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=853
[2024-11-25T16:51:12.620+0000] {processor.py:153} INFO - Started process (PID=853) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:51:12.620+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:51:12.621+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:51:12.621+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:51:12.623+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:51:12.623+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:51:13.124+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:51:13.124+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:51:13.125+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:51:13.125+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:51:13.218+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:51:13.220+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:51:13.220+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:51:13.223+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:51:13.223+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:51:13.223+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:51:13.229+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:51:13.229+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:51:13.229+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:51:13.229+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:51:13.237+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:51:13.237+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:51:13.243+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:51:13.243+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:51:13.244+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:51:13.244+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:51:13.257+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:51:13.257+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:51:13.264+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.652 seconds
[2024-11-25T16:51:43.532+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:51:43.532+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 859)
[2024-11-25T16:51:43.533+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:51:43.533+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=859
[2024-11-25T16:51:43.539+0000] {processor.py:153} INFO - Started process (PID=859) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:51:43.540+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:51:43.541+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:51:43.541+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:51:43.542+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:51:43.542+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:51:43.912+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:51:43.912+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:51:43.913+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:51:43.912+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:51:44.173+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:51:44.174+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:51:44.174+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:51:44.178+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:51:44.178+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:51:44.178+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:51:44.183+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:51:44.183+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:51:44.184+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:51:44.184+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:51:44.192+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:51:44.192+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:51:44.198+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:51:44.198+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:51:44.198+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:51:44.198+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:51:44.207+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:51:44.207+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:51:44.214+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.683 seconds
[2024-11-25T16:52:14.615+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:52:14.614+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 865)
[2024-11-25T16:52:14.616+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:52:14.616+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=865
[2024-11-25T16:52:14.621+0000] {processor.py:153} INFO - Started process (PID=865) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:52:14.622+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:52:14.623+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:52:14.623+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:52:14.625+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:52:14.625+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:52:14.977+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:52:14.976+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:52:14.978+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:52:14.977+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:52:15.233+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:52:15.235+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:52:15.234+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:52:15.238+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:52:15.238+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:52:15.238+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:52:15.245+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:52:15.245+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:52:15.245+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:52:15.245+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:52:15.256+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:52:15.256+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:52:15.272+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:52:15.272+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:52:15.273+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:52:15.273+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:52:15.283+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:52:15.283+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:52:15.291+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.677 seconds
[2024-11-25T16:52:45.652+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:52:45.651+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 871)
[2024-11-25T16:52:45.653+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:52:45.653+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=871
[2024-11-25T16:52:45.657+0000] {processor.py:153} INFO - Started process (PID=871) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:52:45.658+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:52:45.659+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:52:45.659+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:52:45.661+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:52:45.661+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:52:46.019+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:52:46.019+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:52:46.020+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:52:46.019+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:52:46.289+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:52:46.292+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:52:46.292+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:52:46.297+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:52:46.297+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:52:46.298+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:52:46.303+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:52:46.303+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:52:46.304+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:52:46.303+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:52:46.311+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:52:46.311+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:52:46.317+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:52:46.317+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:52:46.317+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:52:46.317+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:52:46.345+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:52:46.345+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:52:46.352+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.701 seconds
[2024-11-25T16:53:16.843+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:53:16.842+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 877)
[2024-11-25T16:53:16.844+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:53:16.843+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=877
[2024-11-25T16:53:16.849+0000] {processor.py:153} INFO - Started process (PID=877) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:53:16.850+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:53:16.851+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:53:16.851+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:53:16.853+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:53:16.853+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:53:17.373+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:53:17.373+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:53:17.374+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:53:17.374+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:53:17.468+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:53:17.471+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:53:17.471+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:53:17.476+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:53:17.476+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:53:17.478+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:53:17.484+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:53:17.484+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:53:17.484+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:53:17.484+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:53:17.491+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:53:17.491+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:53:17.497+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:53:17.497+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:53:17.498+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:53:17.497+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:53:17.518+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:53:17.518+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:53:17.525+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.683 seconds
[2024-11-25T16:53:47.990+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:53:47.990+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 883)
[2024-11-25T16:53:47.991+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:53:47.991+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=883
[2024-11-25T16:53:47.999+0000] {processor.py:153} INFO - Started process (PID=883) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:53:48.000+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:53:48.000+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:53:48.000+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:53:48.002+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:53:48.002+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:53:48.563+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:53:48.563+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:53:48.563+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:53:48.563+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:53:48.661+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:53:48.663+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:53:48.663+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:53:48.666+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:53:48.666+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:53:48.667+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:53:48.672+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:53:48.671+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:53:48.672+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:53:48.672+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:53:48.680+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:53:48.680+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:53:48.686+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:53:48.686+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:53:48.686+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:53:48.686+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:53:48.695+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:53:48.695+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:53:48.701+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.711 seconds
[2024-11-25T16:54:18.932+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:54:18.931+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 889)
[2024-11-25T16:54:18.933+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:54:18.933+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=889
[2024-11-25T16:54:18.939+0000] {processor.py:153} INFO - Started process (PID=889) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:54:18.940+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:54:18.941+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:54:18.940+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:54:18.942+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:54:18.942+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:54:19.453+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:54:19.453+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:54:19.454+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:54:19.453+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:54:19.555+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:54:19.557+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:54:19.557+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:54:19.561+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:54:19.561+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:54:19.561+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:54:19.566+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:54:19.566+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:54:19.566+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:54:19.566+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:54:19.578+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:54:19.578+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:54:19.583+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:54:19.583+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:54:19.584+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:54:19.583+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:54:19.592+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:54:19.592+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:54:19.599+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.668 seconds
[2024-11-25T16:54:50.101+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:54:50.101+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 895)
[2024-11-25T16:54:50.103+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:54:50.103+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=895
[2024-11-25T16:54:50.109+0000] {processor.py:153} INFO - Started process (PID=895) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:54:50.110+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:54:50.111+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:54:50.110+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:54:50.112+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:54:50.112+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:54:50.629+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:54:50.629+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:54:50.629+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:54:50.629+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:54:50.729+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:54:50.730+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:54:50.730+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:54:50.734+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:54:50.734+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:54:50.734+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:54:50.739+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:54:50.739+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:54:50.739+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:54:50.739+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:54:50.748+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:54:50.748+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:54:50.754+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:54:50.754+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:54:50.755+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:54:50.755+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:54:50.765+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:54:50.764+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:54:50.773+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.672 seconds
[2024-11-25T16:55:21.009+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:55:21.008+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 901)
[2024-11-25T16:55:21.014+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:55:21.014+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=901
[2024-11-25T16:55:21.041+0000] {processor.py:153} INFO - Started process (PID=901) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:55:21.042+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:55:21.043+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:55:21.043+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:55:21.045+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:55:21.045+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:55:21.533+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:55:21.533+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:55:21.533+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:55:21.533+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:55:21.639+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:55:21.640+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:55:21.640+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:55:21.644+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:55:21.644+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:55:21.644+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:55:21.650+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:55:21.650+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:55:21.650+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:55:21.650+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:55:21.659+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:55:21.659+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:55:21.664+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:55:21.664+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:55:21.665+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:55:21.664+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:55:21.674+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:55:21.674+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:55:21.681+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.673 seconds
[2024-11-25T16:55:51.765+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:55:51.765+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 907)
[2024-11-25T16:55:51.767+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:55:51.767+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=907
[2024-11-25T16:55:51.775+0000] {processor.py:153} INFO - Started process (PID=907) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:55:51.776+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:55:51.777+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:55:51.777+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:55:51.779+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:55:51.779+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:55:52.161+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:55:52.161+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:55:52.162+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:55:52.161+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:55:52.417+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:55:52.435+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:55:52.435+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:55:52.440+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:55:52.439+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:55:52.440+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:55:52.445+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:55:52.445+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:55:52.445+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:55:52.445+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:55:52.453+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:55:52.453+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:55:52.459+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:55:52.459+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:55:52.459+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:55:52.459+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:55:52.469+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:55:52.469+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:55:52.476+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.711 seconds
[2024-11-25T16:56:22.526+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:56:22.526+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 913)
[2024-11-25T16:56:22.527+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:56:22.527+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=913
[2024-11-25T16:56:22.533+0000] {processor.py:153} INFO - Started process (PID=913) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:56:22.534+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:56:22.535+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:56:22.535+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:56:22.537+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:56:22.536+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:56:23.084+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:56:23.084+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:56:23.085+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:56:23.085+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:56:23.180+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:56:23.182+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:56:23.182+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:56:23.185+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:56:23.185+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:56:23.186+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:56:23.191+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:56:23.190+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:56:23.191+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:56:23.191+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:56:23.199+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:56:23.199+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:56:23.208+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:56:23.207+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:56:23.208+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:56:23.208+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:56:23.219+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:56:23.219+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:56:23.225+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.700 seconds
[2024-11-25T16:56:53.343+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:56:53.342+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 919)
[2024-11-25T16:56:53.344+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:56:53.344+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=919
[2024-11-25T16:56:53.383+0000] {processor.py:153} INFO - Started process (PID=919) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:56:53.385+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:56:53.386+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:56:53.386+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:56:53.387+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:56:53.387+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:56:53.942+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:56:53.941+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:56:53.943+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:56:53.942+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:56:54.058+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:56:54.059+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:56:54.059+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:56:54.063+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:56:54.063+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:56:54.063+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:56:54.069+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:56:54.069+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:56:54.069+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:56:54.069+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:56:54.091+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:56:54.090+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:56:54.098+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:56:54.097+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:56:54.098+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:56:54.098+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:56:54.109+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:56:54.109+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:56:54.117+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.775 seconds
[2024-11-25T16:57:24.633+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:57:24.633+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 925)
[2024-11-25T16:57:24.634+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:57:24.634+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=925
[2024-11-25T16:57:24.640+0000] {processor.py:153} INFO - Started process (PID=925) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:57:24.640+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:57:24.641+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:57:24.641+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:57:24.642+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:57:24.642+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:57:25.250+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:57:25.250+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:57:25.251+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:57:25.251+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:57:25.354+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:57:25.356+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:57:25.356+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:57:25.359+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:57:25.359+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:57:25.359+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:57:25.365+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:57:25.365+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:57:25.365+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:57:25.365+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:57:25.376+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:57:25.376+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:57:25.382+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:57:25.382+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:57:25.383+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:57:25.382+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:57:25.392+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:57:25.391+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:57:25.398+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.765 seconds
[2024-11-25T16:57:55.596+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:57:55.596+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 931)
[2024-11-25T16:57:55.598+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:57:55.598+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=931
[2024-11-25T16:57:55.608+0000] {processor.py:153} INFO - Started process (PID=931) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:57:55.609+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:57:55.610+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:57:55.609+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:57:55.611+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:57:55.611+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:57:56.113+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:57:56.113+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:57:56.114+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:57:56.114+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:57:56.218+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:57:56.220+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:57:56.220+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:57:56.224+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:57:56.224+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:57:56.224+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:57:56.230+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:57:56.230+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:57:56.230+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:57:56.230+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:57:56.237+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:57:56.237+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:57:56.242+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:57:56.242+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:57:56.243+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:57:56.242+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:57:56.251+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:57:56.251+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:57:56.258+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.662 seconds
[2024-11-25T16:58:26.318+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:58:26.318+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 937)
[2024-11-25T16:58:26.321+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:58:26.321+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=937
[2024-11-25T16:58:26.329+0000] {processor.py:153} INFO - Started process (PID=937) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:58:26.330+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:58:26.331+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:58:26.331+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:58:26.332+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:58:26.332+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:58:26.976+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:58:26.976+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:58:26.977+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:58:26.977+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:58:27.070+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:58:27.071+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:58:27.071+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:58:27.075+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:58:27.075+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:58:27.075+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:58:27.080+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:58:27.080+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:58:27.081+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:58:27.081+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:58:27.112+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:58:27.112+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:58:27.119+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:58:27.119+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:58:27.120+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:58:27.119+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:58:27.131+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:58:27.131+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:58:27.138+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.821 seconds
[2024-11-25T16:58:57.707+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:58:57.706+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 943)
[2024-11-25T16:58:57.712+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:58:57.712+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=943
[2024-11-25T16:58:57.737+0000] {processor.py:153} INFO - Started process (PID=943) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:58:57.738+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:58:57.741+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:58:57.741+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:58:57.744+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:58:57.744+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:58:58.246+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:58:58.246+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:58:58.247+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:58:58.247+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:58:58.337+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:58:58.339+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:58:58.339+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:58:58.342+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:58:58.342+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:58:58.343+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:58:58.347+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:58:58.347+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:58:58.348+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:58:58.348+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:58:58.355+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:58:58.355+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:58:58.360+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:58:58.360+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:58:58.361+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:58:58.360+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:58:58.371+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:58:58.371+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:58:58.378+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.673 seconds
[2024-11-25T16:59:28.849+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:59:28.848+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 949)
[2024-11-25T16:59:28.851+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:59:28.851+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=949
[2024-11-25T16:59:28.859+0000] {processor.py:153} INFO - Started process (PID=949) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:59:28.860+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:59:28.861+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:59:28.861+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:59:28.863+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:59:28.863+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:59:29.211+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:59:29.211+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T16:59:29.212+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:59:29.212+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T16:59:29.509+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T16:59:29.511+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:59:29.511+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T16:59:29.514+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:59:29.514+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T16:59:29.514+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:59:29.519+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:59:29.519+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T16:59:29.519+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:59:29.519+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T16:59:29.529+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:59:29.529+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T16:59:29.534+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:59:29.534+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T16:59:29.535+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:59:29.535+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T16:59:29.544+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:59:29.544+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T16:59:29.550+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.702 seconds
[2024-11-25T16:59:59.831+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:59:59.830+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 955)
[2024-11-25T16:59:59.832+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:59:59.832+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=955
[2024-11-25T16:59:59.838+0000] {processor.py:153} INFO - Started process (PID=955) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:59:59.839+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T16:59:59.840+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:59:59.840+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T16:59:59.842+0000] {logging_mixin.py:137} INFO - [2024-11-25T16:59:59.842+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:00:00.375+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:00:00.375+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:00:00.376+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:00:00.376+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:00:00.481+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:00:00.483+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:00:00.483+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:00:00.487+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:00:00.487+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:00:00.487+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:00:00.492+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:00:00.492+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:00:00.492+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:00:00.492+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:00:00.500+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:00:00.500+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:00:00.506+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:00:00.506+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:00:00.507+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:00:00.506+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:00:00.516+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:00:00.516+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:00:00.523+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.693 seconds
[2024-11-25T17:00:31.040+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:00:31.039+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 961)
[2024-11-25T17:00:31.043+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:00:31.043+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=961
[2024-11-25T17:00:31.061+0000] {processor.py:153} INFO - Started process (PID=961) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:00:31.062+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:00:31.063+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:00:31.063+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:00:31.065+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:00:31.065+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:00:31.593+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:00:31.593+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:00:31.594+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:00:31.594+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:00:31.686+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:00:31.687+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:00:31.687+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:00:31.690+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:00:31.690+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:00:31.691+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:00:31.696+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:00:31.696+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:00:31.696+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:00:31.696+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:00:31.707+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:00:31.707+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:00:31.713+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:00:31.713+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:00:31.714+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:00:31.713+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:00:31.723+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:00:31.723+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:00:31.729+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.690 seconds
[2024-11-25T17:01:01.848+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:01:01.847+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 967)
[2024-11-25T17:01:01.849+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:01:01.849+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=967
[2024-11-25T17:01:01.872+0000] {processor.py:153} INFO - Started process (PID=967) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:01:01.873+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:01:01.875+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:01:01.875+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:01:01.878+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:01:01.877+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:01:02.413+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:01:02.412+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:01:02.413+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:01:02.413+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:01:02.520+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:01:02.521+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:01:02.521+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:01:02.525+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:01:02.525+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:01:02.525+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:01:02.530+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:01:02.530+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:01:02.531+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:01:02.531+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:01:02.538+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:01:02.538+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:01:02.544+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:01:02.544+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:01:02.544+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:01:02.544+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:01:02.553+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:01:02.553+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:01:02.560+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.713 seconds
[2024-11-25T17:01:32.684+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:01:32.684+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 973)
[2024-11-25T17:01:32.686+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:01:32.686+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=973
[2024-11-25T17:01:32.694+0000] {processor.py:153} INFO - Started process (PID=973) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:01:32.694+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:01:32.695+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:01:32.695+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:01:32.697+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:01:32.697+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:01:33.200+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:01:33.200+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:01:33.201+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:01:33.200+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:01:33.293+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:01:33.294+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:01:33.294+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:01:33.298+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:01:33.298+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:01:33.298+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:01:33.303+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:01:33.303+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:01:33.304+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:01:33.304+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:01:33.311+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:01:33.311+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:01:33.317+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:01:33.317+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:01:33.317+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:01:33.317+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:01:33.326+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:01:33.326+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:01:33.333+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.650 seconds
[2024-11-25T17:02:03.411+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:02:03.410+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 979)
[2024-11-25T17:02:03.413+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:02:03.413+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=979
[2024-11-25T17:02:03.418+0000] {processor.py:153} INFO - Started process (PID=979) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:02:03.419+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:02:03.420+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:02:03.420+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:02:03.422+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:02:03.422+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:02:03.930+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:02:03.930+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:02:03.931+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:02:03.930+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:02:04.028+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:02:04.029+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:02:04.029+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:02:04.033+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:02:04.033+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:02:04.033+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:02:04.039+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:02:04.038+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:02:04.039+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:02:04.039+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:02:04.047+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:02:04.047+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:02:04.053+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:02:04.053+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:02:04.053+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:02:04.053+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:02:04.063+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:02:04.063+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:02:04.070+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.660 seconds
[2024-11-25T17:02:34.291+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:02:34.291+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 985)
[2024-11-25T17:02:34.292+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:02:34.292+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=985
[2024-11-25T17:02:34.298+0000] {processor.py:153} INFO - Started process (PID=985) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:02:34.298+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:02:34.299+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:02:34.299+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:02:34.301+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:02:34.301+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:02:34.670+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:02:34.670+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:02:34.671+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:02:34.670+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:02:34.908+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:02:34.910+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:02:34.910+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:02:34.914+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:02:34.913+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:02:34.914+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:02:34.919+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:02:34.919+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:02:34.919+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:02:34.919+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:02:34.928+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:02:34.927+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:02:34.933+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:02:34.933+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:02:34.934+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:02:34.933+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:02:34.950+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:02:34.950+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:02:34.964+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.673 seconds
[2024-11-25T17:03:05.404+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:03:05.403+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 991)
[2024-11-25T17:03:05.405+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:03:05.405+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=991
[2024-11-25T17:03:05.426+0000] {processor.py:153} INFO - Started process (PID=991) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:03:05.428+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:03:05.429+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:03:05.429+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:03:05.430+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:03:05.430+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:03:05.948+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:03:05.948+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:03:05.949+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:03:05.949+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:03:06.041+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:03:06.042+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:03:06.042+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:03:06.046+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:03:06.046+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:03:06.046+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:03:06.051+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:03:06.051+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:03:06.051+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:03:06.051+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:03:06.062+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:03:06.062+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:03:06.070+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:03:06.070+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:03:06.070+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:03:06.070+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:03:06.097+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:03:06.097+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:03:06.104+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.701 seconds
[2024-11-25T17:03:36.259+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:03:36.259+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 997)
[2024-11-25T17:03:36.260+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:03:36.260+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=997
[2024-11-25T17:03:36.265+0000] {processor.py:153} INFO - Started process (PID=997) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:03:36.266+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:03:36.267+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:03:36.267+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:03:36.269+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:03:36.269+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:03:36.776+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:03:36.776+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:03:36.777+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:03:36.777+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:03:36.870+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:03:36.872+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:03:36.872+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:03:36.875+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:03:36.875+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:03:36.875+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:03:36.881+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:03:36.881+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:03:36.881+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:03:36.881+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:03:36.907+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:03:36.907+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:03:36.913+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:03:36.913+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:03:36.914+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:03:36.914+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:03:36.923+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:03:36.923+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:03:36.930+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.671 seconds
[2024-11-25T17:04:07.017+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:04:07.016+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1003)
[2024-11-25T17:04:07.018+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:04:07.018+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1003
[2024-11-25T17:04:07.027+0000] {processor.py:153} INFO - Started process (PID=1003) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:04:07.029+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:04:07.030+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:04:07.030+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:04:07.032+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:04:07.032+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:04:07.535+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:04:07.534+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:04:07.535+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:04:07.535+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:04:07.633+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:04:07.634+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:04:07.634+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:04:07.638+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:04:07.638+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:04:07.638+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:04:07.648+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:04:07.648+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:04:07.648+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:04:07.648+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:04:07.658+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:04:07.658+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:04:07.665+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:04:07.665+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:04:07.665+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:04:07.665+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:04:07.674+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:04:07.673+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:04:07.681+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.664 seconds
[2024-11-25T17:04:37.816+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:04:37.816+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1009)
[2024-11-25T17:04:37.817+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:04:37.817+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1009
[2024-11-25T17:04:37.821+0000] {processor.py:153} INFO - Started process (PID=1009) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:04:37.822+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:04:37.822+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:04:37.822+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:04:37.823+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:04:37.823+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:04:38.338+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:04:38.338+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:04:38.339+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:04:38.339+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:04:38.449+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:04:38.451+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:04:38.451+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:04:38.454+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:04:38.454+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:04:38.455+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:04:38.460+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:04:38.460+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:04:38.460+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:04:38.460+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:04:38.470+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:04:38.470+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:04:38.475+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:04:38.475+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:04:38.476+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:04:38.475+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:04:38.484+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:04:38.484+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:04:38.491+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.675 seconds
[2024-11-25T17:05:08.555+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:05:08.554+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1015)
[2024-11-25T17:05:08.556+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:05:08.556+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1015
[2024-11-25T17:05:08.564+0000] {processor.py:153} INFO - Started process (PID=1015) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:05:08.565+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:05:08.566+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:05:08.566+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:05:08.568+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:05:08.567+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:05:09.080+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:05:09.080+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:05:09.081+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:05:09.080+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:05:09.174+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:05:09.175+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:05:09.175+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:05:09.178+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:05:09.178+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:05:09.179+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:05:09.184+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:05:09.184+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:05:09.184+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:05:09.184+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:05:09.193+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:05:09.193+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:05:09.198+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:05:09.198+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:05:09.198+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:05:09.198+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:05:09.207+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:05:09.207+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:05:09.214+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.660 seconds
[2024-11-25T17:05:39.437+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:05:39.437+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1021)
[2024-11-25T17:05:39.438+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:05:39.438+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1021
[2024-11-25T17:05:39.444+0000] {processor.py:153} INFO - Started process (PID=1021) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:05:39.445+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:05:39.446+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:05:39.446+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:05:39.448+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:05:39.448+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:05:39.936+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:05:39.936+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:05:39.937+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:05:39.936+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:05:40.039+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:05:40.040+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:05:40.040+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:05:40.044+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:05:40.044+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:05:40.044+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:05:40.049+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:05:40.049+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:05:40.049+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:05:40.049+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:05:40.059+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:05:40.059+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:05:40.065+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:05:40.065+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:05:40.065+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:05:40.065+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:05:40.074+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:05:40.074+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:05:40.081+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.645 seconds
[2024-11-25T17:06:10.532+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:06:10.532+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1027)
[2024-11-25T17:06:10.534+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:06:10.534+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1027
[2024-11-25T17:06:10.540+0000] {processor.py:153} INFO - Started process (PID=1027) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:06:10.541+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:06:10.542+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:06:10.542+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:06:10.543+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:06:10.543+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:06:10.900+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:06:10.900+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:06:10.901+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:06:10.900+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:06:11.151+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:06:11.153+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:06:11.153+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:06:11.156+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:06:11.156+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:06:11.157+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:06:11.162+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:06:11.162+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:06:11.162+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:06:11.162+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:06:11.172+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:06:11.172+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:06:11.178+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:06:11.178+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:06:11.178+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:06:11.178+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:06:11.187+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:06:11.187+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:06:11.194+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.662 seconds
[2024-11-25T17:06:41.411+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:06:41.410+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1033)
[2024-11-25T17:06:41.412+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:06:41.412+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1033
[2024-11-25T17:06:41.419+0000] {processor.py:153} INFO - Started process (PID=1033) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:06:41.420+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:06:41.422+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:06:41.422+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:06:41.424+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:06:41.424+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:06:41.923+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:06:41.923+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:06:41.924+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:06:41.924+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:06:42.014+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:06:42.016+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:06:42.016+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:06:42.019+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:06:42.019+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:06:42.019+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:06:42.024+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:06:42.024+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:06:42.024+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:06:42.024+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:06:42.034+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:06:42.034+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:06:42.039+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:06:42.039+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:06:42.040+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:06:42.040+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:06:42.051+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:06:42.051+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:06:42.057+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.647 seconds
[2024-11-25T17:07:12.107+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:07:12.106+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1039)
[2024-11-25T17:07:12.108+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:07:12.108+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1039
[2024-11-25T17:07:12.115+0000] {processor.py:153} INFO - Started process (PID=1039) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:07:12.116+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:07:12.117+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:07:12.117+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:07:12.119+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:07:12.119+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:07:12.617+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:07:12.617+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:07:12.618+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:07:12.618+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:07:12.715+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:07:12.717+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:07:12.717+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:07:12.720+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:07:12.720+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:07:12.720+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:07:12.725+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:07:12.725+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:07:12.725+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:07:12.725+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:07:12.735+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:07:12.735+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:07:12.741+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:07:12.741+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:07:12.741+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:07:12.741+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:07:12.750+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:07:12.750+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:07:12.756+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.650 seconds
[2024-11-25T17:07:43.040+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:07:43.039+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1045)
[2024-11-25T17:07:43.041+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:07:43.041+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1045
[2024-11-25T17:07:43.050+0000] {processor.py:153} INFO - Started process (PID=1045) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:07:43.052+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:07:43.053+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:07:43.053+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:07:43.055+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:07:43.055+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:07:43.572+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:07:43.572+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:07:43.573+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:07:43.573+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:07:43.675+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:07:43.678+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:07:43.678+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:07:43.686+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:07:43.686+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:07:43.686+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:07:43.692+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:07:43.691+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:07:43.692+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:07:43.692+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:07:43.699+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:07:43.699+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:07:43.705+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:07:43.705+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:07:43.706+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:07:43.705+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:07:43.714+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:07:43.714+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:07:43.721+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.682 seconds
[2024-11-25T17:08:14.215+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:08:14.214+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1051)
[2024-11-25T17:08:14.217+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:08:14.217+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1051
[2024-11-25T17:08:14.224+0000] {processor.py:153} INFO - Started process (PID=1051) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:08:14.225+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:08:14.226+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:08:14.226+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:08:14.228+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:08:14.228+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:08:14.760+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:08:14.760+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:08:14.761+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:08:14.760+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:08:14.853+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:08:14.854+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:08:14.854+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:08:14.857+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:08:14.857+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:08:14.858+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:08:14.863+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:08:14.863+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:08:14.863+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:08:14.863+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:08:14.873+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:08:14.872+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:08:14.878+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:08:14.878+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:08:14.878+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:08:14.878+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:08:14.887+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:08:14.887+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:08:14.894+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.680 seconds
[2024-11-25T17:08:45.010+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:08:45.010+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1057)
[2024-11-25T17:08:45.011+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:08:45.011+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1057
[2024-11-25T17:08:45.017+0000] {processor.py:153} INFO - Started process (PID=1057) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:08:45.018+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:08:45.019+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:08:45.019+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:08:45.021+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:08:45.021+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:08:45.497+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:08:45.497+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:08:45.498+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:08:45.498+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:08:45.588+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:08:45.590+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:08:45.590+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:08:45.593+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:08:45.593+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:08:45.593+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:08:45.598+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:08:45.598+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:08:45.599+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:08:45.599+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:08:45.606+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:08:45.606+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:08:45.612+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:08:45.612+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:08:45.612+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:08:45.612+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:08:45.621+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:08:45.621+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:08:45.629+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.619 seconds
[2024-11-25T17:09:15.858+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:09:15.857+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1063)
[2024-11-25T17:09:15.859+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:09:15.859+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1063
[2024-11-25T17:09:15.866+0000] {processor.py:153} INFO - Started process (PID=1063) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:09:15.867+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:09:15.868+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:09:15.868+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:09:15.870+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:09:15.870+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:09:16.227+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:09:16.227+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:09:16.229+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:09:16.228+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:09:16.484+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:09:16.485+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:09:16.485+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:09:16.489+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:09:16.489+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:09:16.489+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:09:16.494+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:09:16.494+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:09:16.495+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:09:16.494+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:09:16.504+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:09:16.504+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:09:16.509+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:09:16.509+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:09:16.510+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:09:16.509+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:09:16.519+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:09:16.518+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:09:16.526+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.669 seconds
[2024-11-25T17:09:47.018+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:09:47.018+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1069)
[2024-11-25T17:09:47.020+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:09:47.020+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1069
[2024-11-25T17:09:47.027+0000] {processor.py:153} INFO - Started process (PID=1069) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:09:47.028+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:09:47.029+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:09:47.029+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:09:47.031+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:09:47.031+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:09:47.650+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:09:47.650+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:09:47.652+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:09:47.652+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:09:47.757+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:09:47.759+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:09:47.759+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:09:47.763+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:09:47.763+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:09:47.763+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:09:47.769+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:09:47.769+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:09:47.769+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:09:47.769+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:09:47.782+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:09:47.782+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:09:47.788+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:09:47.788+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:09:47.788+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:09:47.788+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:09:47.798+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:09:47.798+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:09:47.807+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.789 seconds
[2024-11-25T17:10:17.935+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:10:17.935+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1075)
[2024-11-25T17:10:17.937+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:10:17.937+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1075
[2024-11-25T17:10:17.944+0000] {processor.py:153} INFO - Started process (PID=1075) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:10:17.944+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:10:17.945+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:10:17.945+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:10:17.948+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:10:17.947+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:10:18.474+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:10:18.474+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:10:18.475+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:10:18.475+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:10:18.568+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:10:18.569+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:10:18.569+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:10:18.572+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:10:18.572+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:10:18.573+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:10:18.578+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:10:18.578+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:10:18.578+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:10:18.578+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:10:18.592+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:10:18.592+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:10:18.617+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:10:18.617+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:10:18.617+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:10:18.617+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:10:18.628+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:10:18.628+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:10:18.634+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.700 seconds
[2024-11-25T17:10:48.821+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:10:48.820+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1081)
[2024-11-25T17:10:48.822+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:10:48.822+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1081
[2024-11-25T17:10:48.830+0000] {processor.py:153} INFO - Started process (PID=1081) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:10:48.831+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:10:48.832+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:10:48.832+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:10:48.836+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:10:48.835+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:10:49.375+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:10:49.375+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:10:49.375+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:10:49.375+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:10:49.479+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:10:49.481+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:10:49.481+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:10:49.487+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:10:49.487+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:10:49.487+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:10:49.496+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:10:49.496+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:10:49.496+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:10:49.496+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:10:49.506+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:10:49.506+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:10:49.512+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:10:49.512+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:10:49.512+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:10:49.512+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:10:49.522+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:10:49.522+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:10:49.528+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.708 seconds
[2024-11-25T17:11:19.595+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:11:19.594+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1087)
[2024-11-25T17:11:19.596+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:11:19.596+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1087
[2024-11-25T17:11:19.602+0000] {processor.py:153} INFO - Started process (PID=1087) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:11:19.603+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:11:19.604+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:11:19.604+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:11:19.606+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:11:19.606+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:11:20.098+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:11:20.098+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:11:20.099+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:11:20.099+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:11:20.193+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:11:20.194+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:11:20.194+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:11:20.197+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:11:20.197+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:11:20.198+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:11:20.203+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:11:20.203+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:11:20.203+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:11:20.203+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:11:20.212+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:11:20.212+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:11:20.218+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:11:20.218+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:11:20.218+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:11:20.218+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:11:20.227+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:11:20.227+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:11:20.233+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.639 seconds
[2024-11-25T17:11:50.497+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:11:50.497+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1093)
[2024-11-25T17:11:50.498+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:11:50.498+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1093
[2024-11-25T17:11:50.505+0000] {processor.py:153} INFO - Started process (PID=1093) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:11:50.506+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:11:50.507+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:11:50.506+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:11:50.508+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:11:50.508+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:11:51.004+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:11:51.003+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:11:51.004+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:11:51.004+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:11:51.095+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:11:51.096+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:11:51.096+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:11:51.099+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:11:51.099+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:11:51.100+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:11:51.105+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:11:51.105+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:11:51.105+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:11:51.105+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:11:51.113+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:11:51.113+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:11:51.118+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:11:51.118+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:11:51.119+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:11:51.118+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:11:51.127+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:11:51.127+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:11:51.135+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.639 seconds
[2024-11-25T17:12:21.598+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:12:21.598+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1099)
[2024-11-25T17:12:21.599+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:12:21.599+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1099
[2024-11-25T17:12:21.602+0000] {processor.py:153} INFO - Started process (PID=1099) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:12:21.603+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:12:21.604+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:12:21.604+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:12:21.605+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:12:21.605+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:12:22.175+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:12:22.175+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:12:22.176+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:12:22.176+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:12:22.274+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:12:22.275+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:12:22.275+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:12:22.279+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:12:22.279+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:12:22.279+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:12:22.284+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:12:22.284+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:12:22.284+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:12:22.284+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:12:22.293+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:12:22.293+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:12:22.299+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:12:22.298+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:12:22.299+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:12:22.299+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:12:22.308+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:12:22.308+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:12:22.315+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.717 seconds
[2024-11-25T17:12:52.578+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:12:52.578+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1105)
[2024-11-25T17:12:52.580+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:12:52.580+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1105
[2024-11-25T17:12:52.588+0000] {processor.py:153} INFO - Started process (PID=1105) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:12:52.589+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:12:52.590+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:12:52.590+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:12:52.592+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:12:52.592+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:12:52.948+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:12:52.948+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:12:52.955+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:12:52.949+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:12:53.218+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:12:53.220+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:12:53.220+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:12:53.224+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:12:53.224+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:12:53.224+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:12:53.229+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:12:53.229+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:12:53.230+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:12:53.230+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:12:53.237+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:12:53.237+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:12:53.242+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:12:53.242+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:12:53.243+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:12:53.242+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:12:53.252+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:12:53.252+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:12:53.259+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.681 seconds
[2024-11-25T17:13:23.768+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:13:23.768+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1111)
[2024-11-25T17:13:23.770+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:13:23.770+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1111
[2024-11-25T17:13:23.777+0000] {processor.py:153} INFO - Started process (PID=1111) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:13:23.778+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:13:23.779+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:13:23.779+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:13:23.781+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:13:23.781+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:13:24.310+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:13:24.310+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:13:24.311+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:13:24.311+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:13:24.403+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:13:24.405+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:13:24.405+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:13:24.409+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:13:24.408+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:13:24.409+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:13:24.414+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:13:24.414+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:13:24.414+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:13:24.414+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:13:24.421+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:13:24.421+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:13:24.427+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:13:24.427+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:13:24.427+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:13:24.427+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:13:24.436+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:13:24.436+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:13:24.442+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.675 seconds
[2024-11-25T17:13:54.694+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:13:54.693+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1117)
[2024-11-25T17:13:54.695+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:13:54.695+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1117
[2024-11-25T17:13:54.719+0000] {processor.py:153} INFO - Started process (PID=1117) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:13:54.722+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:13:54.725+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:13:54.724+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:13:54.727+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:13:54.727+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:13:55.230+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:13:55.230+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:13:55.232+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:13:55.231+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:13:55.326+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:13:55.327+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:13:55.327+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:13:55.330+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:13:55.330+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:13:55.330+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:13:55.336+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:13:55.335+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:13:55.336+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:13:55.336+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:13:55.345+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:13:55.344+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:13:55.350+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:13:55.350+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:13:55.351+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:13:55.351+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:13:55.360+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:13:55.360+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:13:55.367+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.674 seconds
[2024-11-25T17:14:25.862+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:14:25.861+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1123)
[2024-11-25T17:14:25.865+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:14:25.864+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1123
[2024-11-25T17:14:25.871+0000] {processor.py:153} INFO - Started process (PID=1123) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:14:25.872+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:14:25.873+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:14:25.873+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:14:25.875+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:14:25.875+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:14:26.368+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:14:26.368+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:14:26.370+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:14:26.369+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:14:26.463+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:14:26.465+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:14:26.464+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:14:26.468+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:14:26.468+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:14:26.468+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:14:26.473+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:14:26.473+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:14:26.473+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:14:26.473+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:14:26.481+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:14:26.481+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:14:26.487+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:14:26.487+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:14:26.488+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:14:26.487+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:14:26.496+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:14:26.496+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:14:26.512+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.651 seconds
[2024-11-25T17:14:56.806+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:14:56.806+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1129)
[2024-11-25T17:14:56.808+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:14:56.807+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1129
[2024-11-25T17:14:56.815+0000] {processor.py:153} INFO - Started process (PID=1129) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:14:56.816+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:14:56.817+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:14:56.816+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:14:56.818+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:14:56.818+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:14:57.329+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:14:57.329+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:14:57.330+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:14:57.329+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:14:57.436+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:14:57.437+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:14:57.437+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:14:57.441+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:14:57.441+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:14:57.441+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:14:57.446+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:14:57.446+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:14:57.447+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:14:57.447+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:14:57.455+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:14:57.455+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:14:57.460+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:14:57.460+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:14:57.461+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:14:57.461+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:14:57.469+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:14:57.469+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:14:57.476+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.671 seconds
[2024-11-25T17:15:28.026+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:15:28.026+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1135)
[2024-11-25T17:15:28.027+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:15:28.026+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1135
[2024-11-25T17:15:28.030+0000] {processor.py:153} INFO - Started process (PID=1135) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:15:28.030+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:15:28.031+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:15:28.031+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:15:28.033+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:15:28.033+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:15:28.508+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:15:28.508+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:15:28.509+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:15:28.509+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:15:28.615+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:15:28.617+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:15:28.617+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:15:28.621+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:15:28.620+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:15:28.621+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:15:28.626+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:15:28.626+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:15:28.626+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:15:28.626+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:15:28.634+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:15:28.634+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:15:28.640+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:15:28.640+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:15:28.640+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:15:28.640+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:15:28.650+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:15:28.650+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:15:28.656+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.631 seconds
[2024-11-25T17:15:58.863+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:15:58.862+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1141)
[2024-11-25T17:15:58.864+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:15:58.864+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1141
[2024-11-25T17:15:58.874+0000] {processor.py:153} INFO - Started process (PID=1141) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:15:58.875+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:15:58.879+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:15:58.879+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:15:58.881+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:15:58.881+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:15:59.412+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:15:59.412+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:15:59.413+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:15:59.412+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:15:59.504+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:15:59.506+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:15:59.505+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:15:59.509+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:15:59.509+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:15:59.509+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:15:59.515+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:15:59.515+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:15:59.515+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:15:59.515+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:15:59.533+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:15:59.533+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:15:59.542+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:15:59.542+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:15:59.542+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:15:59.542+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:15:59.551+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:15:59.551+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:15:59.558+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.696 seconds
[2024-11-25T17:16:30.115+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:16:30.114+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1147)
[2024-11-25T17:16:30.117+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:16:30.117+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1147
[2024-11-25T17:16:30.122+0000] {processor.py:153} INFO - Started process (PID=1147) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:16:30.123+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:16:30.124+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:16:30.124+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:16:30.125+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:16:30.125+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:16:30.617+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:16:30.617+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:16:30.618+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:16:30.618+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:16:30.727+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:16:30.729+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:16:30.729+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:16:30.733+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:16:30.733+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:16:30.733+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:16:30.738+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:16:30.738+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:16:30.738+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:16:30.738+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:16:30.748+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:16:30.748+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:16:30.755+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:16:30.755+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:16:30.755+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:16:30.755+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:16:30.764+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:16:30.764+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:16:30.771+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.657 seconds
[2024-11-25T17:17:01.210+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:17:01.209+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1153)
[2024-11-25T17:17:01.212+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:17:01.212+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1153
[2024-11-25T17:17:01.218+0000] {processor.py:153} INFO - Started process (PID=1153) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:17:01.218+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:17:01.219+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:17:01.219+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:17:01.221+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:17:01.221+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:17:01.775+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:17:01.775+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:17:01.776+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:17:01.776+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:17:01.873+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:17:01.875+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:17:01.875+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:17:01.881+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:17:01.880+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:17:01.881+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:17:01.889+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:17:01.889+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:17:01.890+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:17:01.890+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:17:01.908+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:17:01.907+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:17:01.914+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:17:01.914+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:17:01.914+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:17:01.914+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:17:01.924+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:17:01.923+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:17:01.931+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.721 seconds
[2024-11-25T17:17:32.182+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:17:32.181+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1159)
[2024-11-25T17:17:32.183+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:17:32.183+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1159
[2024-11-25T17:17:32.188+0000] {processor.py:153} INFO - Started process (PID=1159) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:17:32.189+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:17:32.190+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:17:32.190+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:17:32.191+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:17:32.191+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:17:32.685+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:17:32.685+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:17:32.686+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:17:32.686+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:17:32.799+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:17:32.801+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:17:32.801+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:17:32.806+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:17:32.806+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:17:32.806+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:17:32.812+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:17:32.812+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:17:32.812+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:17:32.812+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:17:32.820+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:17:32.820+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:17:32.826+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:17:32.826+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:17:32.827+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:17:32.827+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:17:32.836+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:17:32.836+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:17:32.845+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.663 seconds
[2024-11-25T17:18:03.361+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:18:03.361+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1165)
[2024-11-25T17:18:03.362+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:18:03.362+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1165
[2024-11-25T17:18:03.368+0000] {processor.py:153} INFO - Started process (PID=1165) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:18:03.368+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:18:03.369+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:18:03.369+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:18:03.371+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:18:03.371+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:18:03.904+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:18:03.904+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:18:03.906+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:18:03.905+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:18:04.009+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:18:04.011+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:18:04.010+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:18:04.014+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:18:04.014+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:18:04.014+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:18:04.020+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:18:04.020+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:18:04.020+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:18:04.020+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:18:04.029+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:18:04.029+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:18:04.035+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:18:04.035+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:18:04.035+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:18:04.035+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:18:04.044+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:18:04.044+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:18:04.051+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.690 seconds
[2024-11-25T17:18:34.541+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:18:34.541+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1171)
[2024-11-25T17:18:34.542+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:18:34.542+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1171
[2024-11-25T17:18:34.552+0000] {processor.py:153} INFO - Started process (PID=1171) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:18:34.553+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:18:34.554+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:18:34.554+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:18:34.556+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:18:34.556+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:18:35.062+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:18:35.062+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:18:35.063+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:18:35.063+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:18:35.154+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:18:35.156+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:18:35.156+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:18:35.159+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:18:35.159+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:18:35.159+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:18:35.165+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:18:35.165+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:18:35.165+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:18:35.165+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:18:35.190+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:18:35.190+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:18:35.196+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:18:35.196+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:18:35.197+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:18:35.196+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:18:35.206+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:18:35.205+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:18:35.213+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.673 seconds
[2024-11-25T17:19:05.528+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:19:05.527+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1177)
[2024-11-25T17:19:05.529+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:19:05.529+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1177
[2024-11-25T17:19:05.536+0000] {processor.py:153} INFO - Started process (PID=1177) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:19:05.537+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:19:05.539+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:19:05.539+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:19:05.540+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:19:05.540+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:19:06.045+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:19:06.045+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:19:06.046+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:19:06.045+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:19:06.138+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:19:06.139+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:19:06.139+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:19:06.143+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:19:06.143+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:19:06.143+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:19:06.148+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:19:06.148+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:19:06.148+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:19:06.148+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:19:06.159+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:19:06.159+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:19:06.166+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:19:06.166+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:19:06.166+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:19:06.166+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:19:06.176+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:19:06.176+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:19:06.183+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.656 seconds
[2024-11-25T17:19:36.654+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:19:36.653+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1183)
[2024-11-25T17:19:36.655+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:19:36.655+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1183
[2024-11-25T17:19:36.672+0000] {processor.py:153} INFO - Started process (PID=1183) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:19:36.673+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:19:36.674+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:19:36.674+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:19:36.676+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:19:36.676+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:19:37.003+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:19:37.002+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:19:37.004+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:19:37.003+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:19:37.263+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:19:37.265+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:19:37.265+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:19:37.268+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:19:37.268+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:19:37.269+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:19:37.274+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:19:37.274+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:19:37.274+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:19:37.274+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:19:37.283+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:19:37.283+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:19:37.289+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:19:37.289+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:19:37.289+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:19:37.289+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:19:37.299+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:19:37.299+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:19:37.305+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.653 seconds
[2024-11-25T17:20:07.484+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:20:07.483+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1189)
[2024-11-25T17:20:07.485+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:20:07.485+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1189
[2024-11-25T17:20:07.494+0000] {processor.py:153} INFO - Started process (PID=1189) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:20:07.494+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:20:07.496+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:20:07.495+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:20:07.497+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:20:07.497+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:20:07.997+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:20:07.997+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:20:07.998+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:20:07.998+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:20:08.101+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:20:08.103+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:20:08.103+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:20:08.106+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:20:08.106+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:20:08.106+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:20:08.112+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:20:08.111+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:20:08.112+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:20:08.112+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:20:08.121+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:20:08.121+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:20:08.127+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:20:08.127+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:20:08.128+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:20:08.127+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:20:08.137+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:20:08.137+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:20:08.144+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.661 seconds
[2024-11-25T17:20:38.631+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:20:38.630+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1195)
[2024-11-25T17:20:38.632+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:20:38.632+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1195
[2024-11-25T17:20:38.641+0000] {processor.py:153} INFO - Started process (PID=1195) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:20:38.643+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:20:38.646+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:20:38.646+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:20:38.648+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:20:38.648+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:20:39.168+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:20:39.168+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:20:39.168+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:20:39.168+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:20:39.262+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:20:39.263+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:20:39.263+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:20:39.266+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:20:39.266+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:20:39.266+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:20:39.271+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:20:39.271+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:20:39.272+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:20:39.271+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:20:39.280+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:20:39.279+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:20:39.285+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:20:39.285+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:20:39.286+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:20:39.286+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:20:39.294+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:20:39.294+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:20:39.301+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.671 seconds
[2024-11-25T17:21:09.527+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:21:09.526+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1201)
[2024-11-25T17:21:09.528+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:21:09.527+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1201
[2024-11-25T17:21:09.532+0000] {processor.py:153} INFO - Started process (PID=1201) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:21:09.532+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:21:09.533+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:21:09.533+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:21:09.534+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:21:09.534+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:21:10.116+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:21:10.116+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:21:10.117+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:21:10.116+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:21:10.217+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:21:10.219+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:21:10.219+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:21:10.222+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:21:10.222+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:21:10.223+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:21:10.228+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:21:10.228+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:21:10.228+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:21:10.228+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:21:10.238+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:21:10.238+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:21:10.244+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:21:10.244+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:21:10.244+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:21:10.244+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:21:10.253+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:21:10.253+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:21:10.260+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.733 seconds
[2024-11-25T17:21:40.765+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:21:40.764+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1207)
[2024-11-25T17:21:40.766+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:21:40.766+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1207
[2024-11-25T17:21:40.775+0000] {processor.py:153} INFO - Started process (PID=1207) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:21:40.776+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:21:40.778+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:21:40.778+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:21:40.780+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:21:40.780+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:21:41.284+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:21:41.283+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:21:41.284+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:21:41.284+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:21:41.376+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:21:41.378+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:21:41.378+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:21:41.381+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:21:41.381+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:21:41.381+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:21:41.387+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:21:41.387+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:21:41.387+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:21:41.387+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:21:41.396+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:21:41.396+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:21:41.401+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:21:41.401+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:21:41.402+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:21:41.401+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:21:41.411+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:21:41.411+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:21:41.424+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.660 seconds
[2024-11-25T17:22:11.579+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:22:11.579+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1213)
[2024-11-25T17:22:11.580+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:22:11.580+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1213
[2024-11-25T17:22:11.590+0000] {processor.py:153} INFO - Started process (PID=1213) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:22:11.591+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:22:11.592+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:22:11.592+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:22:11.594+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:22:11.594+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:22:12.168+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:22:12.168+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:22:12.169+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:22:12.168+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:22:12.260+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:22:12.262+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:22:12.262+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:22:12.266+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:22:12.265+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:22:12.266+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:22:12.271+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:22:12.271+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:22:12.271+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:22:12.271+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:22:12.277+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:22:12.277+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:22:12.283+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:22:12.283+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:22:12.283+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:22:12.283+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:22:12.292+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:22:12.292+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:22:12.299+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.720 seconds
[2024-11-25T17:22:42.742+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:22:42.741+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1219)
[2024-11-25T17:22:42.743+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:22:42.743+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1219
[2024-11-25T17:22:42.763+0000] {processor.py:153} INFO - Started process (PID=1219) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:22:42.765+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:22:42.767+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:22:42.767+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:22:42.770+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:22:42.770+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:22:43.277+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:22:43.277+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:22:43.278+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:22:43.278+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:22:43.369+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:22:43.371+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:22:43.371+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:22:43.374+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:22:43.374+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:22:43.375+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:22:43.380+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:22:43.380+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:22:43.380+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:22:43.380+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:22:43.390+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:22:43.390+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:22:43.396+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:22:43.396+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:22:43.396+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:22:43.396+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:22:43.407+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:22:43.406+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:22:43.431+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.690 seconds
[2024-11-25T17:23:13.820+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:23:13.819+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1225)
[2024-11-25T17:23:13.821+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:23:13.821+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1225
[2024-11-25T17:23:13.827+0000] {processor.py:153} INFO - Started process (PID=1225) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:23:13.828+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:23:13.829+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:23:13.829+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:23:13.832+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:23:13.832+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:23:14.357+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:23:14.357+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:23:14.358+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:23:14.358+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:23:14.450+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:23:14.451+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:23:14.451+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:23:14.455+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:23:14.454+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:23:14.455+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:23:14.460+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:23:14.460+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:23:14.460+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:23:14.460+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:23:14.470+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:23:14.470+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:23:14.480+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:23:14.480+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:23:14.481+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:23:14.480+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:23:14.501+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:23:14.500+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:23:14.507+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.689 seconds
[2024-11-25T17:23:44.998+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:23:44.997+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1231)
[2024-11-25T17:23:45.000+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:23:45.000+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1231
[2024-11-25T17:23:45.008+0000] {processor.py:153} INFO - Started process (PID=1231) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:23:45.009+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:23:45.010+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:23:45.010+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:23:45.012+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:23:45.012+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:23:45.511+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:23:45.511+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:23:45.512+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:23:45.512+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:23:45.607+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:23:45.608+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:23:45.608+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:23:45.611+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:23:45.611+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:23:45.611+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:23:45.616+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:23:45.616+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:23:45.617+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:23:45.617+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:23:45.626+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:23:45.626+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:23:45.632+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:23:45.632+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:23:45.633+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:23:45.632+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:23:45.642+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:23:45.642+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:23:45.650+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.653 seconds
[2024-11-25T17:24:15.728+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:24:15.727+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1237)
[2024-11-25T17:24:15.729+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:24:15.729+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1237
[2024-11-25T17:24:15.736+0000] {processor.py:153} INFO - Started process (PID=1237) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:24:15.737+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:24:15.738+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:24:15.738+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:24:15.740+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:24:15.740+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:24:16.248+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:24:16.248+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:24:16.249+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:24:16.249+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:24:16.345+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:24:16.346+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:24:16.346+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:24:16.349+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:24:16.349+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:24:16.349+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:24:16.354+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:24:16.354+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:24:16.355+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:24:16.354+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:24:16.364+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:24:16.364+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:24:16.370+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:24:16.370+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:24:16.370+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:24:16.370+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:24:16.378+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:24:16.378+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:24:16.385+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.658 seconds
[2024-11-25T17:24:46.525+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:24:46.525+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1243)
[2024-11-25T17:24:46.526+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:24:46.526+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1243
[2024-11-25T17:24:46.533+0000] {processor.py:153} INFO - Started process (PID=1243) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:24:46.534+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:24:46.534+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:24:46.534+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:24:46.536+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:24:46.536+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:24:47.041+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:24:47.040+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:24:47.041+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:24:47.041+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:24:47.132+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:24:47.134+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:24:47.134+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:24:47.137+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:24:47.137+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:24:47.137+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:24:47.143+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:24:47.142+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:24:47.143+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:24:47.143+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:24:47.150+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:24:47.150+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:24:47.156+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:24:47.156+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:24:47.156+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:24:47.156+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:24:47.165+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:24:47.165+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:24:47.171+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.647 seconds
[2024-11-25T17:25:17.667+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:25:17.667+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1249)
[2024-11-25T17:25:17.668+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:25:17.668+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1249
[2024-11-25T17:25:17.676+0000] {processor.py:153} INFO - Started process (PID=1249) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:25:17.678+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:25:17.679+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:25:17.679+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:25:17.681+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:25:17.681+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:25:18.207+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:25:18.207+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:25:18.207+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:25:18.207+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:25:18.299+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:25:18.300+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:25:18.300+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:25:18.304+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:25:18.303+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:25:18.304+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:25:18.309+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:25:18.309+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:25:18.309+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:25:18.309+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:25:18.316+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:25:18.316+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:25:18.322+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:25:18.322+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:25:18.322+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:25:18.322+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:25:18.331+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:25:18.331+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:25:18.338+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.672 seconds
[2024-11-25T17:25:48.698+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:25:48.697+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1255)
[2024-11-25T17:25:48.699+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:25:48.699+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1255
[2024-11-25T17:25:48.708+0000] {processor.py:153} INFO - Started process (PID=1255) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:25:48.709+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:25:48.710+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:25:48.710+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:25:48.711+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:25:48.711+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:25:49.243+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:25:49.243+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:25:49.244+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:25:49.244+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:25:49.334+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:25:49.335+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:25:49.335+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:25:49.339+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:25:49.339+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:25:49.339+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:25:49.344+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:25:49.344+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:25:49.344+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:25:49.344+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:25:49.354+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:25:49.354+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:25:49.359+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:25:49.359+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:25:49.360+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:25:49.360+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:25:49.369+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:25:49.369+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:25:49.375+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.678 seconds
[2024-11-25T17:26:19.916+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:26:19.915+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1261)
[2024-11-25T17:26:19.917+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:26:19.917+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1261
[2024-11-25T17:26:19.924+0000] {processor.py:153} INFO - Started process (PID=1261) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:26:19.925+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:26:19.926+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:26:19.926+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:26:19.928+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:26:19.928+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:26:20.452+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:26:20.452+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:26:20.453+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:26:20.453+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:26:20.547+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:26:20.548+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:26:20.548+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:26:20.552+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:26:20.551+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:26:20.552+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:26:20.557+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:26:20.557+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:26:20.557+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:26:20.557+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:26:20.575+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:26:20.575+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:26:20.590+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:26:20.590+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:26:20.590+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:26:20.590+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:26:20.606+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:26:20.606+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:26:20.614+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.699 seconds
[2024-11-25T17:26:50.716+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:26:50.715+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1267)
[2024-11-25T17:26:50.717+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:26:50.717+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1267
[2024-11-25T17:26:50.725+0000] {processor.py:153} INFO - Started process (PID=1267) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:26:50.726+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:26:50.727+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:26:50.727+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:26:50.729+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:26:50.729+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:26:51.279+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:26:51.279+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:26:51.280+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:26:51.279+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:26:51.371+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:26:51.373+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:26:51.373+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:26:51.376+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:26:51.376+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:26:51.376+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:26:51.381+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:26:51.381+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:26:51.382+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:26:51.382+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:26:51.392+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:26:51.392+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:26:51.398+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:26:51.398+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:26:51.398+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:26:51.398+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:26:51.429+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:26:51.429+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:26:51.436+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.721 seconds
[2024-11-25T17:27:21.926+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:27:21.926+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1273)
[2024-11-25T17:27:21.927+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:27:21.927+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1273
[2024-11-25T17:27:21.962+0000] {processor.py:153} INFO - Started process (PID=1273) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:27:21.963+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:27:21.964+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:27:21.964+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:27:21.966+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:27:21.966+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:27:22.494+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:27:22.494+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:27:22.495+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:27:22.494+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:27:22.587+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:27:22.589+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:27:22.589+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:27:22.593+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:27:22.592+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:27:22.593+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:27:22.598+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:27:22.598+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:27:22.599+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:27:22.599+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:27:22.624+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:27:22.624+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:27:22.631+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:27:22.631+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:27:22.631+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:27:22.631+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:27:22.640+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:27:22.640+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:27:22.647+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.722 seconds
[2024-11-25T17:27:53.064+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:27:53.064+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1279)
[2024-11-25T17:27:53.065+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:27:53.065+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1279
[2024-11-25T17:27:53.070+0000] {processor.py:153} INFO - Started process (PID=1279) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:27:53.070+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:27:53.071+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:27:53.071+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:27:53.072+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:27:53.072+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:27:53.584+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:27:53.584+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:27:53.585+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:27:53.584+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:27:53.676+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:27:53.678+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:27:53.678+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:27:53.681+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:27:53.681+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:27:53.681+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:27:53.687+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:27:53.687+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:27:53.687+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:27:53.687+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:27:53.695+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:27:53.695+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:27:53.701+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:27:53.701+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:27:53.701+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:27:53.701+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:27:53.710+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:27:53.710+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:27:53.717+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.654 seconds
[2024-11-25T17:28:24.202+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:28:24.201+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1285)
[2024-11-25T17:28:24.204+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:28:24.204+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1285
[2024-11-25T17:28:24.212+0000] {processor.py:153} INFO - Started process (PID=1285) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:28:24.213+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:28:24.214+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:28:24.213+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:28:24.215+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:28:24.215+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:28:24.725+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:28:24.725+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:28:24.726+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:28:24.725+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:28:24.816+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:28:24.818+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:28:24.817+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:28:24.821+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:28:24.821+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:28:24.821+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:28:24.827+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:28:24.826+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:28:24.827+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:28:24.827+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:28:24.834+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:28:24.834+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:28:24.840+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:28:24.840+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:28:24.840+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:28:24.840+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:28:24.849+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:28:24.849+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:28:24.857+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.656 seconds
[2024-11-25T17:28:54.940+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:28:54.939+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1291)
[2024-11-25T17:28:54.941+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:28:54.941+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1291
[2024-11-25T17:28:54.951+0000] {processor.py:153} INFO - Started process (PID=1291) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:28:54.952+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:28:54.953+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:28:54.953+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:28:54.955+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:28:54.955+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:28:55.464+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:28:55.464+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:28:55.465+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:28:55.464+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:28:55.568+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:28:55.569+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:28:55.569+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:28:55.573+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:28:55.573+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:28:55.573+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:28:55.578+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:28:55.578+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:28:55.578+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:28:55.578+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:28:55.587+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:28:55.587+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:28:55.593+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:28:55.593+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:28:55.593+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:28:55.593+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:28:55.602+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:28:55.602+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:28:55.609+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.670 seconds
[2024-11-25T17:29:25.824+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:29:25.824+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1297)
[2024-11-25T17:29:25.825+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:29:25.825+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1297
[2024-11-25T17:29:25.830+0000] {processor.py:153} INFO - Started process (PID=1297) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:29:25.831+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:29:25.834+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:29:25.834+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:29:25.837+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:29:25.837+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:29:26.397+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:29:26.397+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:29:26.398+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:29:26.398+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:29:26.498+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:29:26.500+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:29:26.500+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:29:26.504+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:29:26.504+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:29:26.504+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:29:26.510+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:29:26.510+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:29:26.510+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:29:26.510+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:29:26.518+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:29:26.518+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:29:26.524+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:29:26.524+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:29:26.524+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:29:26.524+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:29:26.534+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:29:26.534+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:29:26.540+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.716 seconds
[2024-11-25T17:29:57.029+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:29:57.029+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1303)
[2024-11-25T17:29:57.030+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:29:57.030+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1303
[2024-11-25T17:29:57.036+0000] {processor.py:153} INFO - Started process (PID=1303) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:29:57.037+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:29:57.038+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:29:57.038+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:29:57.040+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:29:57.040+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:29:57.549+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:29:57.548+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:29:57.549+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:29:57.549+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:29:57.661+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:29:57.663+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:29:57.663+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:29:57.667+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:29:57.667+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:29:57.667+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:29:57.673+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:29:57.673+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:29:57.673+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:29:57.673+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:29:57.682+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:29:57.682+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:29:57.699+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:29:57.699+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:29:57.700+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:29:57.699+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:29:57.711+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:29:57.711+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:29:57.719+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.690 seconds
[2024-11-25T17:30:27.883+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:30:27.883+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1309)
[2024-11-25T17:30:27.884+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:30:27.884+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1309
[2024-11-25T17:30:27.890+0000] {processor.py:153} INFO - Started process (PID=1309) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:30:27.891+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:30:27.892+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:30:27.892+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:30:27.894+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:30:27.893+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:30:28.394+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:30:28.394+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:30:28.394+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:30:28.394+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:30:28.498+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:30:28.499+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:30:28.499+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:30:28.503+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:30:28.503+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:30:28.503+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:30:28.508+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:30:28.508+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:30:28.508+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:30:28.508+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:30:28.516+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:30:28.516+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:30:28.522+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:30:28.522+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:30:28.523+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:30:28.522+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:30:28.531+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:30:28.531+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:30:28.537+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.655 seconds
[2024-11-25T17:30:58.606+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:30:58.606+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1315)
[2024-11-25T17:30:58.607+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:30:58.607+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1315
[2024-11-25T17:30:58.613+0000] {processor.py:153} INFO - Started process (PID=1315) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:30:58.614+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:30:58.615+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:30:58.615+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:30:58.617+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:30:58.617+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:30:59.134+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:30:59.134+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:30:59.135+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:30:59.135+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:30:59.228+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:30:59.230+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:30:59.229+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:30:59.233+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:30:59.233+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:30:59.233+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:30:59.238+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:30:59.238+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:30:59.238+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:30:59.238+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:30:59.246+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:30:59.246+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:30:59.252+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:30:59.252+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:30:59.252+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:30:59.252+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:30:59.261+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:30:59.261+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:30:59.267+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.662 seconds
[2024-11-25T17:31:29.488+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:31:29.487+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1321)
[2024-11-25T17:31:29.490+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:31:29.490+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1321
[2024-11-25T17:31:29.495+0000] {processor.py:153} INFO - Started process (PID=1321) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:31:29.496+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:31:29.497+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:31:29.497+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:31:29.499+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:31:29.499+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:31:30.006+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:31:30.006+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:31:30.006+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:31:30.006+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:31:30.097+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:31:30.098+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:31:30.098+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:31:30.101+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:31:30.101+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:31:30.101+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:31:30.107+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:31:30.107+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:31:30.107+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:31:30.107+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:31:30.114+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:31:30.114+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:31:30.119+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:31:30.119+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:31:30.120+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:31:30.119+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:31:30.128+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:31:30.128+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:31:30.136+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.649 seconds
[2024-11-25T17:32:00.604+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:32:00.604+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1327)
[2024-11-25T17:32:00.605+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:32:00.605+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1327
[2024-11-25T17:32:00.611+0000] {processor.py:153} INFO - Started process (PID=1327) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:32:00.611+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:32:00.612+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:32:00.612+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:32:00.614+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:32:00.614+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:32:01.132+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:32:01.132+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:32:01.133+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:32:01.133+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:32:01.223+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:32:01.224+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:32:01.224+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:32:01.228+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:32:01.228+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:32:01.228+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:32:01.233+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:32:01.233+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:32:01.234+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:32:01.234+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:32:01.254+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:32:01.254+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:32:01.260+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:32:01.260+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:32:01.260+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:32:01.260+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:32:01.269+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:32:01.269+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:32:01.276+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.676 seconds
[2024-11-25T17:32:31.520+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:32:31.519+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1333)
[2024-11-25T17:32:31.520+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:32:31.520+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1333
[2024-11-25T17:32:31.526+0000] {processor.py:153} INFO - Started process (PID=1333) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:32:31.526+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:32:31.527+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:32:31.527+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:32:31.528+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:32:31.528+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:32:32.121+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:32:32.121+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:32:32.121+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:32:32.121+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:32:32.223+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:32:32.225+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:32:32.225+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:32:32.228+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:32:32.228+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:32:32.229+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:32:32.234+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:32:32.234+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:32:32.234+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:32:32.234+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:32:32.244+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:32:32.244+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:32:32.250+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:32:32.250+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:32:32.250+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:32:32.250+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:32:32.261+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:32:32.261+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:32:32.268+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.749 seconds
[2024-11-25T17:33:02.772+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:33:02.772+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1339)
[2024-11-25T17:33:02.773+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:33:02.773+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1339
[2024-11-25T17:33:02.779+0000] {processor.py:153} INFO - Started process (PID=1339) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:33:02.780+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:33:02.780+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:33:02.780+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:33:02.782+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:33:02.782+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:33:03.273+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:33:03.273+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:33:03.274+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:33:03.273+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:33:03.377+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:33:03.378+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:33:03.378+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:33:03.382+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:33:03.382+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:33:03.382+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:33:03.387+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:33:03.387+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:33:03.387+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:33:03.387+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:33:03.396+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:33:03.396+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:33:03.402+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:33:03.401+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:33:03.402+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:33:03.402+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:33:03.411+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:33:03.411+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:33:03.418+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.647 seconds
[2024-11-25T17:33:33.635+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:33:33.635+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1345)
[2024-11-25T17:33:33.636+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:33:33.636+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1345
[2024-11-25T17:33:33.643+0000] {processor.py:153} INFO - Started process (PID=1345) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:33:33.644+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:33:33.645+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:33:33.645+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:33:33.647+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:33:33.647+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:33:34.159+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:33:34.159+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:33:34.160+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:33:34.160+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:33:34.254+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:33:34.255+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:33:34.255+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:33:34.259+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:33:34.259+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:33:34.259+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:33:34.264+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:33:34.264+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:33:34.264+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:33:34.264+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:33:34.287+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:33:34.287+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:33:34.294+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:33:34.294+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:33:34.295+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:33:34.295+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:33:34.304+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:33:34.304+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:33:34.311+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.677 seconds
[2024-11-25T17:34:04.722+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:34:04.721+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1351)
[2024-11-25T17:34:04.723+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:34:04.723+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1351
[2024-11-25T17:34:04.731+0000] {processor.py:153} INFO - Started process (PID=1351) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:34:04.732+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:34:04.733+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:34:04.733+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:34:04.735+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:34:04.735+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:34:05.250+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:34:05.249+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:34:05.250+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:34:05.250+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:34:05.359+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:34:05.361+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:34:05.361+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:34:05.364+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:34:05.364+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:34:05.364+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:34:05.370+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:34:05.370+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:34:05.370+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:34:05.370+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:34:05.379+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:34:05.379+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:34:05.386+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:34:05.386+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:34:05.386+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:34:05.386+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:34:05.395+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:34:05.395+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:34:05.402+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.681 seconds
[2024-11-25T17:34:35.751+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:34:35.750+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1357)
[2024-11-25T17:34:35.752+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:34:35.751+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1357
[2024-11-25T17:34:35.768+0000] {processor.py:153} INFO - Started process (PID=1357) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:34:35.768+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:34:35.770+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:34:35.770+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:34:35.771+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:34:35.771+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:34:36.269+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:34:36.269+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:34:36.270+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:34:36.270+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:34:36.361+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:34:36.362+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:34:36.362+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:34:36.366+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:34:36.365+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:34:36.366+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:34:36.371+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:34:36.371+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:34:36.372+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:34:36.372+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:34:36.382+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:34:36.381+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:34:36.387+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:34:36.387+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:34:36.388+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:34:36.387+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:34:36.396+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:34:36.396+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:34:36.403+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.654 seconds
[2024-11-25T17:35:06.911+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:35:06.911+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1363)
[2024-11-25T17:35:06.912+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:35:06.912+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1363
[2024-11-25T17:35:06.933+0000] {processor.py:153} INFO - Started process (PID=1363) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:35:06.937+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:35:06.944+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:35:06.944+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:35:06.946+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:35:06.946+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:35:07.432+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:35:07.432+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:35:07.433+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:35:07.433+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:35:07.543+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:35:07.546+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:35:07.545+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:35:07.549+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:35:07.549+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:35:07.549+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:35:07.555+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:35:07.555+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:35:07.555+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:35:07.555+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:35:07.563+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:35:07.563+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:35:07.569+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:35:07.569+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:35:07.569+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:35:07.569+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:35:07.578+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:35:07.578+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:35:07.586+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.676 seconds
[2024-11-25T17:35:38.083+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:35:38.083+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1369)
[2024-11-25T17:35:38.085+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:35:38.085+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1369
[2024-11-25T17:35:38.094+0000] {processor.py:153} INFO - Started process (PID=1369) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:35:38.095+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:35:38.096+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:35:38.096+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:35:38.098+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:35:38.098+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:35:38.612+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:35:38.612+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:35:38.612+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:35:38.612+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:35:38.705+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:35:38.706+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:35:38.706+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:35:38.710+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:35:38.710+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:35:38.710+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:35:38.715+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:35:38.715+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:35:38.715+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:35:38.715+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:35:38.724+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:35:38.724+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:35:38.729+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:35:38.729+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:35:38.730+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:35:38.730+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:35:38.739+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:35:38.739+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:35:38.746+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.664 seconds
[2024-11-25T17:36:09.078+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:36:09.077+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1375)
[2024-11-25T17:36:09.079+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:36:09.079+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1375
[2024-11-25T17:36:09.088+0000] {processor.py:153} INFO - Started process (PID=1375) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:36:09.089+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:36:09.090+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:36:09.090+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:36:09.092+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:36:09.092+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:36:09.645+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:36:09.645+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:36:09.645+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:36:09.645+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:36:09.736+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:36:09.738+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:36:09.738+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:36:09.742+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:36:09.741+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:36:09.742+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:36:09.747+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:36:09.747+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:36:09.747+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:36:09.747+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:36:09.756+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:36:09.756+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:36:09.762+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:36:09.762+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:36:09.762+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:36:09.762+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:36:09.771+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:36:09.771+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:36:09.783+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.705 seconds
[2024-11-25T17:36:40.272+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:36:40.272+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1381)
[2024-11-25T17:36:40.273+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:36:40.273+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1381
[2024-11-25T17:36:40.278+0000] {processor.py:153} INFO - Started process (PID=1381) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:36:40.279+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:36:40.280+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:36:40.279+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:36:40.281+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:36:40.281+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:36:40.818+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:36:40.817+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:36:40.818+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:36:40.818+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:36:40.908+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:36:40.909+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:36:40.909+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:36:40.913+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:36:40.912+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:36:40.913+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:36:40.918+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:36:40.918+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:36:40.918+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:36:40.918+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:36:40.925+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:36:40.925+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:36:40.931+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:36:40.931+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:36:40.931+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:36:40.931+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:36:40.940+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:36:40.940+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:36:40.947+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.675 seconds
[2024-11-25T17:37:11.173+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:37:11.172+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1387)
[2024-11-25T17:37:11.174+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:37:11.174+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1387
[2024-11-25T17:37:11.180+0000] {processor.py:153} INFO - Started process (PID=1387) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:37:11.181+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:37:11.182+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:37:11.181+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:37:11.184+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:37:11.184+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:37:11.705+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:37:11.705+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:37:11.706+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:37:11.706+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:37:11.808+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:37:11.810+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:37:11.810+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:37:11.813+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:37:11.813+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:37:11.813+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:37:11.819+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:37:11.819+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:37:11.819+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:37:11.819+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:37:11.831+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:37:11.831+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:37:11.838+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:37:11.838+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:37:11.838+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:37:11.838+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:37:11.849+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:37:11.849+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:37:11.857+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.685 seconds
[2024-11-25T17:37:42.267+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:37:42.267+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1393)
[2024-11-25T17:37:42.269+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:37:42.269+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1393
[2024-11-25T17:37:42.277+0000] {processor.py:153} INFO - Started process (PID=1393) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:37:42.278+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:37:42.279+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:37:42.279+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:37:42.281+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:37:42.281+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:37:42.879+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:37:42.878+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:37:42.879+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:37:42.879+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:37:42.980+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:37:42.981+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:37:42.981+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:37:42.985+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:37:42.984+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:37:42.985+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:37:42.990+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:37:42.990+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:37:42.990+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:37:42.990+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:37:43.001+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:37:43.001+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:37:43.012+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:37:43.012+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:37:43.013+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:37:43.012+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:37:43.033+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:37:43.033+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:37:43.042+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.775 seconds
[2024-11-25T17:38:13.169+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:38:13.169+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1399)
[2024-11-25T17:38:13.170+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:38:13.170+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1399
[2024-11-25T17:38:13.175+0000] {processor.py:153} INFO - Started process (PID=1399) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:38:13.176+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:38:13.177+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:38:13.177+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:38:13.179+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:38:13.179+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:38:13.757+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:38:13.757+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:38:13.758+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:38:13.757+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:38:13.880+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:38:13.882+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:38:13.882+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:38:13.892+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:38:13.891+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:38:13.892+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:38:13.900+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:38:13.900+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:38:13.900+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:38:13.900+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:38:13.912+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:38:13.912+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:38:13.919+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:38:13.919+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:38:13.919+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:38:13.919+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:38:13.930+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:38:13.930+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:38:13.938+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.770 seconds
[2024-11-25T17:38:44.446+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:38:44.446+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1405)
[2024-11-25T17:38:44.447+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:38:44.447+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1405
[2024-11-25T17:38:44.453+0000] {processor.py:153} INFO - Started process (PID=1405) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:38:44.455+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:38:44.460+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:38:44.460+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:38:44.466+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:38:44.466+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:38:45.156+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:38:45.156+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:38:45.158+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:38:45.157+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:38:45.366+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:38:45.369+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:38:45.369+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:38:45.374+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:38:45.374+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:38:45.374+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:38:45.382+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:38:45.382+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:38:45.382+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:38:45.382+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:38:45.399+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:38:45.399+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:38:45.408+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:38:45.408+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:38:45.410+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:38:45.409+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:38:45.424+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:38:45.424+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:38:45.435+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.990 seconds
[2024-11-25T17:39:15.938+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:39:15.937+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1411)
[2024-11-25T17:39:15.943+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:39:15.942+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1411
[2024-11-25T17:39:15.962+0000] {processor.py:153} INFO - Started process (PID=1411) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:39:15.963+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:39:15.963+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:39:15.963+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:39:15.965+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:39:15.965+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:39:16.617+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:39:16.617+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:39:16.618+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:39:16.617+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:39:16.782+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:39:16.785+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:39:16.785+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:39:16.790+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:39:16.789+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:39:16.790+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:39:16.797+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:39:16.797+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:39:16.797+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:39:16.797+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:39:16.813+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:39:16.813+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:39:16.821+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:39:16.821+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:39:16.822+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:39:16.821+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:39:16.849+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:39:16.849+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:39:16.859+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.924 seconds
[2024-11-25T17:39:47.388+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:39:47.387+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1417)
[2024-11-25T17:39:47.389+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:39:47.388+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1417
[2024-11-25T17:39:47.394+0000] {processor.py:153} INFO - Started process (PID=1417) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:39:47.395+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:39:47.396+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:39:47.396+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:39:47.397+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:39:47.397+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:39:48.533+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:39:48.533+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:39:48.534+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:39:48.534+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:39:48.667+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:39:48.669+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:39:48.669+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:39:48.673+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:39:48.673+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:39:48.673+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:39:48.689+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:39:48.689+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:39:48.690+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:39:48.689+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:39:48.712+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:39:48.712+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:39:48.724+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:39:48.724+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:39:48.725+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:39:48.724+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:39:48.740+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:39:48.740+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:39:48.751+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 1.364 seconds
[2024-11-25T17:40:19.391+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:40:19.391+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1423)
[2024-11-25T17:40:19.392+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:40:19.392+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1423
[2024-11-25T17:40:19.400+0000] {processor.py:153} INFO - Started process (PID=1423) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:40:19.402+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:40:19.403+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:40:19.403+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:40:19.404+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:40:19.404+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:40:20.084+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:40:20.082+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:40:20.096+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:40:20.092+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:40:20.260+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:40:20.263+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:40:20.263+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:40:20.269+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:40:20.269+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:40:20.269+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:40:20.286+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:40:20.286+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:40:20.287+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:40:20.287+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:40:20.325+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:40:20.325+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:40:20.346+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:40:20.346+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:40:20.348+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:40:20.347+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:40:20.378+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:40:20.378+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:40:20.394+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 1.003 seconds
[2024-11-25T17:40:50.961+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:40:50.960+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1429)
[2024-11-25T17:40:50.965+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:40:50.964+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1429
[2024-11-25T17:40:50.972+0000] {processor.py:153} INFO - Started process (PID=1429) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:40:50.972+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:40:50.973+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:40:50.973+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:40:50.977+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:40:50.977+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:40:51.646+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:40:51.646+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:40:51.647+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:40:51.646+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:40:51.808+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:40:51.811+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:40:51.811+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:40:51.818+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:40:51.818+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:40:51.818+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:40:51.829+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:40:51.829+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:40:51.830+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:40:51.830+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:40:51.854+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:40:51.854+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:40:51.876+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:40:51.876+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:40:51.878+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:40:51.877+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:40:51.904+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:40:51.904+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:40:51.917+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.957 seconds
[2024-11-25T17:41:22.575+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:41:22.574+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1435)
[2024-11-25T17:41:22.576+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:41:22.576+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1435
[2024-11-25T17:41:22.594+0000] {processor.py:153} INFO - Started process (PID=1435) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:41:22.597+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:41:22.601+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:41:22.601+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:41:22.603+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:41:22.603+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:41:23.102+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:41:23.102+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:41:23.104+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:41:23.104+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:41:23.204+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:41:23.206+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:41:23.206+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:41:23.210+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:41:23.210+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:41:23.210+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:41:23.216+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:41:23.216+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:41:23.216+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:41:23.216+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:41:23.224+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:41:23.224+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:41:23.229+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:41:23.229+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:41:23.230+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:41:23.230+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:41:23.238+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:41:23.238+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:41:23.246+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.672 seconds
[2024-11-25T17:41:53.741+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:41:53.740+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1441)
[2024-11-25T17:41:53.742+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:41:53.741+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1441
[2024-11-25T17:41:53.748+0000] {processor.py:153} INFO - Started process (PID=1441) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:41:53.749+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:41:53.750+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:41:53.750+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:41:53.752+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:41:53.752+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:41:54.236+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:41:54.236+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:41:54.237+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:41:54.236+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:41:54.375+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:41:54.376+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:41:54.376+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:41:54.380+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:41:54.379+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:41:54.380+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:41:54.385+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:41:54.385+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:41:54.385+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:41:54.385+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:41:54.393+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:41:54.393+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:41:54.399+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:41:54.399+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:41:54.400+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:41:54.399+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:41:54.439+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:41:54.439+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:41:54.460+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.720 seconds
[2024-11-25T17:42:24.632+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:42:24.632+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1447)
[2024-11-25T17:42:24.633+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:42:24.633+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1447
[2024-11-25T17:42:24.637+0000] {processor.py:153} INFO - Started process (PID=1447) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:42:24.638+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:42:24.643+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:42:24.642+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:42:24.645+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:42:24.645+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:42:25.167+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:42:25.167+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:42:25.168+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:42:25.167+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:42:25.269+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:42:25.270+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:42:25.270+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:42:25.274+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:42:25.274+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:42:25.274+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:42:25.280+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:42:25.280+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:42:25.281+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:42:25.281+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:42:25.291+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:42:25.291+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:42:25.298+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:42:25.298+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:42:25.299+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:42:25.298+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:42:25.324+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:42:25.323+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:42:25.337+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.705 seconds
[2024-11-25T17:42:55.396+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:42:55.396+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1453)
[2024-11-25T17:42:55.398+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:42:55.398+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1453
[2024-11-25T17:42:55.403+0000] {processor.py:153} INFO - Started process (PID=1453) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:42:55.404+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:42:55.405+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:42:55.405+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:42:55.406+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:42:55.406+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:42:55.922+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:42:55.922+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:42:55.923+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:42:55.923+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:42:56.046+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:42:56.049+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:42:56.049+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:42:56.053+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:42:56.053+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:42:56.054+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:42:56.059+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:42:56.059+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:42:56.060+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:42:56.060+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:42:56.069+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:42:56.069+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:42:56.081+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:42:56.080+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:42:56.082+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:42:56.081+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:42:56.116+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:42:56.115+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:42:56.136+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.741 seconds
[2024-11-25T17:43:26.319+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:43:26.319+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1459)
[2024-11-25T17:43:26.320+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:43:26.320+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1459
[2024-11-25T17:43:26.328+0000] {processor.py:153} INFO - Started process (PID=1459) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:43:26.328+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:43:26.329+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:43:26.329+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:43:26.331+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:43:26.331+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:43:26.859+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:43:26.859+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:43:26.860+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:43:26.860+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:43:26.996+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:43:26.998+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:43:26.998+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:43:27.005+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:43:27.005+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:43:27.005+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:43:27.019+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:43:27.019+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:43:27.019+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:43:27.019+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:43:27.044+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:43:27.044+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:43:27.058+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:43:27.058+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:43:27.060+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:43:27.059+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:43:27.092+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:43:27.092+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:43:27.104+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.785 seconds
[2024-11-25T17:43:57.154+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:43:57.153+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1465)
[2024-11-25T17:43:57.155+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:43:57.155+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1465
[2024-11-25T17:43:57.161+0000] {processor.py:153} INFO - Started process (PID=1465) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:43:57.162+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:43:57.163+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:43:57.163+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:43:57.165+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:43:57.165+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:43:57.653+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:43:57.653+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:43:57.653+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:43:57.653+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:43:57.745+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:43:57.747+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:43:57.747+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:43:57.750+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:43:57.750+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:43:57.750+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:43:57.755+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:43:57.755+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:43:57.756+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:43:57.756+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:43:57.765+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:43:57.765+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:43:57.771+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:43:57.771+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:43:57.771+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:43:57.771+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:43:57.780+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:43:57.780+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:43:57.787+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.634 seconds
[2024-11-25T17:44:27.995+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:44:27.994+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1471)
[2024-11-25T17:44:27.997+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:44:27.997+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1471
[2024-11-25T17:44:28.002+0000] {processor.py:153} INFO - Started process (PID=1471) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:44:28.003+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:44:28.004+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:44:28.004+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:44:28.006+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:44:28.006+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:44:28.960+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:44:28.960+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:44:28.962+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:44:28.962+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:44:29.133+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:44:29.135+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:44:29.135+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:44:29.139+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:44:29.139+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:44:29.139+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:44:29.146+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:44:29.146+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:44:29.146+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:44:29.146+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:44:29.171+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:44:29.171+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:44:29.179+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:44:29.179+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:44:29.179+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:44:29.179+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:44:29.190+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:44:29.190+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:44:29.199+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 1.205 seconds
[2024-11-25T17:44:59.658+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:44:59.657+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1477)
[2024-11-25T17:44:59.659+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:44:59.659+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1477
[2024-11-25T17:44:59.666+0000] {processor.py:153} INFO - Started process (PID=1477) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:44:59.666+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:44:59.667+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:44:59.667+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:44:59.669+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:44:59.669+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:45:00.545+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:45:00.544+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:45:00.546+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:45:00.545+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:45:00.672+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:45:00.674+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:45:00.674+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:45:00.678+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:45:00.678+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:45:00.678+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:45:00.685+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:45:00.685+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:45:00.685+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:45:00.685+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:45:00.694+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:45:00.694+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:45:00.701+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:45:00.701+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:45:00.701+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:45:00.701+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:45:00.711+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:45:00.711+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:45:00.718+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 1.062 seconds
[2024-11-25T17:45:31.265+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:45:31.265+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1483)
[2024-11-25T17:45:31.267+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:45:31.267+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1483
[2024-11-25T17:45:31.272+0000] {processor.py:153} INFO - Started process (PID=1483) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:45:31.273+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:45:31.274+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:45:31.274+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:45:31.275+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:45:31.275+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:45:32.010+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:45:32.009+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:45:32.012+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:45:32.011+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:45:32.233+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:45:32.235+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:45:32.235+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:45:32.240+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:45:32.240+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:45:32.240+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:45:32.248+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:45:32.248+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:45:32.248+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:45:32.248+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:45:32.278+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:45:32.278+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:45:32.293+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:45:32.293+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:45:32.294+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:45:32.293+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:45:32.316+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:45:32.316+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:45:32.329+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 1.064 seconds
[2024-11-25T17:46:02.891+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:46:02.890+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1489)
[2024-11-25T17:46:02.892+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:46:02.892+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1489
[2024-11-25T17:46:02.901+0000] {processor.py:153} INFO - Started process (PID=1489) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:46:02.902+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:46:02.903+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:46:02.903+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:46:02.905+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:46:02.905+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:46:03.412+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:46:03.412+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:46:03.413+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:46:03.412+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:46:03.506+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:46:03.508+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:46:03.507+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:46:03.511+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:46:03.511+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:46:03.511+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:46:03.520+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:46:03.520+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:46:03.520+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:46:03.520+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:46:03.536+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:46:03.535+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:46:03.542+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:46:03.542+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:46:03.542+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:46:03.542+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:46:03.553+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:46:03.553+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:46:03.560+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.670 seconds
[2024-11-25T17:46:34.171+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:46:34.171+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1495)
[2024-11-25T17:46:34.172+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:46:34.172+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1495
[2024-11-25T17:46:34.183+0000] {processor.py:153} INFO - Started process (PID=1495) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:46:34.184+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:46:34.185+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:46:34.185+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:46:34.188+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:46:34.188+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:46:34.940+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:46:34.940+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:46:34.941+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:46:34.941+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:46:35.055+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:46:35.056+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:46:35.056+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:46:35.060+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:46:35.060+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:46:35.060+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:46:35.066+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:46:35.066+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:46:35.066+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:46:35.066+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:46:35.079+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:46:35.079+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:46:35.089+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:46:35.089+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:46:35.090+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:46:35.090+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:46:35.107+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:46:35.107+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:46:35.118+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.947 seconds
[2024-11-25T17:47:05.440+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:47:05.440+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1501)
[2024-11-25T17:47:05.441+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:47:05.441+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1501
[2024-11-25T17:47:05.449+0000] {processor.py:153} INFO - Started process (PID=1501) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:47:05.450+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:47:05.451+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:47:05.451+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:47:05.453+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:47:05.453+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:47:05.966+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:47:05.966+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:47:05.967+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:47:05.967+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:47:06.057+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:47:06.059+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:47:06.059+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:47:06.062+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:47:06.062+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:47:06.063+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:47:06.068+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:47:06.067+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:47:06.068+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:47:06.068+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:47:06.078+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:47:06.078+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:47:06.084+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:47:06.084+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:47:06.084+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:47:06.084+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:47:06.103+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:47:06.103+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:47:06.110+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.671 seconds
[2024-11-25T17:47:36.741+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:47:36.737+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1507)
[2024-11-25T17:47:36.744+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:47:36.744+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1507
[2024-11-25T17:47:36.753+0000] {processor.py:153} INFO - Started process (PID=1507) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:47:36.754+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:47:36.756+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:47:36.756+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:47:36.759+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:47:36.759+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:47:37.517+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:47:37.517+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:47:37.519+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:47:37.518+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:47:37.730+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:47:37.877+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:47:37.877+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:47:37.895+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:47:37.894+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:47:37.895+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:47:37.913+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:47:37.913+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:47:37.914+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:47:37.914+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:47:37.932+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:47:37.931+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:47:37.941+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:47:37.941+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:47:37.941+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:47:37.941+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:47:37.952+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:47:37.952+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:47:37.961+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 1.230 seconds
[2024-11-25T17:48:08.689+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:48:08.688+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1513)
[2024-11-25T17:48:08.693+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:48:08.693+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1513
[2024-11-25T17:48:08.702+0000] {processor.py:153} INFO - Started process (PID=1513) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:48:08.703+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:48:08.704+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:48:08.704+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:48:08.708+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:48:08.707+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:48:09.704+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:48:09.703+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:48:09.704+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:48:09.704+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:48:09.852+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:48:09.857+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:48:09.856+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:48:09.868+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:48:09.867+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:48:09.868+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:48:09.877+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:48:09.877+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:48:09.877+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:48:09.877+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:48:09.898+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:48:09.898+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:48:09.906+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:48:09.906+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:48:09.907+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:48:09.907+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:48:09.920+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:48:09.920+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:48:09.929+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 1.241 seconds
[2024-11-25T17:48:40.262+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:48:40.261+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1519)
[2024-11-25T17:48:40.262+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:48:40.262+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1519
[2024-11-25T17:48:40.265+0000] {processor.py:153} INFO - Started process (PID=1519) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:48:40.266+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:48:40.266+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:48:40.266+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:48:40.268+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:48:40.268+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:48:40.867+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:48:40.867+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:48:40.868+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:48:40.868+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:48:40.977+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:48:40.979+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:48:40.979+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:48:40.983+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:48:40.982+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:48:40.983+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:48:40.989+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:48:40.989+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:48:40.989+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:48:40.989+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:48:41.000+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:48:41.000+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:48:41.006+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:48:41.006+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:48:41.006+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:48:41.006+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:48:41.023+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:48:41.023+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:48:41.038+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.776 seconds
[2024-11-25T17:49:11.630+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:49:11.630+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1525)
[2024-11-25T17:49:11.631+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:49:11.631+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1525
[2024-11-25T17:49:11.635+0000] {processor.py:153} INFO - Started process (PID=1525) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:49:11.635+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:49:11.636+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:49:11.636+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:49:11.637+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:49:11.637+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:49:12.176+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:49:12.176+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:49:12.177+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:49:12.176+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:49:12.274+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:49:12.275+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:49:12.275+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:49:12.279+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:49:12.279+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:49:12.279+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:49:12.285+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:49:12.285+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:49:12.286+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:49:12.285+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:49:12.295+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:49:12.295+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:49:12.301+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:49:12.301+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:49:12.301+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:49:12.301+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:49:12.310+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:49:12.310+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:49:12.317+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.687 seconds
[2024-11-25T17:49:42.811+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:49:42.810+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1531)
[2024-11-25T17:49:42.812+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:49:42.812+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1531
[2024-11-25T17:49:42.819+0000] {processor.py:153} INFO - Started process (PID=1531) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:49:42.820+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:49:42.821+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:49:42.821+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:49:42.823+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:49:42.823+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:49:43.292+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:49:43.292+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:49:43.293+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:49:43.292+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:49:43.385+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:49:43.387+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:49:43.387+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:49:43.390+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:49:43.390+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:49:43.390+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:49:43.395+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:49:43.395+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:49:43.396+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:49:43.396+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:49:43.405+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:49:43.405+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:49:43.411+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:49:43.411+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:49:43.411+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:49:43.411+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:49:43.420+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:49:43.420+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:49:43.427+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.617 seconds
[2024-11-25T17:50:13.823+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:50:13.823+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1537)
[2024-11-25T17:50:13.824+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:50:13.824+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1537
[2024-11-25T17:50:13.829+0000] {processor.py:153} INFO - Started process (PID=1537) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:50:13.830+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:50:13.830+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:50:13.830+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:50:13.832+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:50:13.832+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:50:14.957+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:50:14.957+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:50:14.961+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:50:14.958+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:50:15.176+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:50:15.180+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:50:15.180+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:50:15.186+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:50:15.185+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:50:15.186+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:50:15.193+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:50:15.193+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:50:15.193+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:50:15.193+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:50:15.212+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:50:15.211+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:50:15.227+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:50:15.227+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:50:15.228+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:50:15.227+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:50:15.245+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:50:15.245+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:50:15.256+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 1.434 seconds
[2024-11-25T17:50:45.427+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:50:45.426+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1543)
[2024-11-25T17:50:45.428+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:50:45.428+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1543
[2024-11-25T17:50:45.436+0000] {processor.py:153} INFO - Started process (PID=1543) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:50:45.437+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:50:45.438+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:50:45.438+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:50:45.439+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:50:45.439+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:50:46.188+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:50:46.188+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:50:46.189+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:50:46.189+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:50:46.362+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:50:46.364+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:50:46.364+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:50:46.370+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:50:46.369+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:50:46.370+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:50:46.379+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:50:46.379+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:50:46.379+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:50:46.379+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:50:46.434+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:50:46.434+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:50:46.449+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:50:46.449+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:50:46.450+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:50:46.449+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:50:46.467+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:50:46.467+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:50:46.476+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 1.050 seconds
[2024-11-25T17:51:17.050+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:51:17.049+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1549)
[2024-11-25T17:51:17.051+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:51:17.051+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1549
[2024-11-25T17:51:17.055+0000] {processor.py:153} INFO - Started process (PID=1549) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:51:17.056+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:51:17.056+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:51:17.056+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:51:17.058+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:51:17.057+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:51:17.608+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:51:17.607+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:51:17.608+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:51:17.608+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:51:17.711+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:51:17.713+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:51:17.713+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:51:17.717+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:51:17.716+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:51:17.717+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:51:17.723+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:51:17.723+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:51:17.723+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:51:17.723+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:51:17.740+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:51:17.740+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:51:17.750+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:51:17.750+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:51:17.751+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:51:17.750+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:51:17.765+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:51:17.765+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:51:17.774+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.725 seconds
[2024-11-25T17:51:48.325+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:51:48.325+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1555)
[2024-11-25T17:51:48.326+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:51:48.326+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1555
[2024-11-25T17:51:48.339+0000] {processor.py:153} INFO - Started process (PID=1555) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:51:48.340+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:51:48.341+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:51:48.341+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:51:48.343+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:51:48.343+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:51:49.044+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:51:49.044+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:51:49.045+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:51:49.045+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:51:49.219+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:51:49.222+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:51:49.222+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:51:49.232+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:51:49.232+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:51:49.233+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:51:49.241+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:51:49.241+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:51:49.241+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:51:49.241+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:51:49.258+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:51:49.258+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:51:49.265+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:51:49.265+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:51:49.266+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:51:49.266+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:51:49.278+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:51:49.278+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:51:49.291+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.966 seconds
[2024-11-25T17:52:19.884+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:52:19.884+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1561)
[2024-11-25T17:52:19.886+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:52:19.886+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1561
[2024-11-25T17:52:19.891+0000] {processor.py:153} INFO - Started process (PID=1561) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:52:19.892+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:52:19.894+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:52:19.893+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:52:19.896+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:52:19.896+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:52:20.532+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:52:20.532+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:52:20.533+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:52:20.532+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:52:20.691+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:52:20.693+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:52:20.693+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:52:20.698+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:52:20.698+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:52:20.698+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:52:20.705+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:52:20.705+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:52:20.705+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:52:20.705+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:52:20.716+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:52:20.716+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:52:20.728+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:52:20.728+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:52:20.728+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:52:20.728+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:52:20.743+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:52:20.743+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:52:20.751+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.868 seconds
[2024-11-25T17:52:51.489+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:52:51.488+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1567)
[2024-11-25T17:52:51.490+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:52:51.490+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1567
[2024-11-25T17:52:51.494+0000] {processor.py:153} INFO - Started process (PID=1567) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:52:51.495+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:52:51.495+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:52:51.495+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:52:51.497+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:52:51.497+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:52:52.020+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:52:52.020+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:52:52.020+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:52:52.020+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:52:52.116+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:52:52.117+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:52:52.117+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:52:52.125+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:52:52.124+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:52:52.125+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:52:52.136+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:52:52.136+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:52:52.138+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:52:52.138+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:52:52.174+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:52:52.173+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:52:52.187+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:52:52.187+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:52:52.188+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:52:52.187+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:52:52.206+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:52:52.205+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:52:52.215+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.727 seconds
[2024-11-25T17:53:22.681+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:53:22.678+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1573)
[2024-11-25T17:53:22.685+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:53:22.685+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1573
[2024-11-25T17:53:22.695+0000] {processor.py:153} INFO - Started process (PID=1573) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:53:22.696+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:53:22.700+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:53:22.700+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:53:22.706+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:53:22.706+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:53:23.688+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:53:23.688+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:53:23.689+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:53:23.688+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:53:23.817+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:53:23.818+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:53:23.818+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:53:23.822+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:53:23.822+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:53:23.822+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:53:23.828+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:53:23.828+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:53:23.828+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:53:23.828+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:53:23.835+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:53:23.835+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:53:23.841+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:53:23.841+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:53:23.841+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:53:23.841+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:53:23.852+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:53:23.852+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:53:23.859+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 1.182 seconds
[2024-11-25T17:53:54.328+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:53:54.327+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1579)
[2024-11-25T17:53:54.328+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:53:54.328+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1579
[2024-11-25T17:53:54.332+0000] {processor.py:153} INFO - Started process (PID=1579) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:53:54.333+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:53:54.334+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:53:54.334+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:53:54.335+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:53:54.335+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:53:54.881+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:53:54.881+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:53:54.882+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:53:54.882+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:53:55.002+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:53:55.003+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:53:55.003+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:53:55.007+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:53:55.007+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:53:55.007+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:53:55.013+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:53:55.013+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:53:55.013+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:53:55.013+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:53:55.025+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:53:55.025+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:53:55.036+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:53:55.036+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:53:55.037+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:53:55.036+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:53:55.051+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:53:55.051+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:53:55.063+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.736 seconds
[2024-11-25T17:54:25.543+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:54:25.542+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1585)
[2024-11-25T17:54:25.544+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:54:25.544+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1585
[2024-11-25T17:54:25.570+0000] {processor.py:153} INFO - Started process (PID=1585) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:54:25.574+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:54:25.578+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:54:25.578+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:54:25.585+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:54:25.585+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:54:26.215+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:54:26.215+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:54:26.216+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:54:26.215+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:54:26.312+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:54:26.313+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:54:26.313+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:54:26.317+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:54:26.317+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:54:26.317+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:54:26.323+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:54:26.323+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:54:26.323+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:54:26.323+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:54:26.361+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:54:26.361+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:54:26.369+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:54:26.369+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:54:26.369+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:54:26.369+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:54:26.388+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:54:26.388+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:54:26.399+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.857 seconds
[2024-11-25T17:54:56.912+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:54:56.912+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1591)
[2024-11-25T17:54:56.913+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:54:56.913+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1591
[2024-11-25T17:54:56.923+0000] {processor.py:153} INFO - Started process (PID=1591) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:54:56.925+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:54:56.926+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:54:56.926+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:54:56.928+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:54:56.928+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:54:57.579+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:54:57.579+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:54:57.580+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:54:57.580+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:54:57.687+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:54:57.689+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:54:57.688+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:54:57.692+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:54:57.692+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:54:57.693+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:54:57.698+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:54:57.698+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:54:57.698+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:54:57.698+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:54:57.712+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:54:57.712+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:54:57.718+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:54:57.718+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:54:57.719+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:54:57.718+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:54:57.729+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:54:57.729+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:54:57.739+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.828 seconds
[2024-11-25T17:55:27.936+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:55:27.936+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1597)
[2024-11-25T17:55:27.937+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:55:27.937+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1597
[2024-11-25T17:55:27.943+0000] {processor.py:153} INFO - Started process (PID=1597) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:55:27.944+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:55:27.945+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:55:27.944+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:55:27.946+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:55:27.946+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:55:28.420+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:55:28.420+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:55:28.420+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:55:28.420+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:55:28.527+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:55:28.528+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:55:28.528+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:55:28.532+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:55:28.532+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:55:28.533+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:55:28.538+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:55:28.538+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:55:28.538+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:55:28.538+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:55:28.547+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:55:28.547+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:55:28.555+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:55:28.555+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:55:28.555+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:55:28.555+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:55:28.567+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:55:28.567+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:55:28.575+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.639 seconds
[2024-11-25T17:55:59.013+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:55:59.012+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1603)
[2024-11-25T17:55:59.014+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:55:59.014+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1603
[2024-11-25T17:55:59.022+0000] {processor.py:153} INFO - Started process (PID=1603) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:55:59.023+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:55:59.024+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:55:59.024+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:55:59.027+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:55:59.027+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:55:59.526+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:55:59.526+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:55:59.526+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:55:59.526+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:55:59.620+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:55:59.621+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:55:59.621+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:55:59.625+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:55:59.625+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:55:59.625+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:55:59.630+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:55:59.630+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:55:59.630+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:55:59.630+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:55:59.642+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:55:59.642+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:55:59.654+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:55:59.654+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:55:59.654+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:55:59.654+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:55:59.664+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:55:59.664+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:55:59.675+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.663 seconds
[2024-11-25T17:56:30.117+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:56:30.117+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1609)
[2024-11-25T17:56:30.119+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:56:30.118+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1609
[2024-11-25T17:56:30.123+0000] {processor.py:153} INFO - Started process (PID=1609) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:56:30.124+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:56:30.125+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:56:30.125+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:56:30.126+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:56:30.126+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:56:30.608+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:56:30.608+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:56:30.609+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:56:30.609+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:56:30.704+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:56:30.705+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:56:30.705+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:56:30.709+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:56:30.709+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:56:30.709+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:56:30.714+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:56:30.714+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:56:30.715+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:56:30.715+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:56:30.725+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:56:30.725+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:56:30.731+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:56:30.731+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:56:30.731+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:56:30.731+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:56:30.744+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:56:30.744+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:56:30.750+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.634 seconds
[2024-11-25T17:57:01.192+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:57:01.192+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1615)
[2024-11-25T17:57:01.193+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:57:01.193+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1615
[2024-11-25T17:57:01.199+0000] {processor.py:153} INFO - Started process (PID=1615) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:57:01.200+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:57:01.201+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:57:01.201+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:57:01.202+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:57:01.202+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:57:01.724+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:57:01.724+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:57:01.725+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:57:01.724+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:57:01.820+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:57:01.825+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:57:01.825+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:57:01.832+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:57:01.832+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:57:01.832+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:57:01.838+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:57:01.838+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:57:01.838+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:57:01.838+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:57:01.850+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:57:01.850+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:57:01.856+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:57:01.856+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:57:01.856+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:57:01.856+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:57:01.866+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:57:01.866+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:57:01.872+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.680 seconds
[2024-11-25T17:57:31.965+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:57:31.964+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1621)
[2024-11-25T17:57:31.966+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:57:31.965+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1621
[2024-11-25T17:57:31.971+0000] {processor.py:153} INFO - Started process (PID=1621) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:57:31.972+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:57:31.972+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:57:31.972+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:57:31.974+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:57:31.974+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:57:32.439+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:57:32.439+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:57:32.440+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:57:32.440+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:57:32.549+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:57:32.550+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:57:32.550+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:57:32.554+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:57:32.554+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:57:32.554+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:57:32.559+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:57:32.559+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:57:32.560+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:57:32.560+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:57:32.583+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:57:32.583+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:57:32.590+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:57:32.590+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:57:32.591+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:57:32.590+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:57:32.601+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:57:32.601+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:57:32.608+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.643 seconds
[2024-11-25T17:58:03.002+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:58:03.001+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1627)
[2024-11-25T17:58:03.004+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:58:03.003+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1627
[2024-11-25T17:58:03.010+0000] {processor.py:153} INFO - Started process (PID=1627) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:58:03.011+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:58:03.012+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:58:03.012+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:58:03.014+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:58:03.014+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:58:03.704+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:58:03.704+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:58:03.705+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:58:03.705+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:58:03.812+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:58:03.813+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:58:03.813+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:58:03.818+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:58:03.817+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:58:03.818+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:58:03.826+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:58:03.826+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:58:03.826+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:58:03.826+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:58:03.919+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:58:03.919+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:58:03.961+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:58:03.960+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:58:03.966+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:58:03.963+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:58:04.004+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:58:04.004+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:58:04.028+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 1.027 seconds
[2024-11-25T17:58:34.643+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:58:34.642+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1633)
[2024-11-25T17:58:34.644+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:58:34.644+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1633
[2024-11-25T17:58:34.651+0000] {processor.py:153} INFO - Started process (PID=1633) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:58:34.651+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:58:34.652+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:58:34.652+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:58:34.655+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:58:34.655+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:58:35.353+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:58:35.353+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:58:35.354+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:58:35.354+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:58:35.518+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:58:35.520+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:58:35.520+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:58:35.534+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:58:35.533+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:58:35.534+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:58:35.545+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:58:35.545+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:58:35.545+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:58:35.545+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:58:35.610+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:58:35.610+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:58:35.621+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:58:35.621+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:58:35.623+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:58:35.622+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:58:35.668+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:58:35.666+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:58:35.683+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 1.040 seconds
[2024-11-25T17:59:06.112+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:59:06.112+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1639)
[2024-11-25T17:59:06.113+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:59:06.113+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1639
[2024-11-25T17:59:06.117+0000] {processor.py:153} INFO - Started process (PID=1639) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:59:06.118+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:59:06.119+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:59:06.119+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:59:06.120+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:59:06.120+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:59:06.846+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:59:06.846+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:59:06.847+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:59:06.847+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:59:07.147+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:59:07.150+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:59:07.150+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:59:07.156+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:59:07.156+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:59:07.157+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:59:07.169+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:59:07.169+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:59:07.169+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:59:07.169+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:59:07.184+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:59:07.184+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:59:07.196+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:59:07.196+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:59:07.198+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:59:07.196+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:59:07.230+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:59:07.230+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:59:07.246+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 1.134 seconds
[2024-11-25T17:59:37.669+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:59:37.669+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1645)
[2024-11-25T17:59:37.670+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:59:37.670+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1645
[2024-11-25T17:59:37.674+0000] {processor.py:153} INFO - Started process (PID=1645) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:59:37.674+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T17:59:37.675+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:59:37.675+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:59:37.676+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:59:37.676+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:59:38.177+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:59:38.177+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T17:59:38.178+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:59:38.177+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T17:59:38.290+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T17:59:38.292+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:59:38.292+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T17:59:38.296+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:59:38.296+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T17:59:38.297+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T17:59:38.305+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:59:38.305+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T17:59:38.305+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:59:38.305+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T17:59:38.320+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:59:38.320+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T17:59:38.329+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:59:38.329+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T17:59:38.330+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:59:38.329+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T17:59:38.353+0000] {logging_mixin.py:137} INFO - [2024-11-25T17:59:38.353+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T17:59:38.366+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.697 seconds
[2024-11-25T18:00:08.830+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:00:08.830+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1651)
[2024-11-25T18:00:08.831+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:00:08.831+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1651
[2024-11-25T18:00:08.837+0000] {processor.py:153} INFO - Started process (PID=1651) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:00:08.837+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:00:08.838+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:00:08.838+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:00:08.840+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:00:08.840+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:00:09.452+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:00:09.451+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:00:09.453+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:00:09.452+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:00:09.710+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:00:09.712+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:00:09.712+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:00:09.718+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:00:09.718+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:00:09.718+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:00:09.727+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:00:09.727+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:00:09.727+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:00:09.727+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:00:09.748+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:00:09.748+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:00:09.756+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:00:09.756+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:00:09.757+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:00:09.757+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:00:09.776+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:00:09.776+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:00:09.785+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.955 seconds
[2024-11-25T18:00:40.398+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:00:40.397+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1657)
[2024-11-25T18:00:40.398+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:00:40.398+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1657
[2024-11-25T18:00:40.402+0000] {processor.py:153} INFO - Started process (PID=1657) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:00:40.403+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:00:40.404+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:00:40.404+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:00:40.406+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:00:40.406+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:00:41.002+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:00:41.001+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:00:41.003+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:00:41.002+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:00:41.130+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:00:41.133+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:00:41.133+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:00:41.139+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:00:41.139+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:00:41.139+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:00:41.149+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:00:41.149+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:00:41.150+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:00:41.150+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:00:41.164+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:00:41.164+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:00:41.174+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:00:41.174+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:00:41.175+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:00:41.174+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:00:41.191+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:00:41.191+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:00:41.205+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.808 seconds
[2024-11-25T18:01:11.857+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:01:11.857+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1663)
[2024-11-25T18:01:11.858+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:01:11.858+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1663
[2024-11-25T18:01:11.865+0000] {processor.py:153} INFO - Started process (PID=1663) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:01:11.865+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:01:11.867+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:01:11.867+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:01:11.868+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:01:11.868+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:01:12.416+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:01:12.416+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:01:12.417+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:01:12.417+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:01:12.545+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:01:12.546+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:01:12.546+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:01:12.551+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:01:12.550+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:01:12.551+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:01:12.558+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:01:12.558+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:01:12.558+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:01:12.558+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:01:12.585+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:01:12.585+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:01:12.593+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:01:12.593+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:01:12.594+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:01:12.593+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:01:12.606+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:01:12.606+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:01:12.613+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.757 seconds
[2024-11-25T18:01:43.194+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:01:43.193+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1669)
[2024-11-25T18:01:43.195+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:01:43.194+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1669
[2024-11-25T18:01:43.201+0000] {processor.py:153} INFO - Started process (PID=1669) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:01:43.202+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:01:43.203+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:01:43.203+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:01:43.204+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:01:43.204+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:01:43.940+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:01:43.940+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:01:43.941+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:01:43.941+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:01:44.220+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:01:44.223+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:01:44.223+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:01:44.230+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:01:44.229+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:01:44.230+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:01:44.240+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:01:44.240+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:01:44.241+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:01:44.240+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:01:44.275+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:01:44.275+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:01:44.289+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:01:44.289+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:01:44.291+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:01:44.289+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:01:44.309+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:01:44.309+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:01:44.318+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 1.125 seconds
[2024-11-25T18:02:14.906+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:02:14.906+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1675)
[2024-11-25T18:02:14.908+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:02:14.907+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1675
[2024-11-25T18:02:14.913+0000] {processor.py:153} INFO - Started process (PID=1675) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:02:14.913+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:02:14.916+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:02:14.916+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:02:14.921+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:02:14.921+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:02:15.576+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:02:15.575+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:02:15.576+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:02:15.576+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:02:15.694+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:02:15.695+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:02:15.695+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:02:15.700+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:02:15.700+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:02:15.700+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:02:15.706+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:02:15.706+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:02:15.706+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:02:15.706+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:02:15.721+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:02:15.721+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:02:15.727+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:02:15.727+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:02:15.727+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:02:15.727+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:02:15.737+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:02:15.737+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:02:15.744+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.838 seconds
[2024-11-25T18:02:46.245+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:02:46.245+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1681)
[2024-11-25T18:02:46.246+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:02:46.246+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1681
[2024-11-25T18:02:46.251+0000] {processor.py:153} INFO - Started process (PID=1681) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:02:46.251+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:02:46.252+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:02:46.252+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:02:46.253+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:02:46.253+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:02:46.732+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:02:46.732+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:02:46.733+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:02:46.733+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:02:46.844+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:02:46.846+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:02:46.846+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:02:46.851+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:02:46.851+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:02:46.852+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:02:46.860+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:02:46.860+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:02:46.860+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:02:46.860+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:02:46.874+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:02:46.874+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:02:46.888+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:02:46.888+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:02:46.889+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:02:46.888+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:02:46.911+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:02:46.911+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:02:46.921+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.677 seconds
[2024-11-25T18:03:17.605+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:03:17.604+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1687)
[2024-11-25T18:03:17.607+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:03:17.607+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1687
[2024-11-25T18:03:17.611+0000] {processor.py:153} INFO - Started process (PID=1687) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:03:17.612+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:03:17.613+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:03:17.613+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:03:17.615+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:03:17.615+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:03:18.322+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:03:18.321+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:03:18.322+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:03:18.322+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:03:18.507+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:03:18.509+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:03:18.509+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:03:18.514+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:03:18.514+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:03:18.514+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:03:18.520+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:03:18.520+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:03:18.521+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:03:18.521+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:03:18.537+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:03:18.537+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:03:18.546+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:03:18.546+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:03:18.549+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:03:18.546+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:03:18.576+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:03:18.576+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:03:18.587+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.982 seconds
[2024-11-25T18:03:49.375+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:03:49.373+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1693)
[2024-11-25T18:03:49.378+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:03:49.378+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1693
[2024-11-25T18:03:49.385+0000] {processor.py:153} INFO - Started process (PID=1693) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:03:49.386+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:03:49.387+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:03:49.387+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:03:49.389+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:03:49.389+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:03:50.770+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:03:50.770+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:03:50.771+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:03:50.771+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:03:50.936+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:03:50.941+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:03:50.941+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:03:50.949+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:03:50.949+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:03:50.950+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:03:50.969+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:03:50.969+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:03:50.969+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:03:50.969+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:03:51.012+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:03:51.012+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:03:51.022+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:03:51.022+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:03:51.022+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:03:51.022+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:03:51.041+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:03:51.041+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:03:51.056+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 1.684 seconds
[2024-11-25T18:04:21.492+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:04:21.492+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1699)
[2024-11-25T18:04:21.494+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:04:21.493+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1699
[2024-11-25T18:04:21.500+0000] {processor.py:153} INFO - Started process (PID=1699) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:04:21.500+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:04:21.502+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:04:21.501+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:04:21.503+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:04:21.503+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:04:22.042+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:04:22.042+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:04:22.043+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:04:22.042+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:04:22.146+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:04:22.147+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:04:22.147+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:04:22.152+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:04:22.151+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:04:22.152+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:04:22.158+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:04:22.158+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:04:22.158+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:04:22.158+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:04:22.167+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:04:22.167+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:04:22.174+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:04:22.174+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:04:22.174+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:04:22.174+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:04:22.184+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:04:22.183+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:04:22.191+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.700 seconds
[2024-11-25T18:04:52.712+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:04:52.712+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1705)
[2024-11-25T18:04:52.713+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:04:52.713+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1705
[2024-11-25T18:04:52.720+0000] {processor.py:153} INFO - Started process (PID=1705) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:04:52.721+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:04:52.722+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:04:52.722+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:04:52.724+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:04:52.724+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:04:53.221+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:04:53.221+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:04:53.222+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:04:53.222+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:04:53.336+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:04:53.337+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:04:53.337+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:04:53.341+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:04:53.341+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:04:53.341+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:04:53.347+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:04:53.347+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:04:53.347+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:04:53.347+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:04:53.359+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:04:53.359+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:04:53.364+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:04:53.364+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:04:53.365+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:04:53.365+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:04:53.373+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:04:53.373+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:04:53.381+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.669 seconds
[2024-11-25T18:05:23.525+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:05:23.525+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1711)
[2024-11-25T18:05:23.526+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:05:23.526+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1711
[2024-11-25T18:05:23.531+0000] {processor.py:153} INFO - Started process (PID=1711) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:05:23.532+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:05:23.533+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:05:23.532+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:05:23.535+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:05:23.535+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:05:24.010+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:05:24.010+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:05:24.011+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:05:24.010+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:05:24.106+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:05:24.107+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:05:24.107+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:05:24.111+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:05:24.111+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:05:24.111+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:05:24.130+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:05:24.130+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:05:24.131+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:05:24.130+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:05:24.139+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:05:24.139+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:05:24.145+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:05:24.145+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:05:24.145+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:05:24.145+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:05:24.155+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:05:24.155+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:05:24.162+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.638 seconds
[2024-11-25T18:05:54.301+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:05:54.300+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1717)
[2024-11-25T18:05:54.302+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:05:54.302+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1717
[2024-11-25T18:05:54.309+0000] {processor.py:153} INFO - Started process (PID=1717) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:05:54.309+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:05:54.310+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:05:54.310+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:05:54.311+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:05:54.311+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:05:54.953+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:05:54.953+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:05:54.954+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:05:54.953+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:05:55.066+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:05:55.069+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:05:55.069+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:05:55.073+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:05:55.073+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:05:55.073+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:05:55.079+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:05:55.079+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:05:55.079+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:05:55.079+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:05:55.092+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:05:55.092+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:05:55.099+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:05:55.099+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:05:55.099+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:05:55.099+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:05:55.110+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:05:55.110+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:05:55.122+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.822 seconds
[2024-11-25T18:06:25.669+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:06:25.668+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1723)
[2024-11-25T18:06:25.669+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:06:25.669+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1723
[2024-11-25T18:06:25.698+0000] {processor.py:153} INFO - Started process (PID=1723) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:06:25.703+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:06:25.708+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:06:25.707+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:06:25.712+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:06:25.712+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:06:26.315+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:06:26.315+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:06:26.316+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:06:26.315+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:06:26.406+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:06:26.407+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:06:26.407+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:06:26.411+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:06:26.410+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:06:26.411+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:06:26.416+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:06:26.416+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:06:26.416+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:06:26.416+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:06:26.424+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:06:26.424+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:06:26.429+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:06:26.429+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:06:26.430+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:06:26.429+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:06:26.438+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:06:26.438+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:06:26.445+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.778 seconds
[2024-11-25T18:06:56.932+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:06:56.932+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1729)
[2024-11-25T18:06:56.935+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:06:56.934+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1729
[2024-11-25T18:06:56.953+0000] {processor.py:153} INFO - Started process (PID=1729) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:06:56.954+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:06:56.956+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:06:56.955+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:06:56.957+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:06:56.957+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:06:57.500+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:06:57.499+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:06:57.500+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:06:57.500+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:06:57.625+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:06:57.627+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:06:57.627+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:06:57.631+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:06:57.631+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:06:57.631+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:06:57.637+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:06:57.637+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:06:57.638+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:06:57.638+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:06:57.658+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:06:57.658+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:06:57.670+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:06:57.670+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:06:57.670+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:06:57.670+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:06:57.685+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:06:57.685+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:06:57.692+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.761 seconds
[2024-11-25T18:07:28.141+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:07:28.141+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1735)
[2024-11-25T18:07:28.142+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:07:28.142+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1735
[2024-11-25T18:07:28.147+0000] {processor.py:153} INFO - Started process (PID=1735) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:07:28.148+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:07:28.149+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:07:28.149+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:07:28.150+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:07:28.150+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:07:28.633+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:07:28.633+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:07:28.634+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:07:28.634+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:07:28.728+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:07:28.729+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:07:28.729+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:07:28.733+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:07:28.733+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:07:28.733+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:07:28.738+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:07:28.738+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:07:28.738+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:07:28.738+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:07:28.748+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:07:28.748+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:07:28.753+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:07:28.753+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:07:28.754+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:07:28.753+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:07:28.768+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:07:28.768+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:07:28.779+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.638 seconds
[2024-11-25T18:07:59.232+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:07:59.231+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1741)
[2024-11-25T18:07:59.233+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:07:59.233+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1741
[2024-11-25T18:07:59.241+0000] {processor.py:153} INFO - Started process (PID=1741) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:07:59.241+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:07:59.242+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:07:59.242+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:07:59.244+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:07:59.244+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:07:59.684+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:07:59.684+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:07:59.685+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:07:59.685+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:07:59.776+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:07:59.778+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:07:59.778+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:07:59.781+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:07:59.781+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:07:59.781+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:07:59.789+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:07:59.789+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:07:59.789+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:07:59.789+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:07:59.804+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:07:59.804+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:07:59.810+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:07:59.810+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:07:59.810+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:07:59.810+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:07:59.819+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:07:59.819+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:07:59.826+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.595 seconds
[2024-11-25T18:08:30.276+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:08:30.274+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1747)
[2024-11-25T18:08:30.278+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:08:30.277+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1747
[2024-11-25T18:08:30.291+0000] {processor.py:153} INFO - Started process (PID=1747) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:08:30.292+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:08:30.293+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:08:30.293+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:08:30.295+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:08:30.295+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:08:30.783+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:08:30.783+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:08:30.784+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:08:30.784+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:08:30.888+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:08:30.890+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:08:30.890+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:08:30.894+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:08:30.894+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:08:30.894+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:08:30.900+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:08:30.900+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:08:30.900+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:08:30.900+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:08:30.910+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:08:30.910+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:08:30.915+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:08:30.915+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:08:30.916+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:08:30.915+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:08:30.924+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:08:30.924+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:08:30.932+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.657 seconds
[2024-11-25T18:09:01.210+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:09:01.209+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1753)
[2024-11-25T18:09:01.214+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:09:01.214+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1753
[2024-11-25T18:09:01.221+0000] {processor.py:153} INFO - Started process (PID=1753) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:09:01.222+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:09:01.223+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:09:01.223+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:09:01.225+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:09:01.225+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:09:02.259+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:09:02.259+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:09:02.261+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:09:02.260+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:09:02.500+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:09:02.503+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:09:02.503+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:09:02.513+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:09:02.512+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:09:02.514+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:09:02.545+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:09:02.545+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:09:02.546+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:09:02.546+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:09:02.598+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:09:02.598+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:09:02.634+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:09:02.634+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:09:02.635+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:09:02.635+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:09:02.662+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:09:02.662+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:09:02.692+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 1.483 seconds
[2024-11-25T18:09:32.929+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:09:32.929+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1759)
[2024-11-25T18:09:32.931+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:09:32.931+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1759
[2024-11-25T18:09:32.942+0000] {processor.py:153} INFO - Started process (PID=1759) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:09:32.944+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:09:32.946+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:09:32.946+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:09:32.949+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:09:32.949+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:09:33.661+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:09:33.661+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:09:33.662+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:09:33.661+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:09:33.773+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:09:33.774+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:09:33.774+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:09:33.778+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:09:33.778+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:09:33.778+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:09:33.784+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:09:33.784+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:09:33.784+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:09:33.784+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:09:33.797+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:09:33.797+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:09:33.804+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:09:33.803+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:09:33.804+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:09:33.804+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:09:33.814+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:09:33.814+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:09:33.831+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.902 seconds
[2024-11-25T18:10:04.459+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:10:04.458+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1765)
[2024-11-25T18:10:04.459+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:10:04.459+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1765
[2024-11-25T18:10:04.463+0000] {processor.py:153} INFO - Started process (PID=1765) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:10:04.463+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:10:04.464+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:10:04.464+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:10:04.465+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:10:04.465+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:10:04.967+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:10:04.967+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:10:04.968+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:10:04.967+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:10:05.147+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:10:05.150+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:10:05.150+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:10:05.157+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:10:05.157+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:10:05.158+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:10:05.170+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:10:05.170+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:10:05.170+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:10:05.170+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:10:05.183+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:10:05.183+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:10:05.197+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:10:05.197+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:10:05.198+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:10:05.197+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:10:05.225+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:10:05.225+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:10:05.238+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.779 seconds
[2024-11-25T18:10:35.926+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:10:35.922+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1771)
[2024-11-25T18:10:35.929+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:10:35.928+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1771
[2024-11-25T18:10:36.024+0000] {processor.py:153} INFO - Started process (PID=1771) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:10:36.026+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:10:36.027+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:10:36.027+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:10:36.028+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:10:36.028+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:10:37.092+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:10:37.092+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:10:37.093+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:10:37.093+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:10:37.243+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:10:37.244+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:10:37.244+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:10:37.248+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:10:37.248+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:10:37.248+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:10:37.254+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:10:37.254+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:10:37.255+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:10:37.255+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:10:37.271+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:10:37.271+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:10:37.277+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:10:37.277+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:10:37.278+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:10:37.277+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:10:37.288+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:10:37.288+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:10:37.300+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 1.382 seconds
[2024-11-25T18:11:07.618+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:11:07.617+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1777)
[2024-11-25T18:11:07.619+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:11:07.619+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1777
[2024-11-25T18:11:07.624+0000] {processor.py:153} INFO - Started process (PID=1777) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:11:07.625+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:11:07.626+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:11:07.626+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:11:07.628+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:11:07.628+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:11:08.210+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:11:08.209+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:11:08.212+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:11:08.211+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:11:08.350+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:11:08.352+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:11:08.352+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:11:08.356+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:11:08.356+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:11:08.356+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:11:08.366+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:11:08.366+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:11:08.367+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:11:08.367+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:11:08.383+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:11:08.383+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:11:08.391+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:11:08.391+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:11:08.392+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:11:08.392+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:11:08.403+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:11:08.403+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:11:08.409+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.792 seconds
[2024-11-25T18:11:39.092+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:11:39.092+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1783)
[2024-11-25T18:11:39.093+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:11:39.093+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1783
[2024-11-25T18:11:39.110+0000] {processor.py:153} INFO - Started process (PID=1783) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:11:39.111+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:11:39.112+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:11:39.112+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:11:39.114+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:11:39.114+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:11:39.793+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:11:39.793+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:11:39.794+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:11:39.794+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:11:39.919+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:11:39.920+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:11:39.920+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:11:39.925+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:11:39.925+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:11:39.926+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:11:39.944+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:11:39.944+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:11:39.945+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:11:39.945+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:11:39.974+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:11:39.974+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:11:39.991+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:11:39.991+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:11:39.993+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:11:39.991+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:11:40.022+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:11:40.022+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:11:40.042+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.950 seconds
[2024-11-25T18:12:10.561+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:12:10.560+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1789)
[2024-11-25T18:12:10.562+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:12:10.561+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1789
[2024-11-25T18:12:10.566+0000] {processor.py:153} INFO - Started process (PID=1789) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:12:10.567+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:12:10.568+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:12:10.568+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:12:10.569+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:12:10.569+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:12:11.111+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:12:11.111+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:12:11.112+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:12:11.112+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:12:11.224+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:12:11.226+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:12:11.226+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:12:11.230+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:12:11.229+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:12:11.230+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:12:11.236+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:12:11.236+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:12:11.236+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:12:11.236+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:12:11.258+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:12:11.258+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:12:11.266+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:12:11.266+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:12:11.266+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:12:11.266+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:12:11.278+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:12:11.278+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:12:11.287+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.726 seconds
[2024-11-25T18:12:41.698+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:12:41.697+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1795)
[2024-11-25T18:12:41.699+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:12:41.699+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1795
[2024-11-25T18:12:41.706+0000] {processor.py:153} INFO - Started process (PID=1795) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:12:41.707+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:12:41.708+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:12:41.708+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:12:41.710+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:12:41.710+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:12:42.552+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:12:42.551+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:12:42.553+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:12:42.552+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:12:42.676+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:12:42.677+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:12:42.677+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:12:42.681+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:12:42.681+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:12:42.681+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:12:42.689+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:12:42.689+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:12:42.689+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:12:42.689+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:12:42.710+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:12:42.710+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:12:42.720+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:12:42.720+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:12:42.721+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:12:42.721+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:12:42.747+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:12:42.747+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:12:42.753+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 1.056 seconds
[2024-11-25T18:13:13.163+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:13:13.162+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1801)
[2024-11-25T18:13:13.164+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:13:13.164+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1801
[2024-11-25T18:13:13.173+0000] {processor.py:153} INFO - Started process (PID=1801) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:13:13.175+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:13:13.176+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:13:13.176+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:13:13.178+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:13:13.178+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:13:13.857+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:13:13.857+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:13:13.858+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:13:13.858+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:13:13.962+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:13:13.964+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:13:13.964+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:13:13.968+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:13:13.968+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:13:13.969+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:13:13.975+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:13:13.975+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:13:13.975+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:13:13.975+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:13:13.987+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:13:13.987+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:13:13.995+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:13:13.995+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:13:13.996+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:13:13.995+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:13:14.007+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:13:14.007+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:13:14.016+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.854 seconds
[2024-11-25T18:13:44.632+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:13:44.631+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1807)
[2024-11-25T18:13:44.635+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:13:44.634+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1807
[2024-11-25T18:13:44.640+0000] {processor.py:153} INFO - Started process (PID=1807) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:13:44.641+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:13:44.642+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:13:44.641+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:13:44.643+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:13:44.643+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:13:45.346+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:13:45.346+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:13:45.347+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:13:45.347+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:13:45.539+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:13:45.543+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:13:45.543+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:13:45.555+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:13:45.554+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:13:45.555+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:13:45.571+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:13:45.571+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:13:45.571+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:13:45.571+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:13:45.592+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:13:45.592+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:13:45.618+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:13:45.618+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:13:45.619+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:13:45.619+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:13:45.637+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:13:45.636+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:13:45.645+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 1.014 seconds
[2024-11-25T18:14:16.329+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:14:16.329+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1813)
[2024-11-25T18:14:16.330+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:14:16.329+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1813
[2024-11-25T18:14:16.332+0000] {processor.py:153} INFO - Started process (PID=1813) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:14:16.333+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:14:16.333+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:14:16.333+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:14:16.335+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:14:16.335+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:14:17.117+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:14:17.116+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:14:17.120+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:14:17.119+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:14:17.329+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:14:17.332+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:14:17.332+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:14:17.338+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:14:17.338+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:14:17.339+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:14:17.348+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:14:17.348+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:14:17.348+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:14:17.348+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:14:17.360+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:14:17.360+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:14:17.369+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:14:17.369+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:14:17.370+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:14:17.369+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:14:17.387+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:14:17.386+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:14:17.396+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 1.067 seconds
[2024-11-25T18:14:48.051+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:14:48.051+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1819)
[2024-11-25T18:14:48.053+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:14:48.053+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1819
[2024-11-25T18:14:48.069+0000] {processor.py:153} INFO - Started process (PID=1819) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:14:48.069+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:14:48.070+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:14:48.070+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:14:48.071+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:14:48.071+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:14:48.642+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:14:48.642+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:14:48.643+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:14:48.643+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:14:48.761+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:14:48.763+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:14:48.763+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:14:48.767+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:14:48.767+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:14:48.767+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:14:48.774+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:14:48.774+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:14:48.774+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:14:48.774+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:14:48.789+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:14:48.788+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:14:48.795+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:14:48.795+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:14:48.795+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:14:48.795+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:14:48.816+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:14:48.816+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:14:48.832+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.781 seconds
[2024-11-25T18:15:19.477+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:15:19.476+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1825)
[2024-11-25T18:15:19.479+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:15:19.478+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1825
[2024-11-25T18:15:19.484+0000] {processor.py:153} INFO - Started process (PID=1825) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:15:19.485+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:15:19.486+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:15:19.485+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:15:19.487+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:15:19.487+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:15:20.025+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:15:20.025+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:15:20.025+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:15:20.025+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:15:20.129+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:15:20.131+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:15:20.131+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:15:20.135+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:15:20.135+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:15:20.135+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:15:20.141+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:15:20.141+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:15:20.141+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:15:20.141+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:15:20.152+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:15:20.152+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:15:20.158+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:15:20.158+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:15:20.158+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:15:20.158+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:15:20.168+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:15:20.168+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:15:20.177+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.701 seconds
[2024-11-25T18:15:50.968+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:15:50.967+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1831)
[2024-11-25T18:15:50.970+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:15:50.970+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1831
[2024-11-25T18:15:50.975+0000] {processor.py:153} INFO - Started process (PID=1831) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:15:50.976+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:15:50.977+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:15:50.977+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:15:50.982+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:15:50.981+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:15:51.457+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:15:51.457+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:15:51.458+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:15:51.457+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:15:51.559+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:15:51.561+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:15:51.561+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:15:51.564+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:15:51.564+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:15:51.564+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:15:51.570+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:15:51.570+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:15:51.570+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:15:51.570+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:15:51.580+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:15:51.580+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:15:51.586+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:15:51.586+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:15:51.587+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:15:51.586+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:15:51.596+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:15:51.596+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:15:51.603+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.636 seconds
[2024-11-25T18:16:22.110+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:16:22.110+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1837)
[2024-11-25T18:16:22.111+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:16:22.111+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1837
[2024-11-25T18:16:22.117+0000] {processor.py:153} INFO - Started process (PID=1837) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:16:22.118+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:16:22.119+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:16:22.119+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:16:22.120+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:16:22.120+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:16:22.623+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:16:22.623+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:16:22.624+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:16:22.623+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:16:22.730+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:16:22.732+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:16:22.732+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:16:22.738+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:16:22.737+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:16:22.738+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:16:22.745+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:16:22.745+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:16:22.745+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:16:22.745+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:16:22.756+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:16:22.756+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:16:22.762+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:16:22.762+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:16:22.762+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:16:22.762+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:16:22.771+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:16:22.771+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:16:22.778+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.668 seconds
[2024-11-25T18:16:53.002+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:16:53.001+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1843)
[2024-11-25T18:16:53.002+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:16:53.002+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1843
[2024-11-25T18:16:53.016+0000] {processor.py:153} INFO - Started process (PID=1843) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:16:53.016+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:16:53.017+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:16:53.017+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:16:53.018+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:16:53.018+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:16:53.664+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:16:53.664+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:16:53.664+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:16:53.664+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:16:53.778+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:16:53.780+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:16:53.780+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:16:53.784+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:16:53.784+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:16:53.784+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:16:53.790+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:16:53.790+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:16:53.790+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:16:53.790+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:16:53.800+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:16:53.800+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:16:53.808+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:16:53.807+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:16:53.808+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:16:53.808+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:16:53.821+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:16:53.821+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:16:53.837+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.836 seconds
[2024-11-25T18:17:24.346+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:17:24.345+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1849)
[2024-11-25T18:17:24.347+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:17:24.347+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1849
[2024-11-25T18:17:24.352+0000] {processor.py:153} INFO - Started process (PID=1849) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:17:24.353+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:17:24.353+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:17:24.353+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:17:24.355+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:17:24.355+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:17:24.819+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:17:24.818+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:17:24.819+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:17:24.819+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:17:24.917+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:17:24.919+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:17:24.919+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:17:24.922+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:17:24.922+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:17:24.922+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:17:24.929+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:17:24.929+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:17:24.929+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:17:24.929+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:17:24.936+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:17:24.936+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:17:24.942+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:17:24.942+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:17:24.942+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:17:24.942+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:17:24.953+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:17:24.953+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:17:24.959+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.614 seconds
[2024-11-25T18:17:55.330+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:17:55.329+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1855)
[2024-11-25T18:17:55.331+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:17:55.331+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1855
[2024-11-25T18:17:55.337+0000] {processor.py:153} INFO - Started process (PID=1855) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:17:55.337+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:17:55.338+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:17:55.338+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:17:55.340+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:17:55.340+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:17:56.040+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:17:56.040+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:17:56.041+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:17:56.041+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:17:56.203+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:17:56.206+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:17:56.205+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:17:56.211+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:17:56.211+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:17:56.211+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:17:56.224+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:17:56.224+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:17:56.224+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:17:56.224+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:17:56.245+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:17:56.245+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:17:56.254+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:17:56.254+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:17:56.255+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:17:56.255+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:17:56.266+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:17:56.266+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:17:56.272+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.943 seconds
[2024-11-25T18:18:27.003+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:18:27.003+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1861)
[2024-11-25T18:18:27.005+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:18:27.005+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1861
[2024-11-25T18:18:27.009+0000] {processor.py:153} INFO - Started process (PID=1861) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:18:27.010+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:18:27.011+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:18:27.011+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:18:27.014+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:18:27.014+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:18:27.846+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:18:27.846+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:18:27.847+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:18:27.847+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:18:28.016+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:18:28.018+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:18:28.018+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:18:28.023+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:18:28.022+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:18:28.023+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:18:28.032+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:18:28.032+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:18:28.032+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:18:28.032+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:18:28.049+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:18:28.049+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:18:28.056+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:18:28.056+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:18:28.057+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:18:28.057+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:18:28.080+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:18:28.079+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:18:28.089+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 1.086 seconds
[2024-11-25T18:18:58.736+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:18:58.736+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1867)
[2024-11-25T18:18:58.737+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:18:58.737+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1867
[2024-11-25T18:18:58.741+0000] {processor.py:153} INFO - Started process (PID=1867) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:18:58.742+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:18:58.742+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:18:58.742+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:18:58.744+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:18:58.744+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:18:59.372+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:18:59.372+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:18:59.373+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:18:59.372+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:18:59.500+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:18:59.502+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:18:59.502+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:18:59.507+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:18:59.506+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:18:59.507+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:18:59.514+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:18:59.514+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:18:59.515+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:18:59.515+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:18:59.570+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:18:59.570+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:18:59.582+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:18:59.582+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:18:59.582+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:18:59.582+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:18:59.599+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:18:59.599+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:18:59.610+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.875 seconds
[2024-11-25T18:19:30.561+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:19:30.560+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1873)
[2024-11-25T18:19:30.564+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:19:30.564+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1873
[2024-11-25T18:19:30.572+0000] {processor.py:153} INFO - Started process (PID=1873) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:19:30.574+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:19:30.576+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:19:30.576+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:19:30.582+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:19:30.582+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:19:31.302+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:19:31.302+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:19:31.303+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:19:31.303+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:19:31.417+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:19:31.419+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:19:31.419+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:19:31.423+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:19:31.423+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:19:31.423+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:19:31.429+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:19:31.429+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:19:31.429+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:19:31.429+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:19:31.447+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:19:31.447+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:19:31.466+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:19:31.466+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:19:31.468+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:19:31.466+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:19:31.490+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:19:31.490+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:19:31.503+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.943 seconds
[2024-11-25T18:20:02.019+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:20:02.019+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1879)
[2024-11-25T18:20:02.020+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:20:02.020+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1879
[2024-11-25T18:20:02.025+0000] {processor.py:153} INFO - Started process (PID=1879) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:20:02.026+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:20:02.027+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:20:02.027+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:20:02.029+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:20:02.029+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:20:02.489+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:20:02.489+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:20:02.490+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:20:02.489+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:20:02.596+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:20:02.598+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:20:02.598+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:20:02.601+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:20:02.601+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:20:02.601+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:20:02.607+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:20:02.607+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:20:02.607+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:20:02.607+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:20:02.618+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:20:02.618+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:20:02.628+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:20:02.628+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:20:02.629+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:20:02.628+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:20:02.644+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:20:02.644+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:20:02.651+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.633 seconds
[2024-11-25T18:20:33.095+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:20:33.094+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1885)
[2024-11-25T18:20:33.096+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:20:33.096+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1885
[2024-11-25T18:20:33.102+0000] {processor.py:153} INFO - Started process (PID=1885) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:20:33.103+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:20:33.104+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:20:33.104+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:20:33.106+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:20:33.106+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:20:33.576+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:20:33.576+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:20:33.577+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:20:33.577+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:20:33.667+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:20:33.669+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:20:33.669+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:20:33.672+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:20:33.672+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:20:33.672+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:20:33.678+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:20:33.678+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:20:33.678+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:20:33.678+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:20:33.684+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:20:33.684+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:20:33.690+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:20:33.690+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:20:33.690+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:20:33.690+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:20:33.701+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:20:33.701+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:20:33.717+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.623 seconds
[2024-11-25T18:21:03.905+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:21:03.905+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1891)
[2024-11-25T18:21:03.907+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:21:03.906+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1891
[2024-11-25T18:21:03.916+0000] {processor.py:153} INFO - Started process (PID=1891) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:21:03.916+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:21:03.918+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:21:03.918+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:21:03.920+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:21:03.920+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:21:04.399+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:21:04.398+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:21:04.399+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:21:04.399+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:21:04.492+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:21:04.493+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:21:04.493+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:21:04.496+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:21:04.496+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:21:04.496+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:21:04.501+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:21:04.501+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:21:04.502+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:21:04.502+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:21:04.509+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:21:04.509+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:21:04.516+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:21:04.516+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:21:04.517+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:21:04.517+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:21:04.536+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:21:04.536+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:21:04.547+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.643 seconds
[2024-11-25T18:21:34.910+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:21:34.909+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1897)
[2024-11-25T18:21:34.911+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:21:34.910+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1897
[2024-11-25T18:21:34.928+0000] {processor.py:153} INFO - Started process (PID=1897) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:21:34.929+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:21:34.930+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:21:34.929+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:21:34.931+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:21:34.931+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:21:35.452+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:21:35.452+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:21:35.453+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:21:35.452+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:21:35.590+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:21:35.592+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:21:35.592+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:21:35.596+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:21:35.596+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:21:35.596+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:21:35.601+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:21:35.601+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:21:35.601+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:21:35.601+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:21:35.609+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:21:35.609+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:21:35.615+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:21:35.615+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:21:35.616+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:21:35.616+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:21:35.625+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:21:35.625+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:21:35.631+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.722 seconds
[2024-11-25T18:22:06.108+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:22:06.107+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1903)
[2024-11-25T18:22:06.110+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:22:06.110+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1903
[2024-11-25T18:22:06.127+0000] {processor.py:153} INFO - Started process (PID=1903) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:22:06.128+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:22:06.129+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:22:06.129+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:22:06.130+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:22:06.130+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:22:06.833+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:22:06.833+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:22:06.834+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:22:06.834+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:22:06.954+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:22:06.955+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:22:06.955+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:22:06.960+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:22:06.959+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:22:06.960+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:22:06.966+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:22:06.965+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:22:06.966+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:22:06.966+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:22:06.987+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:22:06.987+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:22:07.000+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:22:07.000+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:22:07.000+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:22:07.000+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:22:07.011+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:22:07.011+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:22:07.018+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.911 seconds
[2024-11-25T18:22:37.586+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:22:37.586+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1909)
[2024-11-25T18:22:37.587+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:22:37.587+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1909
[2024-11-25T18:22:37.592+0000] {processor.py:153} INFO - Started process (PID=1909) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:22:37.593+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:22:37.594+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:22:37.594+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:22:37.596+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:22:37.596+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:22:38.069+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:22:38.068+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:22:38.073+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:22:38.072+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:22:38.259+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:22:38.261+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:22:38.260+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:22:38.264+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:22:38.264+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:22:38.265+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:22:38.270+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:22:38.270+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:22:38.270+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:22:38.270+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:22:38.280+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:22:38.280+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:22:38.286+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:22:38.286+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:22:38.286+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:22:38.286+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:22:38.296+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:22:38.296+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:22:38.302+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.716 seconds
[2024-11-25T18:23:08.666+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:23:08.665+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1915)
[2024-11-25T18:23:08.667+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:23:08.667+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1915
[2024-11-25T18:23:08.671+0000] {processor.py:153} INFO - Started process (PID=1915) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:23:08.671+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:23:08.672+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:23:08.672+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:23:08.674+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:23:08.674+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:23:09.135+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:23:09.135+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:23:09.136+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:23:09.136+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:23:09.232+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:23:09.233+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:23:09.233+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:23:09.237+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:23:09.237+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:23:09.237+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:23:09.242+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:23:09.242+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:23:09.243+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:23:09.243+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:23:09.251+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:23:09.251+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:23:09.257+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:23:09.257+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:23:09.257+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:23:09.257+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:23:09.275+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:23:09.275+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:23:09.290+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.625 seconds
[2024-11-25T18:23:39.339+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:23:39.339+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1921)
[2024-11-25T18:23:39.340+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:23:39.340+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1921
[2024-11-25T18:23:39.345+0000] {processor.py:153} INFO - Started process (PID=1921) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:23:39.346+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:23:39.347+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:23:39.347+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:23:39.348+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:23:39.348+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:23:39.859+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:23:39.859+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:23:39.860+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:23:39.860+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:23:40.095+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:23:40.096+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:23:40.096+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:23:40.101+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:23:40.100+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:23:40.101+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:23:40.107+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:23:40.107+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:23:40.107+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:23:40.107+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:23:40.118+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:23:40.118+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:23:40.126+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:23:40.126+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:23:40.126+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:23:40.126+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:23:40.136+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:23:40.136+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:23:40.147+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.808 seconds
[2024-11-25T18:24:10.618+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:24:10.618+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1927)
[2024-11-25T18:24:10.620+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:24:10.619+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1927
[2024-11-25T18:24:10.628+0000] {processor.py:153} INFO - Started process (PID=1927) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:24:10.629+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:24:10.630+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:24:10.630+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:24:10.632+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:24:10.632+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:24:11.196+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:24:11.196+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:24:11.197+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:24:11.197+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:24:11.302+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:24:11.304+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:24:11.304+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:24:11.314+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:24:11.314+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:24:11.315+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:24:11.329+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:24:11.329+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:24:11.329+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:24:11.329+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:24:11.349+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:24:11.349+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:24:11.368+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:24:11.368+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:24:11.370+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:24:11.369+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:24:11.395+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:24:11.395+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:24:11.410+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.792 seconds
[2024-11-25T18:24:41.675+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:24:41.674+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1933)
[2024-11-25T18:24:41.676+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:24:41.676+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1933
[2024-11-25T18:24:41.689+0000] {processor.py:153} INFO - Started process (PID=1933) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:24:41.690+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:24:41.691+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:24:41.691+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:24:41.693+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:24:41.693+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:24:42.153+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:24:42.153+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:24:42.154+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:24:42.153+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:24:42.257+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:24:42.258+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:24:42.258+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:24:42.261+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:24:42.261+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:24:42.262+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:24:42.266+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:24:42.266+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:24:42.267+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:24:42.267+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:24:42.275+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:24:42.275+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:24:42.281+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:24:42.281+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:24:42.281+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:24:42.281+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:24:42.289+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:24:42.289+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:24:42.296+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.622 seconds
[2024-11-25T18:25:12.827+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:25:12.826+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1939)
[2024-11-25T18:25:12.828+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:25:12.828+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1939
[2024-11-25T18:25:12.834+0000] {processor.py:153} INFO - Started process (PID=1939) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:25:12.834+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:25:12.835+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:25:12.835+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:25:12.837+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:25:12.837+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:25:13.330+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:25:13.330+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:25:13.331+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:25:13.330+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:25:13.432+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:25:13.433+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:25:13.433+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:25:13.437+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:25:13.436+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:25:13.437+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:25:13.443+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:25:13.443+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:25:13.443+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:25:13.443+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:25:13.451+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:25:13.451+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:25:13.457+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:25:13.457+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:25:13.457+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:25:13.457+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:25:13.466+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:25:13.466+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:25:13.472+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.646 seconds
[2024-11-25T18:25:43.544+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:25:43.543+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1945)
[2024-11-25T18:25:43.545+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:25:43.545+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1945
[2024-11-25T18:25:43.553+0000] {processor.py:153} INFO - Started process (PID=1945) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:25:43.554+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:25:43.555+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:25:43.555+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:25:43.557+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:25:43.557+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:25:44.031+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:25:44.031+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:25:44.032+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:25:44.032+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:25:44.196+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:25:44.197+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:25:44.197+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:25:44.201+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:25:44.201+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:25:44.201+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:25:44.207+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:25:44.207+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:25:44.207+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:25:44.207+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:25:44.215+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:25:44.215+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:25:44.220+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:25:44.220+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:25:44.221+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:25:44.220+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:25:44.229+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:25:44.229+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:25:44.235+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.693 seconds
[2024-11-25T18:26:14.404+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:26:14.404+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1951)
[2024-11-25T18:26:14.406+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:26:14.406+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1951
[2024-11-25T18:26:14.411+0000] {processor.py:153} INFO - Started process (PID=1951) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:26:14.413+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:26:14.415+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:26:14.415+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:26:14.419+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:26:14.419+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:26:14.950+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:26:14.950+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:26:14.951+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:26:14.951+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:26:15.043+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:26:15.045+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:26:15.044+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:26:15.048+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:26:15.048+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:26:15.048+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:26:15.053+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:26:15.053+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:26:15.053+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:26:15.053+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:26:15.061+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:26:15.061+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:26:15.067+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:26:15.067+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:26:15.067+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:26:15.067+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:26:15.080+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:26:15.080+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:26:15.086+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.683 seconds
[2024-11-25T18:26:45.151+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:26:45.151+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1957)
[2024-11-25T18:26:45.152+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:26:45.152+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1957
[2024-11-25T18:26:45.157+0000] {processor.py:153} INFO - Started process (PID=1957) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:26:45.157+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:26:45.158+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:26:45.158+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:26:45.159+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:26:45.159+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:26:45.657+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:26:45.657+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:26:45.658+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:26:45.658+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:26:45.776+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:26:45.778+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:26:45.778+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:26:45.782+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:26:45.782+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:26:45.782+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:26:45.789+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:26:45.788+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:26:45.789+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:26:45.789+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:26:45.798+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:26:45.798+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:26:45.804+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:26:45.804+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:26:45.805+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:26:45.804+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:26:45.816+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:26:45.816+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:26:45.824+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.674 seconds
[2024-11-25T18:27:15.896+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:27:15.896+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1963)
[2024-11-25T18:27:15.897+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:27:15.897+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1963
[2024-11-25T18:27:15.902+0000] {processor.py:153} INFO - Started process (PID=1963) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:27:15.903+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:27:15.904+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:27:15.903+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:27:15.905+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:27:15.905+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:27:16.456+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:27:16.456+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:27:16.456+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:27:16.456+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:27:16.570+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:27:16.571+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:27:16.571+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:27:16.575+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:27:16.575+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:27:16.575+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:27:16.581+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:27:16.581+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:27:16.581+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:27:16.581+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:27:16.594+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:27:16.594+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:27:16.605+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:27:16.605+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:27:16.606+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:27:16.605+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:27:16.630+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:27:16.630+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:27:16.640+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.744 seconds
[2024-11-25T18:27:46.740+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:27:46.739+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1969)
[2024-11-25T18:27:46.741+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:27:46.741+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1969
[2024-11-25T18:27:46.755+0000] {processor.py:153} INFO - Started process (PID=1969) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:27:46.757+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:27:46.759+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:27:46.758+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:27:46.760+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:27:46.760+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:27:47.282+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:27:47.282+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:27:47.283+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:27:47.283+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:27:47.374+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:27:47.376+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:27:47.376+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:27:47.379+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:27:47.379+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:27:47.379+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:27:47.384+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:27:47.384+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:27:47.385+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:27:47.385+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:27:47.392+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:27:47.392+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:27:47.398+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:27:47.398+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:27:47.398+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:27:47.398+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:27:47.407+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:27:47.407+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:27:47.416+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.677 seconds
[2024-11-25T18:28:17.602+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:28:17.601+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1975)
[2024-11-25T18:28:17.603+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:28:17.602+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1975
[2024-11-25T18:28:17.612+0000] {processor.py:153} INFO - Started process (PID=1975) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:28:17.613+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:28:17.613+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:28:17.613+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:28:17.616+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:28:17.616+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:28:21.068+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:28:21.068+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:28:21.069+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:28:21.068+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:28:21.458+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:28:21.461+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:28:21.461+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:28:21.466+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:28:21.466+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:28:21.467+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:28:21.514+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:28:21.513+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:28:21.515+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:28:21.515+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:28:21.547+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:28:21.547+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:28:21.562+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:28:21.562+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:28:21.567+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:28:21.562+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:28:21.732+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:28:21.731+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:28:21.782+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 4.180 seconds
[2024-11-25T18:28:52.063+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:28:52.063+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1981)
[2024-11-25T18:28:52.064+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:28:52.064+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1981
[2024-11-25T18:28:52.081+0000] {processor.py:153} INFO - Started process (PID=1981) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:28:52.082+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:28:52.085+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:28:52.084+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:28:52.091+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:28:52.091+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:28:53.706+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:28:53.706+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:28:53.707+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:28:53.706+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:28:53.886+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:28:53.890+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:28:53.890+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:28:53.895+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:28:53.895+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:28:53.896+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:28:53.904+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:28:53.904+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:28:53.904+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:28:53.904+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:28:53.944+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:28:53.944+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:28:53.956+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:28:53.956+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:28:53.957+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:28:53.957+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:28:53.972+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:28:53.972+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:28:53.981+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 1.919 seconds
[2024-11-25T18:29:24.316+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:29:24.314+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1987)
[2024-11-25T18:29:24.318+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:29:24.318+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1987
[2024-11-25T18:29:24.333+0000] {processor.py:153} INFO - Started process (PID=1987) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:29:24.333+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:29:24.335+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:29:24.335+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:29:24.338+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:29:24.338+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:29:25.135+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:29:25.135+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:29:25.136+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:29:25.136+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:29:25.277+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:29:25.278+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:29:25.278+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:29:25.285+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:29:25.285+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:29:25.285+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:29:25.293+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:29:25.293+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:29:25.293+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:29:25.293+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:29:25.303+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:29:25.303+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:29:25.309+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:29:25.309+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:29:25.309+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:29:25.309+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:29:25.319+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:29:25.319+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:29:25.330+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 1.017 seconds
[2024-11-25T18:29:56.013+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:29:56.012+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1993)
[2024-11-25T18:29:56.015+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:29:56.015+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1993
[2024-11-25T18:29:56.022+0000] {processor.py:153} INFO - Started process (PID=1993) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:29:56.023+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:29:56.025+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:29:56.025+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:29:56.028+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:29:56.028+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:29:56.581+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:29:56.580+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:29:56.581+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:29:56.581+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:29:56.681+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:29:56.682+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:29:56.682+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:29:56.686+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:29:56.686+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:29:56.686+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:29:56.692+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:29:56.692+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:29:56.692+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:29:56.692+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:29:56.702+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:29:56.702+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:29:56.708+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:29:56.708+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:29:56.708+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:29:56.708+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:29:56.729+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:29:56.728+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:29:56.738+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.726 seconds
[2024-11-25T18:30:27.392+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:30:27.392+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 1999)
[2024-11-25T18:30:27.393+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:30:27.393+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1999
[2024-11-25T18:30:27.396+0000] {processor.py:153} INFO - Started process (PID=1999) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:30:27.396+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:30:27.397+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:30:27.397+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:30:27.398+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:30:27.398+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:30:27.883+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:30:27.883+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:30:27.884+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:30:27.884+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:30:27.990+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:30:27.993+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:30:27.993+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:30:28.002+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:30:28.001+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:30:28.002+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:30:28.012+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:30:28.012+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:30:28.013+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:30:28.013+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:30:28.024+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:30:28.024+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:30:28.030+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:30:28.030+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:30:28.031+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:30:28.030+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:30:28.041+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:30:28.041+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:30:28.051+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.660 seconds
[2024-11-25T18:30:58.547+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:30:58.547+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2005)
[2024-11-25T18:30:58.548+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:30:58.548+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2005
[2024-11-25T18:30:58.555+0000] {processor.py:153} INFO - Started process (PID=2005) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:30:58.556+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:30:58.557+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:30:58.557+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:30:58.558+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:30:58.558+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:30:59.116+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:30:59.116+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:30:59.116+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:30:59.116+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:30:59.212+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:30:59.214+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:30:59.214+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:30:59.220+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:30:59.220+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:30:59.220+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:30:59.242+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:30:59.242+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:30:59.242+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:30:59.242+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:30:59.256+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:30:59.256+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:30:59.263+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:30:59.263+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:30:59.263+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:30:59.263+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:30:59.276+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:30:59.276+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:30:59.282+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.735 seconds
[2024-11-25T18:31:29.612+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:31:29.611+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2011)
[2024-11-25T18:31:29.612+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:31:29.612+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2011
[2024-11-25T18:31:29.616+0000] {processor.py:153} INFO - Started process (PID=2011) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:31:29.617+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:31:29.617+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:31:29.617+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:31:29.618+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:31:29.618+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:31:30.243+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:31:30.243+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:31:30.245+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:31:30.244+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:31:30.397+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:31:30.400+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:31:30.400+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:31:30.407+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:31:30.407+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:31:30.408+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:31:30.419+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:31:30.419+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:31:30.420+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:31:30.420+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:31:30.436+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:31:30.436+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:31:30.449+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:31:30.449+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:31:30.450+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:31:30.449+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:31:30.470+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:31:30.470+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:31:30.490+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.878 seconds
[2024-11-25T18:32:00.697+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:32:00.696+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2017)
[2024-11-25T18:32:00.698+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:32:00.698+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2017
[2024-11-25T18:32:00.708+0000] {processor.py:153} INFO - Started process (PID=2017) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:32:00.709+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:32:00.710+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:32:00.710+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:32:00.711+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:32:00.711+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:32:01.221+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:32:01.221+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:32:01.222+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:32:01.221+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:32:01.350+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:32:01.352+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:32:01.352+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:32:01.371+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:32:01.371+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:32:01.372+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:32:01.380+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:32:01.380+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:32:01.380+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:32:01.380+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:32:01.393+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:32:01.393+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:32:01.402+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:32:01.402+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:32:01.402+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:32:01.402+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:32:01.417+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:32:01.417+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:32:01.428+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.732 seconds
[2024-11-25T18:32:31.830+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:32:31.829+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2023)
[2024-11-25T18:32:31.830+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:32:31.830+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2023
[2024-11-25T18:32:31.835+0000] {processor.py:153} INFO - Started process (PID=2023) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:32:31.836+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:32:31.837+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:32:31.837+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:32:31.838+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:32:31.838+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:32:32.304+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:32:32.303+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:32:32.304+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:32:32.304+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:32:32.403+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:32:32.406+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:32:32.406+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:32:32.415+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:32:32.415+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:32:32.415+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:32:32.424+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:32:32.424+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:32:32.424+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:32:32.424+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:32:32.437+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:32:32.436+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:32:32.443+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:32:32.443+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:32:32.444+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:32:32.443+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:32:32.463+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:32:32.463+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:32:32.473+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.644 seconds
[2024-11-25T18:33:02.681+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:33:02.680+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2029)
[2024-11-25T18:33:02.682+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:33:02.682+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2029
[2024-11-25T18:33:02.688+0000] {processor.py:153} INFO - Started process (PID=2029) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:33:02.689+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:33:02.690+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:33:02.689+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:33:02.691+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:33:02.691+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:33:03.190+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:33:03.190+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:33:03.190+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:33:03.190+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:33:03.283+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:33:03.285+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:33:03.285+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:33:03.289+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:33:03.289+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:33:03.289+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:33:03.295+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:33:03.295+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:33:03.295+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:33:03.295+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:33:03.304+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:33:03.304+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:33:03.310+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:33:03.310+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:33:03.310+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:33:03.310+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:33:03.326+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:33:03.326+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:33:03.341+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.661 seconds
[2024-11-25T18:33:33.767+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:33:33.766+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2035)
[2024-11-25T18:33:33.768+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:33:33.768+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2035
[2024-11-25T18:33:33.774+0000] {processor.py:153} INFO - Started process (PID=2035) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:33:33.775+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:33:33.775+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:33:33.775+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:33:33.777+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:33:33.777+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:33:34.270+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:33:34.270+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:33:34.271+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:33:34.270+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:33:34.366+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:33:34.368+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:33:34.368+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:33:34.371+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:33:34.371+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:33:34.371+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:33:34.376+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:33:34.376+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:33:34.377+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:33:34.377+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:33:34.385+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:33:34.385+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:33:34.391+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:33:34.391+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:33:34.391+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:33:34.391+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:33:34.412+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:33:34.412+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:33:34.419+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.653 seconds
[2024-11-25T18:34:04.762+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:34:04.762+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2041)
[2024-11-25T18:34:04.763+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:34:04.763+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2041
[2024-11-25T18:34:04.778+0000] {processor.py:153} INFO - Started process (PID=2041) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:34:04.779+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:34:04.780+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:34:04.780+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:34:04.781+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:34:04.781+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:34:05.307+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:34:05.307+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:34:05.309+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:34:05.309+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:34:05.417+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:34:05.419+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:34:05.419+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:34:05.422+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:34:05.422+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:34:05.423+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:34:05.428+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:34:05.428+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:34:05.428+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:34:05.428+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:34:05.441+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:34:05.441+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:34:05.447+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:34:05.447+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:34:05.447+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:34:05.447+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:34:05.457+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:34:05.457+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:34:05.465+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.703 seconds
[2024-11-25T18:34:35.887+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:34:35.886+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2047)
[2024-11-25T18:34:35.887+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:34:35.887+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2047
[2024-11-25T18:34:35.890+0000] {processor.py:153} INFO - Started process (PID=2047) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:34:35.890+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:34:35.891+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:34:35.891+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:34:35.892+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:34:35.892+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:34:36.392+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:34:36.392+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:34:36.393+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:34:36.393+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:34:36.499+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:34:36.501+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:34:36.501+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:34:36.504+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:34:36.504+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:34:36.504+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:34:36.510+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:34:36.510+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:34:36.510+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:34:36.510+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:34:36.520+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:34:36.520+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:34:36.526+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:34:36.526+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:34:36.527+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:34:36.527+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:34:36.535+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:34:36.535+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:34:36.542+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.656 seconds
[2024-11-25T18:35:06.814+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:35:06.814+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2053)
[2024-11-25T18:35:06.815+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:35:06.815+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2053
[2024-11-25T18:35:06.821+0000] {processor.py:153} INFO - Started process (PID=2053) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:35:06.822+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:35:06.823+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:35:06.823+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:35:06.825+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:35:06.825+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:35:07.445+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:35:07.444+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:35:07.445+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:35:07.445+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:35:07.583+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:35:07.585+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:35:07.585+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:35:07.589+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:35:07.589+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:35:07.589+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:35:07.595+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:35:07.595+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:35:07.596+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:35:07.596+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:35:07.613+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:35:07.613+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:35:07.638+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:35:07.637+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:35:07.649+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:35:07.643+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:35:07.793+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:35:07.792+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:35:07.810+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.997 seconds
[2024-11-25T18:35:38.423+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:35:38.422+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2059)
[2024-11-25T18:35:38.425+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:35:38.425+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2059
[2024-11-25T18:35:38.440+0000] {processor.py:153} INFO - Started process (PID=2059) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:35:38.441+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:35:38.443+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:35:38.443+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:35:38.444+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:35:38.444+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:35:38.902+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:35:38.902+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:35:38.903+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:35:38.903+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:35:39.015+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:35:39.016+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:35:39.016+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:35:39.020+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:35:39.020+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:35:39.020+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:35:39.025+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:35:39.025+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:35:39.026+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:35:39.026+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:35:39.038+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:35:39.038+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:35:39.044+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:35:39.044+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:35:39.044+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:35:39.044+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:35:39.053+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:35:39.053+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:35:39.059+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.637 seconds
[2024-11-25T18:36:09.385+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:36:09.385+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2065)
[2024-11-25T18:36:09.386+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:36:09.386+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2065
[2024-11-25T18:36:09.391+0000] {processor.py:153} INFO - Started process (PID=2065) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:36:09.392+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:36:09.392+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:36:09.392+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:36:09.393+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:36:09.393+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:36:10.111+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:36:10.111+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:36:10.113+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:36:10.112+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:36:10.281+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:36:10.288+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:36:10.288+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:36:10.296+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:36:10.296+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:36:10.297+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:36:10.308+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:36:10.308+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:36:10.308+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:36:10.308+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:36:10.348+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:36:10.347+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:36:10.360+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:36:10.360+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:36:10.361+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:36:10.360+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:36:10.378+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:36:10.378+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:36:10.395+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 1.010 seconds
[2024-11-25T18:36:40.470+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:36:40.470+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2071)
[2024-11-25T18:36:40.471+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:36:40.471+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2071
[2024-11-25T18:36:40.490+0000] {processor.py:153} INFO - Started process (PID=2071) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:36:40.492+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:36:40.493+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:36:40.493+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:36:40.496+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:36:40.496+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:36:41.029+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:36:41.028+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:36:41.030+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:36:41.029+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:36:41.133+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:36:41.135+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:36:41.135+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:36:41.139+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:36:41.139+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:36:41.139+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:36:41.146+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:36:41.146+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:36:41.146+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:36:41.146+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:36:41.154+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:36:41.154+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:36:41.159+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:36:41.159+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:36:41.160+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:36:41.159+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:36:41.169+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:36:41.169+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:36:41.175+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.706 seconds
[2024-11-25T18:37:11.626+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:37:11.625+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2077)
[2024-11-25T18:37:11.627+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:37:11.627+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2077
[2024-11-25T18:37:11.635+0000] {processor.py:153} INFO - Started process (PID=2077) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:37:11.636+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:37:11.637+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:37:11.637+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:37:11.639+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:37:11.639+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:37:12.415+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:37:12.415+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:37:12.416+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:37:12.416+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:37:12.527+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:37:12.529+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:37:12.529+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:37:12.533+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:37:12.533+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:37:12.533+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:37:12.540+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:37:12.540+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:37:12.540+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:37:12.540+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:37:12.555+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:37:12.555+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:37:12.561+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:37:12.561+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:37:12.561+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:37:12.561+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:37:12.571+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:37:12.571+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:37:12.579+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.955 seconds
[2024-11-25T18:37:43.101+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:37:43.100+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2083)
[2024-11-25T18:37:43.102+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:37:43.102+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2083
[2024-11-25T18:37:43.107+0000] {processor.py:153} INFO - Started process (PID=2083) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:37:43.108+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:37:43.109+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:37:43.109+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:37:43.110+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:37:43.110+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:37:43.701+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:37:43.701+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:37:43.701+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:37:43.701+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:37:43.874+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:37:43.876+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:37:43.876+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:37:43.879+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:37:43.879+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:37:43.880+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:37:43.885+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:37:43.885+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:37:43.885+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:37:43.885+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:37:43.895+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:37:43.895+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:37:43.900+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:37:43.900+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:37:43.901+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:37:43.901+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:37:43.912+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:37:43.911+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:37:43.919+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.819 seconds
[2024-11-25T18:38:14.432+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:38:14.431+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2089)
[2024-11-25T18:38:14.432+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:38:14.432+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2089
[2024-11-25T18:38:14.435+0000] {processor.py:153} INFO - Started process (PID=2089) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:38:14.436+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:38:14.436+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:38:14.436+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:38:14.437+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:38:14.437+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:38:14.901+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:38:14.901+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:38:14.902+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:38:14.901+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:38:15.124+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:38:15.126+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:38:15.126+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:38:15.131+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:38:15.130+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:38:15.131+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:38:15.137+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:38:15.137+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:38:15.137+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:38:15.137+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:38:15.149+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:38:15.149+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:38:15.157+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:38:15.157+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:38:15.157+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:38:15.157+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:38:15.170+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:38:15.170+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:38:15.178+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.746 seconds
[2024-11-25T18:38:45.652+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:38:45.652+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2095)
[2024-11-25T18:38:45.653+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:38:45.653+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2095
[2024-11-25T18:38:45.655+0000] {processor.py:153} INFO - Started process (PID=2095) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:38:45.656+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:38:45.656+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:38:45.656+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:38:45.657+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:38:45.657+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:38:46.190+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:38:46.190+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:38:46.190+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:38:46.190+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:38:46.290+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:38:46.291+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:38:46.291+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:38:46.295+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:38:46.295+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:38:46.295+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:38:46.301+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:38:46.301+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:38:46.301+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:38:46.301+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:38:46.312+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:38:46.312+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:38:46.318+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:38:46.318+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:38:46.318+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:38:46.318+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:38:46.327+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:38:46.327+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:38:46.334+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.682 seconds
[2024-11-25T18:39:16.706+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:39:16.706+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2101)
[2024-11-25T18:39:16.707+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:39:16.707+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2101
[2024-11-25T18:39:16.710+0000] {processor.py:153} INFO - Started process (PID=2101) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:39:16.710+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:39:16.711+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:39:16.711+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:39:16.712+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:39:16.712+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:39:17.189+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:39:17.189+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:39:17.190+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:39:17.190+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:39:17.295+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:39:17.297+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:39:17.297+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:39:17.300+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:39:17.300+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:39:17.300+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:39:17.306+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:39:17.306+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:39:17.306+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:39:17.306+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:39:17.317+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:39:17.317+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:39:17.328+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:39:17.327+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:39:17.328+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:39:17.328+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:39:17.343+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:39:17.343+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:39:17.351+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.646 seconds
[2024-11-25T18:39:47.832+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:39:47.831+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2107)
[2024-11-25T18:39:47.834+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:39:47.834+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2107
[2024-11-25T18:39:47.848+0000] {processor.py:153} INFO - Started process (PID=2107) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:39:47.849+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:39:47.849+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:39:47.849+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:39:47.851+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:39:47.851+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:39:48.358+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:39:48.357+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:39:48.358+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:39:48.358+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:39:48.458+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:39:48.459+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:39:48.459+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:39:48.463+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:39:48.462+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:39:48.463+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:39:48.468+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:39:48.468+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:39:48.468+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:39:48.468+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:39:48.479+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:39:48.479+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:39:48.485+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:39:48.485+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:39:48.485+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:39:48.485+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:39:48.509+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:39:48.509+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:39:48.516+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.685 seconds
[2024-11-25T18:40:19.023+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:40:19.022+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2113)
[2024-11-25T18:40:19.024+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:40:19.024+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2113
[2024-11-25T18:40:19.030+0000] {processor.py:153} INFO - Started process (PID=2113) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:40:19.031+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:40:19.032+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:40:19.032+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:40:19.034+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:40:19.034+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:40:19.560+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:40:19.560+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:40:19.561+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:40:19.561+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:40:19.665+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:40:19.668+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:40:19.668+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:40:19.671+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:40:19.671+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:40:19.671+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:40:19.676+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:40:19.676+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:40:19.677+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:40:19.677+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:40:19.687+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:40:19.687+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:40:19.693+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:40:19.693+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:40:19.694+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:40:19.693+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:40:19.704+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:40:19.704+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:40:19.713+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.690 seconds
[2024-11-25T18:40:50.160+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:40:50.159+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2119)
[2024-11-25T18:40:50.162+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:40:50.162+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2119
[2024-11-25T18:40:50.166+0000] {processor.py:153} INFO - Started process (PID=2119) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:40:50.167+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:40:50.168+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:40:50.167+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:40:50.169+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:40:50.169+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:40:50.804+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:40:50.804+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:40:50.805+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:40:50.804+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:40:50.930+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:40:50.931+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:40:50.931+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:40:50.936+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:40:50.936+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:40:50.936+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:40:50.944+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:40:50.944+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:40:50.944+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:40:50.944+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:40:50.977+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:40:50.977+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:40:50.987+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:40:50.987+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:40:50.988+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:40:50.988+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:40:51.002+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:40:51.002+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:40:51.011+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.852 seconds
[2024-11-25T18:41:21.700+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:41:21.699+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2125)
[2024-11-25T18:41:21.703+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:41:21.703+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2125
[2024-11-25T18:41:21.707+0000] {processor.py:153} INFO - Started process (PID=2125) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:41:21.708+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:41:21.709+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:41:21.708+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:41:21.711+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:41:21.711+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:41:22.201+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:41:22.201+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:41:22.202+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:41:22.201+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:41:22.309+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:41:22.311+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:41:22.311+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:41:22.314+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:41:22.314+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:41:22.314+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:41:22.320+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:41:22.320+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:41:22.320+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:41:22.320+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:41:22.329+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:41:22.329+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:41:22.335+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:41:22.335+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:41:22.336+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:41:22.336+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:41:22.344+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:41:22.344+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:41:22.351+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.652 seconds
[2024-11-25T18:41:52.884+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:41:52.883+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2131)
[2024-11-25T18:41:52.885+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:41:52.885+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2131
[2024-11-25T18:41:52.898+0000] {processor.py:153} INFO - Started process (PID=2131) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:41:52.900+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:41:52.902+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:41:52.902+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:41:52.903+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:41:52.903+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:41:53.418+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:41:53.418+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:41:53.418+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:41:53.418+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:41:53.528+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:41:53.530+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:41:53.530+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:41:53.533+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:41:53.533+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:41:53.533+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:41:53.539+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:41:53.539+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:41:53.539+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:41:53.539+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:41:53.547+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:41:53.547+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:41:53.552+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:41:53.552+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:41:53.553+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:41:53.553+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:41:53.561+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:41:53.561+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:41:53.568+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.686 seconds
[2024-11-25T18:42:23.967+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:42:23.966+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2137)
[2024-11-25T18:42:23.968+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:42:23.968+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2137
[2024-11-25T18:42:23.976+0000] {processor.py:153} INFO - Started process (PID=2137) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:42:23.977+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:42:23.978+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:42:23.978+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:42:23.979+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:42:23.979+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:42:24.480+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:42:24.480+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:42:24.481+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:42:24.481+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:42:24.595+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:42:24.597+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:42:24.596+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:42:24.600+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:42:24.600+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:42:24.600+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:42:24.606+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:42:24.606+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:42:24.606+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:42:24.606+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:42:24.615+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:42:24.614+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:42:24.620+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:42:24.620+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:42:24.620+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:42:24.620+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:42:24.629+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:42:24.629+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:42:24.635+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.669 seconds
[2024-11-25T18:42:55.042+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:42:55.041+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2143)
[2024-11-25T18:42:55.045+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:42:55.045+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2143
[2024-11-25T18:42:55.054+0000] {processor.py:153} INFO - Started process (PID=2143) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:42:55.054+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:42:55.055+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:42:55.055+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:42:55.056+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:42:55.056+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:42:55.530+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:42:55.530+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:42:55.530+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:42:55.530+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:42:55.621+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:42:55.622+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:42:55.622+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:42:55.626+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:42:55.626+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:42:55.626+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:42:55.631+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:42:55.631+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:42:55.631+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:42:55.631+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:42:55.641+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:42:55.641+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:42:55.646+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:42:55.646+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:42:55.646+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:42:55.646+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:42:55.656+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:42:55.656+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:42:55.662+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.622 seconds
[2024-11-25T18:43:26.274+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:43:26.273+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2149)
[2024-11-25T18:43:26.275+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:43:26.275+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2149
[2024-11-25T18:43:26.283+0000] {processor.py:153} INFO - Started process (PID=2149) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:43:26.284+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:43:26.285+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:43:26.285+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:43:26.287+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:43:26.287+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:43:26.839+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:43:26.839+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:43:26.839+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:43:26.839+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:43:26.935+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:43:26.936+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:43:26.936+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:43:26.940+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:43:26.939+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:43:26.940+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:43:26.945+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:43:26.945+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:43:26.945+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:43:26.945+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:43:26.962+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:43:26.962+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:43:26.970+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:43:26.970+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:43:26.971+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:43:26.971+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:43:26.982+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:43:26.982+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:43:26.990+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.717 seconds
[2024-11-25T18:43:57.451+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:43:57.451+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2155)
[2024-11-25T18:43:57.452+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:43:57.452+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2155
[2024-11-25T18:43:57.458+0000] {processor.py:153} INFO - Started process (PID=2155) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:43:57.459+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:43:57.460+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:43:57.460+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:43:57.461+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:43:57.461+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:43:58.099+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:43:58.099+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:43:58.101+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:43:58.100+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:43:58.200+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:43:58.202+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:43:58.201+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:43:58.205+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:43:58.205+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:43:58.205+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:43:58.211+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:43:58.211+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:43:58.211+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:43:58.211+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:43:58.219+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:43:58.219+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:43:58.225+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:43:58.225+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:43:58.226+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:43:58.226+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:43:58.246+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:43:58.246+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:43:58.255+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.804 seconds
[2024-11-25T18:44:28.478+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:44:28.478+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2161)
[2024-11-25T18:44:28.480+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:44:28.480+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2161
[2024-11-25T18:44:28.483+0000] {processor.py:153} INFO - Started process (PID=2161) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:44:28.484+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:44:28.484+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:44:28.484+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:44:28.486+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:44:28.486+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:44:28.943+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:44:28.943+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:44:28.944+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:44:28.944+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:44:29.037+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:44:29.038+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:44:29.038+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:44:29.042+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:44:29.041+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:44:29.042+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:44:29.047+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:44:29.047+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:44:29.047+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:44:29.047+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:44:29.054+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:44:29.054+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:44:29.060+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:44:29.060+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:44:29.061+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:44:29.060+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:44:29.069+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:44:29.069+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:44:29.075+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.598 seconds
[2024-11-25T18:44:59.170+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:44:59.170+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2167)
[2024-11-25T18:44:59.171+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:44:59.171+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2167
[2024-11-25T18:44:59.191+0000] {processor.py:153} INFO - Started process (PID=2167) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:44:59.192+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:44:59.193+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:44:59.193+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:44:59.196+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:44:59.195+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:44:59.692+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:44:59.692+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:44:59.693+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:44:59.692+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:44:59.790+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:44:59.792+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:44:59.791+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:44:59.795+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:44:59.795+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:44:59.795+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:44:59.800+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:44:59.800+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:44:59.800+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:44:59.800+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:44:59.807+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:44:59.807+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:44:59.813+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:44:59.813+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:44:59.814+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:44:59.814+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:44:59.829+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:44:59.829+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:44:59.845+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.675 seconds
[2024-11-25T18:45:29.976+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:45:29.976+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2173)
[2024-11-25T18:45:29.977+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:45:29.977+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2173
[2024-11-25T18:45:29.982+0000] {processor.py:153} INFO - Started process (PID=2173) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:45:29.983+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:45:29.984+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:45:29.984+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:45:29.986+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:45:29.986+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:45:30.500+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:45:30.500+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:45:30.501+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:45:30.500+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:45:30.607+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:45:30.608+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:45:30.608+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:45:30.611+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:45:30.611+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:45:30.612+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:45:30.617+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:45:30.617+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:45:30.617+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:45:30.617+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:45:30.629+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:45:30.628+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:45:30.634+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:45:30.634+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:45:30.635+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:45:30.634+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:45:30.643+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:45:30.643+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:45:30.650+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.674 seconds
[2024-11-25T18:46:00.896+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:46:00.895+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2179)
[2024-11-25T18:46:00.897+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:46:00.897+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2179
[2024-11-25T18:46:00.904+0000] {processor.py:153} INFO - Started process (PID=2179) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:46:00.905+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:46:00.906+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:46:00.906+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:46:00.909+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:46:00.908+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:46:01.423+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:46:01.422+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:46:01.423+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:46:01.423+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:46:01.530+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:46:01.532+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:46:01.532+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:46:01.536+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:46:01.536+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:46:01.536+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:46:01.547+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:46:01.547+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:46:01.547+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:46:01.547+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:46:01.559+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:46:01.559+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:46:01.570+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:46:01.570+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:46:01.571+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:46:01.570+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:46:01.586+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:46:01.586+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:46:01.595+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.700 seconds
[2024-11-25T18:46:32.112+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:46:32.112+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2185)
[2024-11-25T18:46:32.113+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:46:32.113+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2185
[2024-11-25T18:46:32.119+0000] {processor.py:153} INFO - Started process (PID=2185) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:46:32.120+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:46:32.121+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:46:32.121+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:46:32.122+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:46:32.122+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:46:32.626+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:46:32.626+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:46:32.627+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:46:32.626+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:46:32.726+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:46:32.728+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:46:32.728+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:46:32.731+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:46:32.731+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:46:32.732+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:46:32.737+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:46:32.737+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:46:32.737+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:46:32.737+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:46:32.748+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:46:32.748+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:46:32.754+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:46:32.754+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:46:32.754+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:46:32.754+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:46:32.763+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:46:32.763+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:46:32.771+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.659 seconds
[2024-11-25T18:47:03.079+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:47:03.079+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2191)
[2024-11-25T18:47:03.080+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:47:03.080+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2191
[2024-11-25T18:47:03.085+0000] {processor.py:153} INFO - Started process (PID=2191) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:47:03.086+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:47:03.087+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:47:03.087+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:47:03.089+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:47:03.089+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:47:03.556+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:47:03.556+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:47:03.557+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:47:03.557+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:47:03.649+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:47:03.650+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:47:03.650+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:47:03.654+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:47:03.654+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:47:03.654+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:47:03.659+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:47:03.659+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:47:03.659+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:47:03.659+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:47:03.673+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:47:03.673+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:47:03.679+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:47:03.679+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:47:03.679+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:47:03.679+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:47:03.688+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:47:03.687+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:47:03.694+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.615 seconds
[2024-11-25T18:47:34.198+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:47:34.198+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2197)
[2024-11-25T18:47:34.199+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:47:34.199+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2197
[2024-11-25T18:47:34.204+0000] {processor.py:153} INFO - Started process (PID=2197) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:47:34.205+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:47:34.206+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:47:34.206+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:47:34.208+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:47:34.208+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:47:34.669+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:47:34.669+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:47:34.669+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:47:34.669+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:47:34.763+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:47:34.765+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:47:34.765+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:47:34.768+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:47:34.768+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:47:34.768+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:47:34.773+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:47:34.773+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:47:34.774+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:47:34.774+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:47:34.784+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:47:34.784+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:47:34.789+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:47:34.789+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:47:34.790+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:47:34.789+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:47:34.820+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:47:34.819+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:47:34.866+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.669 seconds
[2024-11-25T18:48:05.113+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:48:05.113+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2203)
[2024-11-25T18:48:05.114+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:48:05.114+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2203
[2024-11-25T18:48:05.120+0000] {processor.py:153} INFO - Started process (PID=2203) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:48:05.120+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:48:05.121+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:48:05.121+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:48:05.123+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:48:05.123+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:48:05.623+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:48:05.623+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:48:05.623+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:48:05.623+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:48:05.726+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:48:05.727+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:48:05.727+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:48:05.731+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:48:05.731+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:48:05.731+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:48:05.738+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:48:05.738+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:48:05.738+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:48:05.738+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:48:05.758+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:48:05.758+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:48:05.766+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:48:05.766+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:48:05.767+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:48:05.766+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:48:05.776+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:48:05.776+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:48:05.783+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.671 seconds
[2024-11-25T18:48:36.489+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:48:36.489+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2209)
[2024-11-25T18:48:36.490+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:48:36.490+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2209
[2024-11-25T18:48:36.494+0000] {processor.py:153} INFO - Started process (PID=2209) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:48:36.495+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:48:36.496+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:48:36.495+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:48:36.497+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:48:36.497+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:48:37.005+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:48:37.005+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:48:37.005+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:48:37.005+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:48:37.114+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:48:37.115+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:48:37.115+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:48:37.119+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:48:37.119+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:48:37.119+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:48:37.128+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:48:37.128+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:48:37.128+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:48:37.128+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:48:37.141+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:48:37.141+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:48:37.147+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:48:37.147+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:48:37.148+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:48:37.147+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:48:37.158+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:48:37.158+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:48:37.165+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.676 seconds
[2024-11-25T18:49:08.096+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:49:08.095+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2215)
[2024-11-25T18:49:08.098+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:49:08.098+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2215
[2024-11-25T18:49:08.106+0000] {processor.py:153} INFO - Started process (PID=2215) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:49:08.107+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:49:08.108+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:49:08.108+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:49:08.111+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:49:08.111+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:49:08.953+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:49:08.953+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:49:08.954+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:49:08.954+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:49:09.105+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:49:09.107+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:49:09.107+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:49:09.113+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:49:09.113+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:49:09.113+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:49:09.122+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:49:09.122+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:49:09.122+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:49:09.122+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:49:09.137+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:49:09.136+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:49:09.146+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:49:09.146+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:49:09.147+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:49:09.146+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:49:09.188+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:49:09.188+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:49:09.196+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 1.101 seconds
[2024-11-25T18:49:39.814+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:49:39.813+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2221)
[2024-11-25T18:49:39.815+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:49:39.815+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2221
[2024-11-25T18:49:39.822+0000] {processor.py:153} INFO - Started process (PID=2221) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:49:39.823+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:49:39.824+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:49:39.824+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:49:39.825+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:49:39.825+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:49:40.378+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:49:40.378+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:49:40.379+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:49:40.379+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:49:40.480+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:49:40.481+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:49:40.481+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:49:40.485+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:49:40.485+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:49:40.485+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:49:40.491+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:49:40.491+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:49:40.491+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:49:40.491+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:49:40.504+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:49:40.504+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:49:40.510+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:49:40.510+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:49:40.511+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:49:40.510+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:49:40.532+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:49:40.532+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:49:40.541+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.729 seconds
[2024-11-25T18:50:11.215+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:50:11.215+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2227)
[2024-11-25T18:50:11.216+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:50:11.216+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2227
[2024-11-25T18:50:11.220+0000] {processor.py:153} INFO - Started process (PID=2227) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:50:11.220+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:50:11.221+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:50:11.221+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:50:11.222+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:50:11.222+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:50:12.098+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:50:12.097+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:50:12.101+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:50:12.100+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:50:12.254+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:50:12.256+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:50:12.256+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:50:12.262+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:50:12.261+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:50:12.262+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:50:12.272+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:50:12.271+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:50:12.272+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:50:12.272+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:50:12.284+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:50:12.284+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:50:12.297+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:50:12.297+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:50:12.299+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:50:12.298+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:50:12.320+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:50:12.320+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:50:12.332+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 1.118 seconds
[2024-11-25T18:50:43.358+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:50:43.358+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2233)
[2024-11-25T18:50:43.358+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:50:43.358+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2233
[2024-11-25T18:50:43.362+0000] {processor.py:153} INFO - Started process (PID=2233) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:50:43.363+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:50:43.364+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:50:43.364+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:50:43.365+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:50:43.365+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:50:44.007+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:50:44.006+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:50:44.007+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:50:44.007+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:50:44.109+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:50:44.111+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:50:44.111+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:50:44.115+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:50:44.115+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:50:44.115+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:50:44.121+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:50:44.121+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:50:44.121+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:50:44.121+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:50:44.131+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:50:44.131+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:50:44.140+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:50:44.140+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:50:44.141+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:50:44.140+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:50:44.151+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:50:44.151+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:50:44.164+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.806 seconds
[2024-11-25T18:51:14.261+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:51:14.260+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2239)
[2024-11-25T18:51:14.261+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:51:14.261+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2239
[2024-11-25T18:51:14.268+0000] {processor.py:153} INFO - Started process (PID=2239) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:51:14.269+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:51:14.270+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:51:14.270+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:51:14.273+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:51:14.273+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:51:14.819+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:51:14.819+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:51:14.820+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:51:14.819+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:51:14.917+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:51:14.918+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:51:14.918+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:51:14.922+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:51:14.922+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:51:14.922+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:51:14.928+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:51:14.928+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:51:14.928+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:51:14.928+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:51:14.951+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:51:14.951+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:51:14.958+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:51:14.958+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:51:14.958+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:51:14.958+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:51:14.969+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:51:14.969+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:51:14.976+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.716 seconds
[2024-11-25T18:51:45.604+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:51:45.604+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2245)
[2024-11-25T18:51:45.605+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:51:45.605+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2245
[2024-11-25T18:51:45.614+0000] {processor.py:153} INFO - Started process (PID=2245) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:51:45.615+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:51:45.616+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:51:45.616+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:51:45.619+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:51:45.619+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:51:46.183+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:51:46.183+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:51:46.184+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:51:46.183+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:51:46.317+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:51:46.318+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:51:46.318+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:51:46.323+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:51:46.323+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:51:46.323+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:51:46.329+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:51:46.329+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:51:46.329+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:51:46.329+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:51:46.351+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:51:46.351+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:51:46.362+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:51:46.362+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:51:46.363+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:51:46.362+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:51:46.374+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:51:46.374+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:51:46.382+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.778 seconds
[2024-11-25T18:52:17.032+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:52:17.031+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2251)
[2024-11-25T18:52:17.033+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:52:17.033+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2251
[2024-11-25T18:52:17.047+0000] {processor.py:153} INFO - Started process (PID=2251) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:52:17.048+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:52:17.050+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:52:17.049+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:52:17.052+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:52:17.052+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:52:17.691+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:52:17.691+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:52:17.692+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:52:17.691+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:52:17.871+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:52:17.872+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:52:17.872+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:52:17.876+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:52:17.876+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:52:17.876+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:52:17.883+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:52:17.883+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:52:17.883+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:52:17.883+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:52:17.898+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:52:17.898+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:52:17.905+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:52:17.905+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:52:17.905+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:52:17.905+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:52:17.923+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:52:17.923+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:52:17.931+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.900 seconds
[2024-11-25T18:52:48.422+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:52:48.421+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2257)
[2024-11-25T18:52:48.422+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:52:48.422+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2257
[2024-11-25T18:52:48.425+0000] {processor.py:153} INFO - Started process (PID=2257) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:52:48.426+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:52:48.426+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:52:48.426+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:52:48.427+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:52:48.427+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:52:48.949+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:52:48.949+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:52:48.950+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:52:48.950+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:52:49.055+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:52:49.057+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:52:49.057+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:52:49.062+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:52:49.062+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:52:49.062+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:52:49.072+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:52:49.072+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:52:49.073+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:52:49.073+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:52:49.099+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:52:49.099+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:52:49.111+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:52:49.111+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:52:49.112+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:52:49.111+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:52:49.131+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:52:49.131+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:52:49.145+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.724 seconds
[2024-11-25T18:53:20.005+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:53:20.004+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2263)
[2024-11-25T18:53:20.007+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:53:20.006+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2263
[2024-11-25T18:53:20.015+0000] {processor.py:153} INFO - Started process (PID=2263) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:53:20.016+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:53:20.017+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:53:20.017+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:53:20.018+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:53:20.018+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:53:20.547+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:53:20.547+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:53:20.548+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:53:20.548+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:53:20.639+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:53:20.641+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:53:20.640+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:53:20.644+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:53:20.644+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:53:20.645+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:53:20.649+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:53:20.649+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:53:20.650+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:53:20.650+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:53:20.676+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:53:20.676+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:53:20.682+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:53:20.682+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:53:20.683+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:53:20.682+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:53:20.693+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:53:20.693+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:53:20.702+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.698 seconds
[2024-11-25T18:53:51.183+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:53:51.183+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2269)
[2024-11-25T18:53:51.184+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:53:51.184+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2269
[2024-11-25T18:53:51.192+0000] {processor.py:153} INFO - Started process (PID=2269) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:53:51.193+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:53:51.194+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:53:51.194+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:53:51.195+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:53:51.195+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:53:52.622+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:53:52.618+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:53:52.625+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:53:52.624+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:53:52.843+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:53:52.846+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:53:52.846+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:53:52.852+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:53:52.852+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:53:52.853+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:53:52.868+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:53:52.868+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:53:52.868+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:53:52.868+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:53:52.912+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:53:52.912+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:53:52.925+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:53:52.925+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:53:52.926+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:53:52.925+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:53:52.950+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:53:52.950+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:53:52.965+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 1.782 seconds
[2024-11-25T18:54:23.137+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:54:23.136+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2275)
[2024-11-25T18:54:23.138+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:54:23.138+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2275
[2024-11-25T18:54:23.145+0000] {processor.py:153} INFO - Started process (PID=2275) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:54:23.146+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:54:23.148+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:54:23.148+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:54:23.149+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:54:23.149+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:54:23.739+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:54:23.739+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:54:23.739+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:54:23.739+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:54:23.844+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:54:23.845+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:54:23.845+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:54:23.849+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:54:23.849+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:54:23.849+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:54:23.855+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:54:23.855+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:54:23.855+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:54:23.855+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:54:23.867+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:54:23.867+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:54:23.873+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:54:23.873+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:54:23.874+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:54:23.874+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:54:23.889+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:54:23.888+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:54:23.896+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.761 seconds
[2024-11-25T18:54:54.252+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:54:54.251+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2281)
[2024-11-25T18:54:54.253+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:54:54.253+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2281
[2024-11-25T18:54:54.317+0000] {processor.py:153} INFO - Started process (PID=2281) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:54:54.319+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:54:54.325+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:54:54.325+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:54:54.334+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:54:54.334+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:54:55.061+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:54:55.061+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:54:55.062+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:54:55.062+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:54:55.196+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:54:55.198+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:54:55.198+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:54:55.203+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:54:55.203+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:54:55.203+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:54:55.209+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:54:55.209+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:54:55.209+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:54:55.209+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:54:55.226+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:54:55.226+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:54:55.236+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:54:55.236+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:54:55.237+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:54:55.237+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:54:55.253+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:54:55.253+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:54:55.264+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 1.013 seconds
[2024-11-25T18:55:25.813+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:55:25.813+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2287)
[2024-11-25T18:55:25.814+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:55:25.814+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2287
[2024-11-25T18:55:25.827+0000] {processor.py:153} INFO - Started process (PID=2287) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:55:25.828+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:55:25.830+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:55:25.829+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:55:25.831+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:55:25.831+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:55:26.478+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:55:26.477+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:55:26.478+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:55:26.478+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:55:26.585+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:55:26.587+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:55:26.586+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:55:26.590+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:55:26.590+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:55:26.590+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:55:26.596+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:55:26.596+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:55:26.596+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:55:26.596+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:55:26.604+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:55:26.604+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:55:26.610+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:55:26.610+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:55:26.612+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:55:26.611+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:55:26.625+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:55:26.625+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:55:26.637+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.824 seconds
[2024-11-25T18:55:57.201+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:55:57.201+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2293)
[2024-11-25T18:55:57.202+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:55:57.202+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2293
[2024-11-25T18:55:57.206+0000] {processor.py:153} INFO - Started process (PID=2293) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:55:57.206+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:55:57.207+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:55:57.207+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:55:57.208+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:55:57.208+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:55:57.744+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:55:57.744+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:55:57.745+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:55:57.745+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:55:57.839+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:55:57.841+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:55:57.841+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:55:57.845+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:55:57.845+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:55:57.845+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:55:57.851+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:55:57.851+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:55:57.852+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:55:57.852+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:55:57.860+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:55:57.860+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:55:57.871+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:55:57.871+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:55:57.872+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:55:57.872+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:55:57.887+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:55:57.887+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:55:57.894+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.693 seconds
[2024-11-25T18:56:28.343+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:56:28.343+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2299)
[2024-11-25T18:56:28.344+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:56:28.344+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2299
[2024-11-25T18:56:28.350+0000] {processor.py:153} INFO - Started process (PID=2299) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:56:28.351+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:56:28.352+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:56:28.352+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:56:28.354+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:56:28.353+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:56:28.841+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:56:28.840+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:56:28.841+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:56:28.841+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:56:28.938+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:56:28.939+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:56:28.939+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:56:28.943+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:56:28.942+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:56:28.943+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:56:28.949+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:56:28.949+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:56:28.950+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:56:28.949+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:56:28.958+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:56:28.958+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:56:28.964+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:56:28.964+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:56:28.964+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:56:28.964+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:56:28.973+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:56:28.973+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:56:28.980+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.638 seconds
[2024-11-25T18:56:59.434+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:56:59.433+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2305)
[2024-11-25T18:56:59.436+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:56:59.436+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2305
[2024-11-25T18:56:59.469+0000] {processor.py:153} INFO - Started process (PID=2305) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:56:59.472+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:56:59.474+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:56:59.474+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:56:59.483+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:56:59.483+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:57:00.997+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:57:00.997+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:57:00.998+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:57:00.998+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:57:01.157+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:57:01.159+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:57:01.159+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:57:01.166+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:57:01.166+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:57:01.167+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:57:01.179+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:57:01.179+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:57:01.179+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:57:01.179+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:57:01.207+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:57:01.206+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:57:01.218+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:57:01.218+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:57:01.218+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:57:01.218+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:57:01.233+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:57:01.233+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:57:01.242+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 1.808 seconds
[2024-11-25T18:57:31.815+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:57:31.815+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2311)
[2024-11-25T18:57:31.816+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:57:31.816+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2311
[2024-11-25T18:57:31.822+0000] {processor.py:153} INFO - Started process (PID=2311) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:57:31.823+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:57:31.824+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:57:31.824+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:57:31.826+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:57:31.826+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:57:32.529+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:57:32.529+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:57:32.530+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:57:32.530+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:57:32.665+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:57:32.667+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:57:32.667+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:57:32.671+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:57:32.671+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:57:32.672+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:57:32.685+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:57:32.685+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:57:32.686+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:57:32.685+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:57:32.706+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:57:32.706+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:57:32.714+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:57:32.714+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:57:32.714+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:57:32.714+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:57:32.728+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:57:32.728+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:57:32.735+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.921 seconds
[2024-11-25T18:58:03.635+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:58:03.634+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2317)
[2024-11-25T18:58:03.639+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:58:03.639+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2317
[2024-11-25T18:58:03.674+0000] {processor.py:153} INFO - Started process (PID=2317) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:58:03.677+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:58:03.681+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:58:03.681+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:58:03.684+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:58:03.684+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:58:04.361+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:58:04.361+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:58:04.362+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:58:04.362+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:58:04.502+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:58:04.504+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:58:04.504+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:58:04.508+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:58:04.508+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:58:04.508+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:58:04.518+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:58:04.518+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:58:04.518+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:58:04.518+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:58:04.537+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:58:04.537+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:58:04.547+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:58:04.547+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:58:04.550+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:58:04.548+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:58:04.578+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:58:04.578+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:58:04.586+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.952 seconds
[2024-11-25T18:58:35.210+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:58:35.210+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2323)
[2024-11-25T18:58:35.211+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:58:35.211+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2323
[2024-11-25T18:58:35.215+0000] {processor.py:153} INFO - Started process (PID=2323) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:58:35.215+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:58:35.216+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:58:35.216+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:58:35.218+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:58:35.218+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:58:35.718+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:58:35.718+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:58:35.719+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:58:35.718+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:58:35.814+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:58:35.815+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:58:35.815+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:58:35.819+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:58:35.818+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:58:35.819+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:58:35.827+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:58:35.827+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:58:35.827+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:58:35.827+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:58:35.844+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:58:35.844+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:58:35.860+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:58:35.860+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:58:35.861+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:58:35.860+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:58:35.876+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:58:35.876+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:58:35.887+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.678 seconds
[2024-11-25T18:59:06.423+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:59:06.423+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2329)
[2024-11-25T18:59:06.425+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:59:06.425+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2329
[2024-11-25T18:59:06.430+0000] {processor.py:153} INFO - Started process (PID=2329) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:59:06.431+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:59:06.431+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:59:06.431+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:59:06.433+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:59:06.433+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:59:07.112+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:59:07.112+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:59:07.115+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:59:07.115+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:59:07.350+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:59:07.355+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:59:07.355+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:59:07.366+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:59:07.365+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:59:07.367+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:59:07.377+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:59:07.377+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:59:07.378+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:59:07.378+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:59:07.473+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:59:07.473+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:59:07.485+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:59:07.485+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:59:07.486+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:59:07.485+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:59:07.508+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:59:07.508+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:59:07.564+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 1.142 seconds
[2024-11-25T18:59:38.494+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:59:38.491+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2335)
[2024-11-25T18:59:38.501+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:59:38.501+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2335
[2024-11-25T18:59:38.582+0000] {processor.py:153} INFO - Started process (PID=2335) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:59:38.588+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T18:59:38.592+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:59:38.592+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:59:38.605+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:59:38.605+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:59:41.559+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:59:41.559+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T18:59:41.560+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:59:41.559+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T18:59:41.720+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T18:59:41.722+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:59:41.722+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T18:59:41.728+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:59:41.728+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T18:59:41.729+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T18:59:41.747+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:59:41.747+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T18:59:41.747+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:59:41.747+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T18:59:41.781+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:59:41.781+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T18:59:41.795+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:59:41.795+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T18:59:41.796+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:59:41.795+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T18:59:41.820+0000] {logging_mixin.py:137} INFO - [2024-11-25T18:59:41.820+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T18:59:41.835+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 3.350 seconds
[2024-11-25T19:00:12.205+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:00:12.204+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2341)
[2024-11-25T19:00:12.205+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:00:12.205+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2341
[2024-11-25T19:00:12.209+0000] {processor.py:153} INFO - Started process (PID=2341) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:00:12.209+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T19:00:12.210+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:00:12.210+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:00:12.211+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:00:12.211+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:00:12.764+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:00:12.763+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T19:00:12.764+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:00:12.764+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T19:00:12.857+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T19:00:12.858+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:00:12.858+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T19:00:12.862+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:00:12.862+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T19:00:12.862+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:00:12.867+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:00:12.867+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T19:00:12.867+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:00:12.867+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T19:00:12.875+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:00:12.875+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T19:00:12.894+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:00:12.894+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T19:00:12.897+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:00:12.894+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T19:00:12.917+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:00:12.917+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T19:00:12.928+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.724 seconds
[2024-11-25T19:00:43.364+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:00:43.364+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2347)
[2024-11-25T19:00:43.365+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:00:43.365+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2347
[2024-11-25T19:00:43.370+0000] {processor.py:153} INFO - Started process (PID=2347) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:00:43.371+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T19:00:43.372+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:00:43.372+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:00:43.376+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:00:43.375+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:00:43.912+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:00:43.912+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T19:00:43.913+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:00:43.912+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T19:00:44.005+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T19:00:44.007+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:00:44.007+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T19:00:44.010+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:00:44.010+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T19:00:44.010+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:00:44.015+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:00:44.015+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T19:00:44.015+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:00:44.015+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T19:00:44.026+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:00:44.025+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T19:00:44.031+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:00:44.031+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T19:00:44.032+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:00:44.031+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T19:00:44.040+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:00:44.040+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T19:00:44.047+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.683 seconds
[2024-11-25T19:01:15.013+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:01:15.012+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2353)
[2024-11-25T19:01:15.013+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:01:15.013+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2353
[2024-11-25T19:01:15.021+0000] {processor.py:153} INFO - Started process (PID=2353) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:01:15.021+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T19:01:15.022+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:01:15.022+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:01:15.023+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:01:15.023+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:01:15.832+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:01:15.832+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T19:01:15.833+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:01:15.832+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T19:01:15.954+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T19:01:15.955+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:01:15.955+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T19:01:15.959+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:01:15.959+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T19:01:15.959+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:01:15.964+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:01:15.964+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T19:01:15.965+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:01:15.964+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T19:01:15.979+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:01:15.979+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T19:01:15.985+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:01:15.985+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T19:01:15.985+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:01:15.985+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T19:01:15.995+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:01:15.995+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T19:01:16.001+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.989 seconds
[2024-11-25T19:01:46.590+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:01:46.589+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2359)
[2024-11-25T19:01:46.591+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:01:46.591+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2359
[2024-11-25T19:01:46.596+0000] {processor.py:153} INFO - Started process (PID=2359) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:01:46.597+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T19:01:46.598+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:01:46.598+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:01:46.600+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:01:46.600+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:01:47.093+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:01:47.093+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T19:01:47.094+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:01:47.093+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T19:01:47.203+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T19:01:47.205+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:01:47.205+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T19:01:47.210+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:01:47.210+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T19:01:47.210+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:01:47.218+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:01:47.218+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T19:01:47.218+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:01:47.218+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T19:01:47.230+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:01:47.230+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T19:01:47.238+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:01:47.238+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T19:01:47.239+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:01:47.238+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T19:01:47.259+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:01:47.259+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T19:01:47.271+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.682 seconds
[2024-11-25T19:02:17.844+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:02:17.844+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2365)
[2024-11-25T19:02:17.845+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:02:17.845+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2365
[2024-11-25T19:02:17.853+0000] {processor.py:153} INFO - Started process (PID=2365) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:02:17.853+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T19:02:17.854+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:02:17.854+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:02:17.855+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:02:17.855+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:02:18.438+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:02:18.438+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T19:02:18.439+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:02:18.439+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T19:02:18.566+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T19:02:18.568+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:02:18.567+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T19:02:18.573+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:02:18.572+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T19:02:18.573+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:02:18.581+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:02:18.581+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T19:02:18.581+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:02:18.581+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T19:02:18.593+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:02:18.593+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T19:02:18.601+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:02:18.601+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T19:02:18.602+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:02:18.601+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T19:02:18.619+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:02:18.619+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T19:02:18.626+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.783 seconds
[2024-11-25T19:02:49.118+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:02:49.117+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2371)
[2024-11-25T19:02:49.120+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:02:49.120+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2371
[2024-11-25T19:02:49.131+0000] {processor.py:153} INFO - Started process (PID=2371) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:02:49.132+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T19:02:49.133+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:02:49.133+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:02:49.135+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:02:49.134+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:02:49.823+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:02:49.823+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T19:02:49.824+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:02:49.823+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T19:02:49.937+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T19:02:49.939+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:02:49.939+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T19:02:49.943+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:02:49.943+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T19:02:49.943+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:02:49.953+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:02:49.952+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T19:02:49.953+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:02:49.953+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T19:02:49.973+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:02:49.973+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T19:02:49.980+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:02:49.980+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T19:02:49.981+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:02:49.981+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T19:02:49.992+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:02:49.991+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T19:02:49.998+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.881 seconds
[2024-11-25T19:03:20.651+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:03:20.650+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2377)
[2024-11-25T19:03:20.652+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:03:20.652+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2377
[2024-11-25T19:03:20.658+0000] {processor.py:153} INFO - Started process (PID=2377) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:03:20.659+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T19:03:20.660+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:03:20.660+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:03:20.661+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:03:20.661+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:03:21.200+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:03:21.199+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T19:03:21.200+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:03:21.200+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T19:03:21.302+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T19:03:21.303+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:03:21.303+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T19:03:21.307+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:03:21.306+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T19:03:21.307+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:03:21.313+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:03:21.313+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T19:03:21.313+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:03:21.313+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T19:03:21.342+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:03:21.341+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T19:03:21.355+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:03:21.355+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T19:03:21.355+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:03:21.355+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T19:03:21.368+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:03:21.368+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T19:03:21.378+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.728 seconds
[2024-11-25T19:03:51.874+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:03:51.873+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2383)
[2024-11-25T19:03:51.875+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:03:51.875+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2383
[2024-11-25T19:03:51.884+0000] {processor.py:153} INFO - Started process (PID=2383) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:03:51.885+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T19:03:51.887+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:03:51.886+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:03:51.888+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:03:51.888+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:03:52.735+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:03:52.734+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T19:03:52.735+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:03:52.735+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T19:03:52.846+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T19:03:52.847+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:03:52.847+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T19:03:52.853+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:03:52.853+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T19:03:52.853+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:03:52.859+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:03:52.859+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T19:03:52.859+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:03:52.859+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T19:03:52.874+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:03:52.874+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T19:03:52.888+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:03:52.888+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T19:03:52.889+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:03:52.889+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T19:03:52.905+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:03:52.905+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T19:03:52.913+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 1.040 seconds
[2024-11-25T19:04:23.900+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:04:23.899+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2389)
[2024-11-25T19:04:23.901+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:04:23.901+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2389
[2024-11-25T19:04:23.908+0000] {processor.py:153} INFO - Started process (PID=2389) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:04:23.909+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T19:04:23.910+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:04:23.910+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:04:23.914+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:04:23.914+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:04:24.916+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:04:24.916+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T19:04:24.917+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:04:24.916+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T19:04:25.020+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T19:04:25.024+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:04:25.024+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T19:04:25.029+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:04:25.029+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T19:04:25.029+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:04:25.036+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:04:25.036+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T19:04:25.036+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:04:25.036+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T19:04:25.052+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:04:25.052+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T19:04:25.058+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:04:25.058+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T19:04:25.059+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:04:25.059+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T19:04:25.070+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:04:25.070+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T19:04:25.077+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 1.179 seconds
[2024-11-25T19:04:55.719+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:04:55.717+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2395)
[2024-11-25T19:04:55.723+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:04:55.723+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2395
[2024-11-25T19:04:55.745+0000] {processor.py:153} INFO - Started process (PID=2395) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:04:55.747+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T19:04:55.748+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:04:55.748+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:04:55.752+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:04:55.752+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:04:56.903+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:04:56.903+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T19:04:56.904+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:04:56.904+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T19:04:57.141+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T19:04:57.144+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:04:57.144+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T19:04:57.149+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:04:57.149+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T19:04:57.149+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:04:57.157+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:04:57.157+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T19:04:57.157+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:04:57.157+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T19:04:57.175+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:04:57.175+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T19:04:57.197+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:04:57.197+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T19:04:57.199+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:04:57.198+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T19:04:57.220+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:04:57.220+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T19:04:57.231+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 1.517 seconds
[2024-11-25T19:05:27.765+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:05:27.764+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2401)
[2024-11-25T19:05:27.767+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:05:27.767+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2401
[2024-11-25T19:05:27.848+0000] {processor.py:153} INFO - Started process (PID=2401) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:05:27.854+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T19:05:27.859+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:05:27.858+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:05:27.866+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:05:27.865+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:05:28.704+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:05:28.704+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T19:05:28.705+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:05:28.705+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T19:05:29.018+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T19:05:29.024+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:05:29.024+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T19:05:29.046+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:05:29.045+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T19:05:29.047+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:05:29.065+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:05:29.065+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T19:05:29.065+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:05:29.065+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T19:05:29.210+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:05:29.209+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T19:05:29.235+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:05:29.234+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T19:05:29.238+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:05:29.236+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T19:05:29.285+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:05:29.285+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T19:05:29.327+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 1.563 seconds
[2024-11-25T19:06:00.827+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:06:00.820+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2407)
[2024-11-25T19:06:00.838+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:06:00.838+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2407
[2024-11-25T19:06:01.005+0000] {processor.py:153} INFO - Started process (PID=2407) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:06:01.014+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T19:06:01.028+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:06:01.028+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:06:01.048+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:06:01.048+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:06:08.244+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:06:08.244+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T19:06:08.257+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:06:08.254+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T19:06:09.003+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T19:06:09.015+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:06:09.015+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T19:06:09.041+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:06:09.040+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T19:06:09.041+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:06:09.093+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:06:09.093+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T19:06:09.093+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:06:09.093+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T19:06:09.227+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:06:09.227+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T19:06:09.254+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:06:09.254+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T19:06:09.255+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:06:09.254+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T19:06:09.298+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:06:09.298+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T19:06:09.342+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 8.520 seconds
[2024-11-25T19:06:40.671+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:06:40.664+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2413)
[2024-11-25T19:06:40.682+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:06:40.682+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2413
[2024-11-25T19:06:40.790+0000] {processor.py:153} INFO - Started process (PID=2413) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:06:40.798+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T19:06:40.810+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:06:40.810+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:06:40.824+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:06:40.824+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:07:00.815+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:07:00.724+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T19:07:01.352+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:07:00.841+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T19:07:09.363+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T19:07:13.707+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:07:12.293+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/opt/airflow/logs/scheduler/2024-11-25/kafka_to_snowflake_etl_dag.py.log'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/kafka_to_snowflake_etl_dag.py", line 386, in <module>
    kafka_to_snowflake_pipeline_dag = kafka_to_snowflake_pipeline()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 3492, in factory
    with DAG(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 499, in __init__
    warnings.warn(
  File "/usr/local/lib/python3.8/warnings.py", line 109, in _showwarnmsg
    sw(msg.message, msg.category, msg.filename, msg.lineno,
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/settings.py", line 126, in custom_show_warning
    write_console.print(msg, soft_wrap=True)
  File "/home/airflow/.local/lib/python3.8/site-packages/rich/console.py", line 1715, in print
    self._buffer.extend(new_segments)
  File "/home/airflow/.local/lib/python3.8/site-packages/rich/console.py", line 869, in __exit__
    self._exit_buffer()
  File "/home/airflow/.local/lib/python3.8/site-packages/rich/console.py", line 827, in _exit_buffer
    self._check_buffer()
  File "/home/airflow/.local/lib/python3.8/site-packages/rich/console.py", line 2039, in _check_buffer
    self.file.write(text)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1089, in emit
    self.flush()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1069, in flush
    self.stream.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/timeout.py", line 68, in handle_timeout
    self.log.error("Process timed out, PID: %s", str(os.getpid()))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1475, in error
    self._log(ERROR, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1088, in emit
    stream.write(msg + self.terminator)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1093, in emit
    self.handleError(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1006, in handleError
    sys.stderr.write('--- Logging error ---\n')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 149, in write
    self.flush()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 156, in flush
    self._propagate_log(buf)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 137, in _propagate_log
    self.logger.log(self.level, remove_escape_codes(message))
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1512, in log
    self._log(level, msg, args, **kwargs)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 954, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py", line 72, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1187, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1085, in emit
    msg = self.format(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 929, in format
    return fmt.format(record)
  File "/usr/local/lib/python3.8/logging/__init__.py", line 669, in format
    if self.usesTime():
  File "/usr/local/lib/python3.8/logging/__init__.py", line 637, in usesTime
    return self._style.usesTime()
  File "/usr/local/lib/python3.8/logging/__init__.py", line 428, in usesTime
    return self._fmt.find(self.asctime_search) >= 0
RecursionError: maximum recursion depth exceeded while calling a Python object
[2024-11-25T19:07:13.951+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:07:16.257+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 35.593 seconds
[2024-11-25T19:07:47.499+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:07:47.498+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2419)
[2024-11-25T19:07:47.500+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:07:47.500+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2419
[2024-11-25T19:07:47.513+0000] {processor.py:153} INFO - Started process (PID=2419) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:07:47.515+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T19:07:47.518+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:07:47.517+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:07:47.520+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:07:47.520+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:07:51.369+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:07:51.369+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T19:07:51.634+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:07:51.630+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T19:07:51.974+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T19:07:51.990+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:07:51.990+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T19:07:52.003+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:07:52.003+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T19:07:52.003+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:07:52.016+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:07:52.016+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T19:07:52.017+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:07:52.017+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T19:07:52.037+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:07:52.036+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T19:07:52.047+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:07:52.047+0000] {serialized_dag.py:170} DEBUG - Writing Serialized DAG: kafka_to_snowflake_pipeline to the DB
[2024-11-25T19:07:52.050+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:07:52.050+0000] {serialized_dag.py:172} DEBUG - DAG: kafka_to_snowflake_pipeline written to the DB
[2024-11-25T19:07:52.083+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:07:52.083+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T19:07:52.106+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:07:52.106+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T19:07:52.119+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 4.621 seconds
[2024-11-25T19:08:22.562+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:08:22.561+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2425)
[2024-11-25T19:08:22.562+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:08:22.562+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2425
[2024-11-25T19:08:22.575+0000] {processor.py:153} INFO - Started process (PID=2425) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:08:22.576+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T19:08:22.576+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:08:22.576+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:08:22.578+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:08:22.578+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:08:23.171+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:08:23.171+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T19:08:23.171+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:08:23.171+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T19:08:23.274+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T19:08:23.275+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:08:23.275+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T19:08:23.279+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:08:23.279+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T19:08:23.280+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:08:23.286+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:08:23.286+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T19:08:23.286+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:08:23.286+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T19:08:23.297+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:08:23.296+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T19:08:23.303+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:08:23.303+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T19:08:23.304+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:08:23.303+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T19:08:23.313+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:08:23.313+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T19:08:23.319+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.758 seconds
[2024-11-25T19:08:53.683+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:08:53.683+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2431)
[2024-11-25T19:08:53.685+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:08:53.685+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2431
[2024-11-25T19:08:53.691+0000] {processor.py:153} INFO - Started process (PID=2431) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:08:53.692+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T19:08:53.693+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:08:53.693+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:08:53.696+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:08:53.696+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:08:54.623+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:08:54.623+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T19:08:54.798+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:08:54.796+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T19:08:55.006+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T19:08:55.010+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:08:55.009+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T19:08:55.015+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:08:55.014+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T19:08:55.016+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:08:55.026+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:08:55.026+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T19:08:55.026+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:08:55.026+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T19:08:55.051+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:08:55.050+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T19:08:55.062+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:08:55.062+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T19:08:55.063+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:08:55.062+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T19:08:55.079+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:08:55.079+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T19:08:55.096+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 1.413 seconds
[2024-11-25T19:09:25.435+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:09:25.435+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2437)
[2024-11-25T19:09:25.437+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:09:25.437+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2437
[2024-11-25T19:09:25.443+0000] {processor.py:153} INFO - Started process (PID=2437) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:09:25.444+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T19:09:25.445+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:09:25.445+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:09:25.447+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:09:25.447+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:09:26.035+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:09:26.034+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T19:09:26.035+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:09:26.035+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T19:09:26.140+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T19:09:26.142+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:09:26.142+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T19:09:26.147+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:09:26.147+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T19:09:26.148+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:09:26.155+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:09:26.155+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T19:09:26.155+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:09:26.155+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T19:09:26.166+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:09:26.166+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T19:09:26.173+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:09:26.173+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T19:09:26.173+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:09:26.173+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T19:09:26.186+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:09:26.186+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T19:09:26.197+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.762 seconds
[2024-11-25T19:09:56.278+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:09:56.277+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2443)
[2024-11-25T19:09:56.279+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:09:56.279+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2443
[2024-11-25T19:09:56.286+0000] {processor.py:153} INFO - Started process (PID=2443) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:09:56.287+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T19:09:56.297+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:09:56.297+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:09:56.303+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:09:56.303+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:09:56.808+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:09:56.808+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T19:09:56.809+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:09:56.808+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T19:09:56.909+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T19:09:56.910+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:09:56.910+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T19:09:56.914+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:09:56.914+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T19:09:56.914+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:09:56.920+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:09:56.920+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T19:09:56.920+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:09:56.920+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T19:09:56.930+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:09:56.929+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T19:09:56.936+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:09:56.936+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T19:09:56.936+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:09:56.936+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T19:09:56.945+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:09:56.945+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T19:09:56.953+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.676 seconds
[2024-11-25T19:10:27.483+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:10:27.482+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2449)
[2024-11-25T19:10:27.483+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:10:27.483+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2449
[2024-11-25T19:10:27.489+0000] {processor.py:153} INFO - Started process (PID=2449) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:10:27.491+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T19:10:27.494+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:10:27.494+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:10:27.497+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:10:27.496+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:10:27.996+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:10:27.996+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T19:10:27.997+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:10:27.996+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T19:10:28.105+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T19:10:28.106+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:10:28.106+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T19:10:28.110+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:10:28.110+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T19:10:28.110+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:10:28.116+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:10:28.116+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T19:10:28.116+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:10:28.116+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T19:10:28.129+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:10:28.129+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T19:10:28.134+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:10:28.134+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T19:10:28.135+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:10:28.135+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T19:10:28.144+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:10:28.144+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T19:10:28.151+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.669 seconds
[2024-11-25T19:10:58.956+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:10:58.956+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2455)
[2024-11-25T19:10:58.957+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:10:58.957+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2455
[2024-11-25T19:10:58.962+0000] {processor.py:153} INFO - Started process (PID=2455) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:10:58.962+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T19:10:58.964+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:10:58.964+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:10:58.966+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:10:58.966+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:10:59.522+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:10:59.522+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T19:10:59.523+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:10:59.522+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T19:10:59.630+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T19:10:59.632+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:10:59.632+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T19:10:59.640+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:10:59.639+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T19:10:59.640+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:10:59.646+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:10:59.646+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T19:10:59.647+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:10:59.647+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T19:10:59.662+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:10:59.662+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T19:10:59.672+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:10:59.672+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T19:10:59.673+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:10:59.672+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T19:10:59.687+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:10:59.687+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T19:10:59.697+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.741 seconds
[2024-11-25T19:11:30.115+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:11:30.114+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2461)
[2024-11-25T19:11:30.116+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:11:30.116+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2461
[2024-11-25T19:11:30.122+0000] {processor.py:153} INFO - Started process (PID=2461) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:11:30.123+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T19:11:30.123+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:11:30.123+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:11:30.125+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:11:30.125+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:11:30.647+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:11:30.647+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T19:11:30.648+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:11:30.647+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T19:11:30.741+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T19:11:30.742+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:11:30.742+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T19:11:30.746+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:11:30.746+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T19:11:30.746+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:11:30.751+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:11:30.751+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T19:11:30.751+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:11:30.751+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T19:11:30.763+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:11:30.763+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T19:11:30.769+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:11:30.769+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T19:11:30.769+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:11:30.769+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T19:11:30.778+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:11:30.778+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T19:11:30.788+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.674 seconds
[2024-11-25T19:12:01.255+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:12:01.254+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2467)
[2024-11-25T19:12:01.256+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:12:01.256+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2467
[2024-11-25T19:12:01.264+0000] {processor.py:153} INFO - Started process (PID=2467) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:12:01.265+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T19:12:01.266+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:12:01.266+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:12:01.267+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:12:01.267+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:12:01.849+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:12:01.849+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T19:12:01.850+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:12:01.850+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T19:12:01.954+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T19:12:01.956+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:12:01.956+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T19:12:01.959+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:12:01.959+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T19:12:01.960+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:12:01.966+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:12:01.965+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T19:12:01.966+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:12:01.966+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T19:12:01.976+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:12:01.976+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T19:12:01.982+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:12:01.982+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T19:12:01.983+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:12:01.983+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T19:12:01.994+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:12:01.994+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T19:12:02.001+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.747 seconds
[2024-11-25T19:12:32.700+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:12:32.699+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2473)
[2024-11-25T19:12:32.701+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:12:32.701+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2473
[2024-11-25T19:12:32.707+0000] {processor.py:153} INFO - Started process (PID=2473) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:12:32.708+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T19:12:32.709+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:12:32.709+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:12:32.710+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:12:32.710+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:12:33.217+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:12:33.217+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T19:12:33.218+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:12:33.218+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T19:12:33.318+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T19:12:33.320+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:12:33.320+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T19:12:33.323+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:12:33.323+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T19:12:33.324+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:12:33.330+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:12:33.330+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T19:12:33.330+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:12:33.330+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T19:12:33.344+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:12:33.344+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T19:12:33.349+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:12:33.349+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T19:12:33.349+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:12:33.349+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T19:12:33.359+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:12:33.359+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T19:12:33.365+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.666 seconds
[2024-11-25T19:13:04.126+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:13:04.126+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2479)
[2024-11-25T19:13:04.127+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:13:04.127+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2479
[2024-11-25T19:13:04.132+0000] {processor.py:153} INFO - Started process (PID=2479) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:13:04.134+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T19:13:04.137+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:13:04.137+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:13:04.140+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:13:04.139+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:13:04.752+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:13:04.752+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T19:13:04.752+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:13:04.752+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T19:13:04.879+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T19:13:04.881+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:13:04.881+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T19:13:04.885+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:13:04.885+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T19:13:04.885+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:13:04.892+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:13:04.892+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T19:13:04.892+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:13:04.892+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T19:13:04.904+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:13:04.904+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T19:13:04.911+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:13:04.911+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T19:13:04.911+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:13:04.911+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T19:13:04.922+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:13:04.921+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T19:13:04.932+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.807 seconds
[2024-11-25T19:13:35.410+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:13:35.410+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2485)
[2024-11-25T19:13:35.411+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:13:35.411+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2485
[2024-11-25T19:13:35.414+0000] {processor.py:153} INFO - Started process (PID=2485) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:13:35.415+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T19:13:35.416+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:13:35.416+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:13:35.417+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:13:35.417+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:13:36.266+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:13:36.266+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T19:13:36.267+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:13:36.267+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T19:13:36.370+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T19:13:36.372+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:13:36.372+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T19:13:36.378+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:13:36.377+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T19:13:36.379+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:13:36.397+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:13:36.396+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T19:13:36.397+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:13:36.397+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T19:13:36.431+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:13:36.431+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T19:13:36.441+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:13:36.441+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T19:13:36.441+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:13:36.441+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T19:13:36.453+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:13:36.453+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T19:13:36.461+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 1.051 seconds
[2024-11-25T19:14:07.008+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:14:07.008+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2491)
[2024-11-25T19:14:07.009+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:14:07.009+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2491
[2024-11-25T19:14:07.015+0000] {processor.py:153} INFO - Started process (PID=2491) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:14:07.016+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T19:14:07.017+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:14:07.017+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:14:07.019+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:14:07.019+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:14:07.493+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:14:07.493+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T19:14:07.494+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:14:07.493+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T19:14:07.592+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T19:14:07.593+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:14:07.593+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T19:14:07.598+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:14:07.598+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T19:14:07.598+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:14:07.604+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:14:07.604+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T19:14:07.604+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:14:07.604+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T19:14:07.613+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:14:07.613+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T19:14:07.620+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:14:07.620+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T19:14:07.620+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:14:07.620+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T19:14:07.630+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:14:07.630+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T19:14:07.637+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.630 seconds
[2024-11-25T19:14:38.204+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:14:38.203+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2497)
[2024-11-25T19:14:38.206+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:14:38.205+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2497
[2024-11-25T19:14:38.211+0000] {processor.py:153} INFO - Started process (PID=2497) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:14:38.211+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T19:14:38.212+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:14:38.212+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:14:38.214+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:14:38.213+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:14:38.719+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:14:38.719+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T19:14:38.719+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:14:38.719+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T19:14:38.823+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T19:14:38.824+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:14:38.824+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T19:14:38.828+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:14:38.828+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T19:14:38.828+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:14:38.840+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:14:38.840+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T19:14:38.840+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:14:38.840+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T19:14:38.854+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:14:38.854+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T19:14:38.860+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:14:38.860+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T19:14:38.861+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:14:38.861+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T19:14:38.872+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:14:38.872+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T19:14:38.879+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.676 seconds
[2024-11-25T19:15:09.259+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:15:09.258+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2503)
[2024-11-25T19:15:09.260+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:15:09.260+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2503
[2024-11-25T19:15:09.265+0000] {processor.py:153} INFO - Started process (PID=2503) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:15:09.265+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T19:15:09.266+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:15:09.266+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:15:09.267+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:15:09.267+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:15:09.795+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:15:09.795+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T19:15:09.796+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:15:09.795+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T19:15:09.904+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T19:15:09.906+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:15:09.906+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T19:15:09.910+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:15:09.910+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T19:15:09.910+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:15:09.916+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:15:09.916+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T19:15:09.916+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:15:09.916+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T19:15:09.928+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:15:09.928+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T19:15:09.936+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:15:09.936+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T19:15:09.937+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:15:09.936+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T19:15:09.960+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:15:09.959+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T19:15:09.970+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.712 seconds
[2024-11-25T19:15:40.647+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:15:40.646+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2509)
[2024-11-25T19:15:40.648+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:15:40.648+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2509
[2024-11-25T19:15:40.653+0000] {processor.py:153} INFO - Started process (PID=2509) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:15:40.653+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T19:15:40.654+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:15:40.654+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:15:40.655+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:15:40.655+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:15:41.169+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:15:41.169+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T19:15:41.170+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:15:41.170+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T19:15:41.266+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T19:15:41.268+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:15:41.268+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T19:15:41.271+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:15:41.271+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T19:15:41.271+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:15:41.277+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:15:41.277+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T19:15:41.277+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:15:41.277+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T19:15:41.289+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:15:41.289+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T19:15:41.301+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:15:41.301+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T19:15:41.302+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:15:41.301+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T19:15:41.316+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:15:41.316+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T19:15:41.324+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.677 seconds
[2024-11-25T19:16:11.815+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:16:11.815+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2515)
[2024-11-25T19:16:11.816+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:16:11.816+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2515
[2024-11-25T19:16:11.821+0000] {processor.py:153} INFO - Started process (PID=2515) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:16:11.821+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T19:16:11.822+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:16:11.822+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:16:11.824+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:16:11.824+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:16:12.468+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:16:12.467+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T19:16:12.469+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:16:12.468+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T19:16:12.743+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T19:16:12.746+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:16:12.746+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T19:16:12.751+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:16:12.751+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T19:16:12.752+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:16:12.786+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:16:12.786+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T19:16:12.786+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:16:12.786+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T19:16:12.807+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:16:12.806+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T19:16:12.826+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:16:12.825+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T19:16:12.826+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:16:12.826+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T19:16:12.841+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:16:12.841+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T19:16:12.850+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 1.035 seconds
[2024-11-25T19:16:43.386+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:16:43.384+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2521)
[2024-11-25T19:16:43.388+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:16:43.388+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2521
[2024-11-25T19:16:43.394+0000] {processor.py:153} INFO - Started process (PID=2521) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:16:43.395+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T19:16:43.396+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:16:43.396+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:16:43.398+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:16:43.398+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:16:43.938+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:16:43.938+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T19:16:43.939+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:16:43.939+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T19:16:44.034+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T19:16:44.038+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:16:44.038+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T19:16:44.043+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:16:44.043+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T19:16:44.043+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:16:44.049+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:16:44.049+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T19:16:44.049+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:16:44.049+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T19:16:44.061+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:16:44.061+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T19:16:44.068+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:16:44.068+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T19:16:44.069+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:16:44.069+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T19:16:44.079+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:16:44.079+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T19:16:44.087+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.705 seconds
[2024-11-25T19:17:14.601+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:17:14.600+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2527)
[2024-11-25T19:17:14.602+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:17:14.602+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2527
[2024-11-25T19:17:14.609+0000] {processor.py:153} INFO - Started process (PID=2527) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:17:14.610+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T19:17:14.611+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:17:14.611+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:17:14.613+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:17:14.613+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:17:15.165+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:17:15.165+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T19:17:15.166+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:17:15.165+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T19:17:15.268+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T19:17:15.270+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:17:15.270+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T19:17:15.275+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:17:15.274+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T19:17:15.275+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:17:15.281+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:17:15.281+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T19:17:15.281+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:17:15.281+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T19:17:15.291+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:17:15.291+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T19:17:15.297+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:17:15.297+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T19:17:15.298+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:17:15.297+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T19:17:15.307+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:17:15.307+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T19:17:15.314+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.714 seconds
[2024-11-25T19:17:45.866+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:17:45.865+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2533)
[2024-11-25T19:17:45.867+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:17:45.867+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2533
[2024-11-25T19:17:45.873+0000] {processor.py:153} INFO - Started process (PID=2533) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:17:45.874+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T19:17:45.875+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:17:45.875+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:17:45.877+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:17:45.877+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:17:46.399+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:17:46.399+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T19:17:46.400+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:17:46.400+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T19:17:46.506+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T19:17:46.507+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:17:46.507+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T19:17:46.511+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:17:46.511+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T19:17:46.511+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:17:46.518+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:17:46.517+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T19:17:46.518+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:17:46.518+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T19:17:46.527+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:17:46.527+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T19:17:46.533+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:17:46.533+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T19:17:46.533+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:17:46.533+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T19:17:46.543+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:17:46.543+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T19:17:46.553+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.688 seconds
[2024-11-25T19:18:16.950+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:18:16.949+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2539)
[2024-11-25T19:18:16.951+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:18:16.951+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2539
[2024-11-25T19:18:16.960+0000] {processor.py:153} INFO - Started process (PID=2539) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:18:16.960+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T19:18:16.961+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:18:16.961+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:18:16.963+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:18:16.963+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:18:17.468+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:18:17.468+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T19:18:17.469+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:18:17.469+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T19:18:17.650+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T19:18:17.652+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:18:17.652+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T19:18:17.656+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:18:17.655+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T19:18:17.656+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:18:17.662+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:18:17.662+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T19:18:17.663+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:18:17.663+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T19:18:17.678+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:18:17.678+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T19:18:17.685+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:18:17.685+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T19:18:17.685+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:18:17.685+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T19:18:17.697+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:18:17.697+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T19:18:17.705+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.756 seconds
[2024-11-25T19:18:48.111+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:18:48.110+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2545)
[2024-11-25T19:18:48.112+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:18:48.112+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2545
[2024-11-25T19:18:48.120+0000] {processor.py:153} INFO - Started process (PID=2545) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:18:48.120+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T19:18:48.121+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:18:48.121+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:18:48.122+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:18:48.122+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:18:48.654+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:18:48.654+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T19:18:48.654+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:18:48.654+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T19:18:48.815+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T19:18:48.817+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:18:48.817+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T19:18:48.821+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:18:48.821+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T19:18:48.821+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:18:48.827+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:18:48.827+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T19:18:48.827+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:18:48.827+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T19:18:48.835+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:18:48.835+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T19:18:48.842+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:18:48.842+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T19:18:48.842+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:18:48.842+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T19:18:48.851+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:18:48.851+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T19:18:48.858+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.748 seconds
[2024-11-25T19:19:19.370+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:19:19.370+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2551)
[2024-11-25T19:19:19.371+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:19:19.371+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2551
[2024-11-25T19:19:19.377+0000] {processor.py:153} INFO - Started process (PID=2551) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:19:19.378+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T19:19:19.379+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:19:19.379+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:19:19.380+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:19:19.380+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:19:20.203+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:19:20.202+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T19:19:20.206+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:19:20.204+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T19:19:20.442+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T19:19:20.453+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:19:20.453+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T19:19:20.482+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:19:20.481+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T19:19:20.483+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:19:20.496+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:19:20.496+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T19:19:20.496+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:19:20.496+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T19:19:20.604+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:19:20.603+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T19:19:20.704+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:19:20.702+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T19:19:20.726+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:19:20.706+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T19:19:20.777+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:19:20.777+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T19:19:20.789+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 1.419 seconds
[2024-11-25T19:19:50.891+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:19:50.890+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2557)
[2024-11-25T19:19:50.892+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:19:50.892+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2557
[2024-11-25T19:19:50.924+0000] {processor.py:153} INFO - Started process (PID=2557) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:19:50.925+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T19:19:50.926+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:19:50.926+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:19:50.929+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:19:50.929+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:19:51.609+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:19:51.609+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T19:19:51.610+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:19:51.609+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T19:19:51.728+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T19:19:51.730+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:19:51.730+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T19:19:51.734+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:19:51.734+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T19:19:51.735+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:19:51.741+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:19:51.741+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T19:19:51.742+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:19:51.741+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T19:19:51.758+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:19:51.758+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T19:19:51.764+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:19:51.764+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T19:19:51.764+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:19:51.764+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T19:19:51.783+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:19:51.782+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T19:19:51.797+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.907 seconds
[2024-11-25T19:20:21.876+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:20:21.875+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2563)
[2024-11-25T19:20:21.876+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:20:21.876+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2563
[2024-11-25T19:20:21.880+0000] {processor.py:153} INFO - Started process (PID=2563) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:20:21.880+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T19:20:21.881+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:20:21.881+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:20:21.882+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:20:21.882+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:20:22.789+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:20:22.789+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T19:20:22.790+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:20:22.790+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T19:20:22.914+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T19:20:22.916+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:20:22.916+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T19:20:22.921+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:20:22.920+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T19:20:22.921+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:20:22.927+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:20:22.927+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T19:20:22.927+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:20:22.927+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T19:20:22.942+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:20:22.942+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T19:20:22.948+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:20:22.948+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T19:20:22.948+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:20:22.948+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T19:20:22.958+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:20:22.958+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T19:20:22.966+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 1.091 seconds
[2024-11-25T19:20:53.444+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:20:53.444+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2569)
[2024-11-25T19:20:53.446+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:20:53.445+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2569
[2024-11-25T19:20:53.455+0000] {processor.py:153} INFO - Started process (PID=2569) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:20:53.456+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T19:20:53.458+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:20:53.458+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:20:53.463+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:20:53.463+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:20:54.042+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:20:54.042+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T19:20:54.042+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:20:54.042+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T19:20:54.151+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T19:20:54.153+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:20:54.153+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T19:20:54.159+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:20:54.158+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T19:20:54.159+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:20:54.166+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:20:54.166+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T19:20:54.166+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:20:54.166+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T19:20:54.180+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:20:54.180+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T19:20:54.187+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:20:54.187+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T19:20:54.187+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:20:54.187+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T19:20:54.206+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:20:54.205+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T19:20:54.214+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.770 seconds
[2024-11-25T19:21:24.744+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:21:24.744+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2575)
[2024-11-25T19:21:24.745+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:21:24.745+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2575
[2024-11-25T19:21:24.751+0000] {processor.py:153} INFO - Started process (PID=2575) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:21:24.752+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T19:21:24.752+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:21:24.752+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:21:24.754+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:21:24.754+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:21:25.434+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:21:25.434+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T19:21:25.435+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:21:25.434+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T19:21:25.550+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T19:21:25.552+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:21:25.552+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T19:21:25.562+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:21:25.561+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T19:21:25.562+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:21:25.568+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:21:25.568+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T19:21:25.568+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:21:25.568+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T19:21:25.576+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:21:25.576+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T19:21:25.582+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:21:25.582+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T19:21:25.583+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:21:25.582+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T19:21:25.593+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:21:25.593+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T19:21:25.602+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.858 seconds
[2024-11-25T19:21:56.041+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:21:56.040+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2581)
[2024-11-25T19:21:56.045+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:21:56.044+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2581
[2024-11-25T19:21:56.057+0000] {processor.py:153} INFO - Started process (PID=2581) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:21:56.059+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T19:21:56.061+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:21:56.060+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:21:56.063+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:21:56.062+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:21:56.665+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:21:56.665+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T19:21:56.716+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:21:56.715+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T19:21:56.839+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T19:21:56.840+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:21:56.840+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T19:21:56.844+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:21:56.844+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T19:21:56.844+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:21:56.851+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:21:56.851+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T19:21:56.851+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:21:56.851+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T19:21:56.865+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:21:56.865+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T19:21:56.871+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:21:56.871+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T19:21:56.872+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:21:56.871+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T19:21:56.884+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:21:56.884+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T19:21:56.895+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.857 seconds
[2024-11-25T19:22:27.077+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:22:27.077+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2587)
[2024-11-25T19:22:27.079+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:22:27.079+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2587
[2024-11-25T19:22:27.087+0000] {processor.py:153} INFO - Started process (PID=2587) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:22:27.088+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T19:22:27.089+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:22:27.089+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:22:27.095+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:22:27.095+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:22:27.588+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:22:27.587+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T19:22:27.588+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:22:27.588+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T19:22:27.691+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T19:22:27.693+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:22:27.693+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T19:22:27.696+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:22:27.696+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T19:22:27.696+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:22:27.701+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:22:27.701+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T19:22:27.702+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:22:27.702+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T19:22:27.711+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:22:27.711+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T19:22:27.717+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:22:27.717+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T19:22:27.717+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:22:27.717+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T19:22:27.726+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:22:27.726+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T19:22:27.733+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.657 seconds
[2024-11-25T19:22:58.230+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:22:58.229+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2593)
[2024-11-25T19:22:58.231+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:22:58.231+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2593
[2024-11-25T19:22:58.245+0000] {processor.py:153} INFO - Started process (PID=2593) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:22:58.247+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T19:22:58.248+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:22:58.248+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:22:58.250+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:22:58.249+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:22:58.806+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:22:58.806+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T19:22:58.807+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:22:58.807+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T19:22:58.917+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T19:22:58.919+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:22:58.919+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T19:22:58.922+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:22:58.922+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T19:22:58.923+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:22:58.928+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:22:58.928+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T19:22:58.928+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:22:58.928+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T19:22:58.936+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:22:58.936+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T19:22:58.942+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:22:58.942+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T19:22:58.943+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:22:58.943+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T19:22:58.952+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:22:58.952+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T19:22:58.958+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.730 seconds
[2024-11-25T19:23:29.038+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:23:29.038+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2599)
[2024-11-25T19:23:29.039+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:23:29.039+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2599
[2024-11-25T19:23:29.043+0000] {processor.py:153} INFO - Started process (PID=2599) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:23:29.045+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T19:23:29.046+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:23:29.045+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:23:29.048+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:23:29.047+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:23:29.586+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:23:29.586+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T19:23:29.587+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:23:29.587+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T19:23:29.743+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T19:23:29.745+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:23:29.745+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T19:23:29.749+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:23:29.749+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T19:23:29.749+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:23:29.756+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:23:29.756+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T19:23:29.756+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:23:29.756+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T19:23:29.772+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:23:29.772+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T19:23:29.783+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:23:29.783+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T19:23:29.783+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:23:29.783+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T19:23:29.794+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:23:29.794+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T19:23:29.804+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.767 seconds
[2024-11-25T19:24:00.230+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:24:00.229+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2605)
[2024-11-25T19:24:00.231+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:24:00.231+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2605
[2024-11-25T19:24:00.250+0000] {processor.py:153} INFO - Started process (PID=2605) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:24:00.251+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T19:24:00.252+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:24:00.252+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:24:00.254+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:24:00.254+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:24:00.771+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:24:00.770+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T19:24:00.772+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:24:00.771+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T19:24:00.882+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T19:24:00.884+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:24:00.884+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T19:24:00.887+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:24:00.887+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T19:24:00.888+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:24:00.893+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:24:00.893+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T19:24:00.893+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:24:00.893+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T19:24:00.905+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:24:00.905+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T19:24:00.911+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:24:00.911+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T19:24:00.911+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:24:00.911+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T19:24:00.921+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:24:00.920+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T19:24:00.928+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.699 seconds
[2024-11-25T19:24:31.511+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:24:31.510+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2611)
[2024-11-25T19:24:31.512+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:24:31.512+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2611
[2024-11-25T19:24:31.519+0000] {processor.py:153} INFO - Started process (PID=2611) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:24:31.519+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T19:24:31.521+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:24:31.521+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:24:31.522+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:24:31.522+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:24:33.437+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:24:33.437+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T19:24:33.438+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:24:33.438+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T19:24:33.583+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T19:24:33.585+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:24:33.585+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T19:24:33.592+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:24:33.592+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T19:24:33.592+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:24:33.600+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:24:33.600+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T19:24:33.600+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:24:33.600+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T19:24:33.612+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:24:33.612+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T19:24:33.621+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:24:33.621+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T19:24:33.622+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:24:33.622+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T19:24:33.645+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:24:33.645+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T19:24:33.656+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 2.153 seconds
[2024-11-25T19:25:03.855+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:25:03.855+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2617)
[2024-11-25T19:25:03.856+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:25:03.856+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2617
[2024-11-25T19:25:03.871+0000] {processor.py:153} INFO - Started process (PID=2617) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:25:03.871+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T19:25:03.873+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:25:03.873+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:25:03.874+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:25:03.874+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:25:04.961+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:25:04.961+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T19:25:04.962+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:25:04.961+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T19:25:05.103+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T19:25:05.105+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:25:05.105+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T19:25:05.110+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:25:05.109+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T19:25:05.110+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:25:05.117+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:25:05.117+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T19:25:05.117+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:25:05.117+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T19:25:05.132+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:25:05.132+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T19:25:05.139+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:25:05.139+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T19:25:05.140+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:25:05.139+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T19:25:05.161+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:25:05.161+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T19:25:05.170+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 1.316 seconds
[2024-11-25T19:25:35.749+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:25:35.748+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2623)
[2024-11-25T19:25:35.749+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:25:35.749+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2623
[2024-11-25T19:25:35.764+0000] {processor.py:153} INFO - Started process (PID=2623) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:25:35.765+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T19:25:35.767+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:25:35.767+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:25:35.770+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:25:35.770+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:25:36.684+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:25:36.684+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T19:25:36.688+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:25:36.686+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T19:25:36.909+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T19:25:36.912+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:25:36.912+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T19:25:36.916+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:25:36.916+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T19:25:36.917+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:25:36.925+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:25:36.925+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T19:25:36.926+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:25:36.926+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T19:25:36.970+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:25:36.970+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T19:25:36.981+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:25:36.981+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T19:25:36.982+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:25:36.982+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T19:25:37.002+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:25:37.002+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T19:25:37.013+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 1.265 seconds
[2024-11-25T19:26:07.395+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:26:07.394+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2629)
[2024-11-25T19:26:07.397+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:26:07.397+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2629
[2024-11-25T19:26:07.411+0000] {processor.py:153} INFO - Started process (PID=2629) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:26:07.412+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T19:26:07.413+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:26:07.413+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:26:07.414+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:26:07.414+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:26:07.956+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:26:07.956+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T19:26:07.957+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:26:07.956+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T19:26:08.058+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T19:26:08.059+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:26:08.059+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T19:26:08.063+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:26:08.063+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T19:26:08.063+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:26:08.069+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:26:08.069+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T19:26:08.069+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:26:08.069+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T19:26:08.082+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:26:08.082+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T19:26:08.089+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:26:08.089+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T19:26:08.089+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:26:08.089+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T19:26:08.106+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:26:08.105+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T19:26:08.114+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.720 seconds
[2024-11-25T19:26:38.612+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:26:38.612+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2635)
[2024-11-25T19:26:38.613+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:26:38.613+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2635
[2024-11-25T19:26:38.618+0000] {processor.py:153} INFO - Started process (PID=2635) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:26:38.619+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T19:26:38.620+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:26:38.620+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:26:38.621+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:26:38.621+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:26:39.156+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:26:39.156+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T19:26:39.157+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:26:39.157+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T19:26:39.293+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T19:26:39.296+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:26:39.296+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T19:26:39.308+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:26:39.308+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T19:26:39.309+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:26:39.316+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:26:39.316+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T19:26:39.319+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:26:39.319+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T19:26:39.333+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:26:39.333+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T19:26:39.339+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:26:39.339+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T19:26:39.339+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:26:39.339+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T19:26:39.349+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:26:39.349+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T19:26:39.357+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.745 seconds
[2024-11-25T19:27:10.025+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:27:10.025+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2641)
[2024-11-25T19:27:10.026+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:27:10.026+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2641
[2024-11-25T19:27:10.034+0000] {processor.py:153} INFO - Started process (PID=2641) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:27:10.035+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T19:27:10.036+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:27:10.036+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:27:10.038+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:27:10.038+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:27:10.536+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:27:10.535+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T19:27:10.536+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:27:10.536+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T19:27:10.648+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T19:27:10.650+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:27:10.650+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T19:27:10.654+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:27:10.654+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T19:27:10.655+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:27:10.660+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:27:10.660+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T19:27:10.660+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:27:10.660+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T19:27:10.674+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:27:10.674+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T19:27:10.679+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:27:10.679+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T19:27:10.680+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:27:10.679+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T19:27:10.688+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:27:10.688+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T19:27:10.696+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.671 seconds
[2024-11-25T19:27:41.128+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:27:41.127+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2647)
[2024-11-25T19:27:41.129+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:27:41.129+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2647
[2024-11-25T19:27:41.139+0000] {processor.py:153} INFO - Started process (PID=2647) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:27:41.139+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T19:27:41.141+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:27:41.140+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:27:41.142+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:27:41.142+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:27:41.802+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:27:41.800+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T19:27:41.806+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:27:41.804+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T19:27:41.961+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T19:27:41.964+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:27:41.964+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T19:27:41.969+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:27:41.969+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T19:27:41.970+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:27:41.977+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:27:41.976+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T19:27:41.977+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:27:41.977+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T19:27:41.988+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:27:41.988+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T19:27:41.997+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:27:41.997+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T19:27:41.997+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:27:41.997+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T19:27:42.029+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:27:42.029+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T19:27:42.038+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.911 seconds
[2024-11-25T19:28:13.050+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:28:13.048+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2653)
[2024-11-25T19:28:13.053+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:28:13.053+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2653
[2024-11-25T19:28:13.063+0000] {processor.py:153} INFO - Started process (PID=2653) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:28:13.065+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T19:28:13.068+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:28:13.068+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:28:13.072+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:28:13.072+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:28:15.666+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:28:15.664+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T19:28:15.673+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:28:15.668+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T19:28:16.731+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T19:28:16.788+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:28:16.788+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T19:28:16.830+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:28:16.829+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T19:28:16.833+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:28:17.104+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:28:17.060+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T19:28:17.112+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:28:17.108+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T19:28:17.301+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:28:17.300+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T19:28:17.452+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:28:17.451+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T19:28:17.457+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:28:17.454+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T19:28:17.616+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:28:17.615+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T19:28:17.760+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 4.712 seconds
[2024-11-25T19:28:49.144+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:28:49.139+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2659)
[2024-11-25T19:28:49.167+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:28:49.167+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2659
[2024-11-25T19:28:49.262+0000] {processor.py:153} INFO - Started process (PID=2659) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:28:49.268+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T19:28:49.279+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:28:49.279+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:28:49.298+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:28:49.298+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:28:52.837+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:28:52.835+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T19:28:52.845+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:28:52.839+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T19:28:54.045+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T19:28:54.070+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:28:54.070+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T19:28:54.121+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:28:54.119+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T19:28:54.123+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:28:54.167+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:28:54.166+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T19:28:54.168+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:28:54.167+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T19:28:54.328+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:28:54.327+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T19:28:54.378+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:28:54.377+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T19:28:54.382+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:28:54.379+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T19:28:54.474+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:28:54.474+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T19:28:54.565+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 5.427 seconds
[2024-11-25T19:29:25.264+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:29:25.263+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2665)
[2024-11-25T19:29:25.266+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:29:25.266+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2665
[2024-11-25T19:29:25.290+0000] {processor.py:153} INFO - Started process (PID=2665) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:29:25.290+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T19:29:25.292+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:29:25.292+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:29:25.298+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:29:25.298+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:29:26.917+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:29:26.917+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T19:29:26.918+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:29:26.918+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T19:29:27.187+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T19:29:27.190+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:29:27.190+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T19:29:27.198+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:29:27.198+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T19:29:27.198+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:29:27.209+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:29:27.209+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T19:29:27.209+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:29:27.209+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T19:29:27.244+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:29:27.243+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T19:29:27.261+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:29:27.261+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T19:29:27.262+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:29:27.261+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T19:29:27.285+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:29:27.285+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T19:29:27.296+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 2.034 seconds
[2024-11-25T19:29:58.618+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:29:58.617+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2671)
[2024-11-25T19:29:58.620+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:29:58.620+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2671
[2024-11-25T19:29:58.630+0000] {processor.py:153} INFO - Started process (PID=2671) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:29:58.632+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T19:29:58.633+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:29:58.633+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:29:58.639+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:29:58.638+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:30:00.374+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:30:00.365+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T19:30:00.383+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:30:00.379+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T19:30:00.756+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T19:30:00.761+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:30:00.761+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T19:30:00.769+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:30:00.769+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T19:30:00.770+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:30:00.798+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:30:00.798+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T19:30:00.799+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:30:00.799+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T19:30:00.828+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:30:00.828+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T19:30:00.843+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:30:00.843+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T19:30:00.844+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:30:00.843+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T19:30:00.877+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:30:00.877+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T19:30:00.891+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 2.274 seconds
[2024-11-25T19:30:31.664+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:30:31.659+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2677)
[2024-11-25T19:30:31.670+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:30:31.670+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2677
[2024-11-25T19:30:31.705+0000] {processor.py:153} INFO - Started process (PID=2677) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:30:31.709+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T19:30:31.712+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:30:31.712+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:30:31.719+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:30:31.718+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:30:32.683+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:30:32.683+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T19:30:32.685+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:30:32.684+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T19:30:33.031+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T19:30:33.037+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:30:33.037+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T19:30:33.058+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:30:33.053+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T19:30:33.063+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:30:33.081+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:30:33.080+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T19:30:33.081+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:30:33.081+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T19:30:33.131+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:30:33.130+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T19:30:33.156+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:30:33.156+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T19:30:33.157+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:30:33.157+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T19:30:33.180+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:30:33.180+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T19:30:33.192+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 1.536 seconds
[2024-11-25T19:31:03.273+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:31:03.272+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2683)
[2024-11-25T19:31:03.274+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:31:03.274+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2683
[2024-11-25T19:31:03.282+0000] {processor.py:153} INFO - Started process (PID=2683) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:31:03.283+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T19:31:03.284+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:31:03.284+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:31:03.287+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:31:03.287+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:31:03.939+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:31:03.939+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T19:31:03.940+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:31:03.939+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T19:31:04.075+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T19:31:04.077+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:31:04.077+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T19:31:04.083+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:31:04.083+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T19:31:04.083+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:31:04.091+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:31:04.091+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T19:31:04.091+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:31:04.091+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T19:31:04.109+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:31:04.109+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T19:31:04.129+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:31:04.129+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T19:31:04.130+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:31:04.129+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T19:31:04.165+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:31:04.164+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T19:31:04.189+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.916 seconds
[2024-11-25T19:31:34.562+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:31:34.561+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2689)
[2024-11-25T19:31:34.564+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:31:34.564+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2689
[2024-11-25T19:31:34.572+0000] {processor.py:153} INFO - Started process (PID=2689) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:31:34.573+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T19:31:34.574+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:31:34.574+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:31:34.575+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:31:34.575+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:31:35.446+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:31:35.446+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T19:31:35.447+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:31:35.447+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T19:31:35.631+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T19:31:35.634+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:31:35.634+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T19:31:35.640+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:31:35.639+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T19:31:35.640+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:31:35.649+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:31:35.649+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T19:31:35.650+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:31:35.650+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T19:31:35.932+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:31:35.931+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T19:31:35.960+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:31:35.960+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T19:31:35.962+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:31:35.961+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T19:31:36.002+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:31:36.001+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T19:31:36.018+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 1.457 seconds
[2024-11-25T19:32:06.390+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:32:06.390+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2695)
[2024-11-25T19:32:06.392+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:32:06.391+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2695
[2024-11-25T19:32:06.398+0000] {processor.py:153} INFO - Started process (PID=2695) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:32:06.399+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T19:32:06.400+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:32:06.400+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:32:06.401+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:32:06.401+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:32:07.106+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:32:07.105+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T19:32:07.107+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:32:07.106+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T19:32:07.257+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T19:32:07.259+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:32:07.259+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T19:32:07.264+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:32:07.264+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T19:32:07.265+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:32:07.272+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:32:07.271+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T19:32:07.272+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:32:07.272+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T19:32:07.282+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:32:07.282+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T19:32:07.289+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:32:07.289+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T19:32:07.291+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:32:07.290+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T19:32:07.303+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:32:07.303+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T19:32:07.311+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.922 seconds
[2024-11-25T19:32:37.759+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:32:37.758+0000] {settings.py:266} DEBUG - Setting up DB connection pool (PID 2701)
[2024-11-25T19:32:37.760+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:32:37.760+0000] {settings.py:371} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2701
[2024-11-25T19:32:37.770+0000] {processor.py:153} INFO - Started process (PID=2701) to work on /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:32:37.771+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/kafka_to_snowflake_etl_dag.py for tasks to queue
[2024-11-25T19:32:37.775+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:32:37.775+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:32:37.781+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:32:37.780+0000] {dagbag.py:323} DEBUG - Importing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:32:38.536+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:32:38.535+0000] {ssl_wrap_socket.py:44} DEBUG - Injecting ssl_wrap_socket_with_ocsp
[2024-11-25T19:32:38.536+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:32:38.536+0000] {_auth.py:91} DEBUG - cache directory: /home/airflow/.cache/snowflake
[2024-11-25T19:32:38.663+0000] {logging_mixin.py:137} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py:3492 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
[2024-11-25T19:32:38.665+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:32:38.665+0000] {kafka_to_snowflake_etl_dag.py:306} ERROR - Error occurred while fetching parquet files: [Errno 2] No such file or directory: '/opt/airflow/shared/all_data.parquet'
[2024-11-25T19:32:38.670+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:32:38.669+0000] {dagbag.py:503} DEBUG - Loaded DAG <DAG: kafka_to_snowflake_pipeline>
[2024-11-25T19:32:38.670+0000] {processor.py:753} INFO - DAG(s) dict_keys(['kafka_to_snowflake_pipeline']) retrieved from /opt/airflow/dags/kafka_to_snowflake_etl_dag.py
[2024-11-25T19:32:38.677+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:32:38.677+0000] {dagbag.py:648} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-25T19:32:38.677+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:32:38.677+0000] {dagbag.py:653} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-25T19:32:38.690+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:32:38.690+0000] {serialized_dag.py:156} DEBUG - Checking if DAG (kafka_to_snowflake_pipeline) changed
[2024-11-25T19:32:38.701+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:32:38.701+0000] {serialized_dag.py:167} DEBUG - Serialized DAG (kafka_to_snowflake_pipeline) is unchanged. Skipping writing to DB
[2024-11-25T19:32:38.702+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:32:38.702+0000] {dag.py:2676} INFO - Sync 1 DAGs
[2024-11-25T19:32:38.716+0000] {logging_mixin.py:137} INFO - [2024-11-25T19:32:38.716+0000] {dag.py:3423} INFO - Setting next_dagrun for kafka_to_snowflake_pipeline to 2024-11-25T15:54:42.893219+00:00, run_after=2024-11-26T15:54:42.893219+00:00
[2024-11-25T19:32:38.736+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/kafka_to_snowflake_etl_dag.py took 0.978 seconds
