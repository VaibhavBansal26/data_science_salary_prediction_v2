[2024-11-25T20:41:15.175+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: kafka_to_snowflake_pipeline.upload_all_data_to_stage manual__2024-11-25T20:35:16.091167+00:00 [queued]>
[2024-11-25T20:41:15.186+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: kafka_to_snowflake_pipeline.upload_all_data_to_stage manual__2024-11-25T20:35:16.091167+00:00 [queued]>
[2024-11-25T20:41:15.187+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2024-11-25T20:41:15.187+0000] {taskinstance.py:1284} INFO - Starting attempt 2 of 2
[2024-11-25T20:41:15.187+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2024-11-25T20:41:15.213+0000] {taskinstance.py:1304} INFO - Executing <Task(SnowflakeOperator): upload_all_data_to_stage> on 2024-11-25 20:35:16.091167+00:00
[2024-11-25T20:41:15.228+0000] {standard_task_runner.py:55} INFO - Started process 835 to run task
[2024-11-25T20:41:15.234+0000] {standard_task_runner.py:82} INFO - Running: ['airflow', 'tasks', 'run', 'kafka_to_snowflake_pipeline', 'upload_all_data_to_stage', 'manual__2024-11-25T20:35:16.091167+00:00', '--job-id', '39', '--raw', '--subdir', 'DAGS_FOLDER/kafka_to_snowflake_etl_dag.py', '--cfg-path', '/tmp/tmpg19s0f82']
[2024-11-25T20:41:15.242+0000] {standard_task_runner.py:83} INFO - Job 39: Subtask upload_all_data_to_stage
[2024-11-25T20:41:16.031+0000] {task_command.py:389} INFO - Running <TaskInstance: kafka_to_snowflake_pipeline.upload_all_data_to_stage manual__2024-11-25T20:35:16.091167+00:00 [running]> on host f25f59e56f04
[2024-11-25T20:41:16.197+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=Vaibhav Bansal
AIRFLOW_CTX_DAG_ID=kafka_to_snowflake_pipeline
AIRFLOW_CTX_TASK_ID=upload_all_data_to_stage
AIRFLOW_CTX_EXECUTION_DATE=2024-11-25T20:35:16.091167+00:00
AIRFLOW_CTX_TRY_NUMBER=2
AIRFLOW_CTX_DAG_RUN_ID=manual__2024-11-25T20:35:16.091167+00:00
[2024-11-25T20:41:16.200+0000] {sql.py:253} INFO - Executing:  
        PUT file:///opt/airflow/shared/all_data.parquet @~/stage/ AUTO_COMPRESS=FALSE;
        
[2024-11-25T20:41:16.213+0000] {base.py:73} INFO - Using connection ID 'snowflake_default' for task execution.
[2024-11-25T20:41:16.598+0000] {base.py:73} INFO - Using connection ID 'snowflake_default' for task execution.
[2024-11-25T20:41:16.626+0000] {connection.py:414} INFO - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.8.15, Platform: Linux-6.6.22-linuxkit-aarch64-with-glibc2.17
[2024-11-25T20:41:16.636+0000] {connection.py:1197} INFO - Connecting to GLOBAL Snowflake domain
[2024-11-25T20:41:16.637+0000] {connection.py:1278} INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
[2024-11-25T20:41:16.637+0000] {connection.py:1288} INFO - THIS CONNECTION IS IN INSECURE MODE. IT MEANS THE CERTIFICATE WILL BE VALIDATED BUT THE CERTIFICATE REVOCATION STATUS WILL NOT BE CHECKED.
[2024-11-25T20:41:17.111+0000] {ssl_wrap_socket.py:101} INFO - THIS CONNECTION IS IN INSECURE MODE. IT MEANS THE CERTIFICATE WILL BE VALIDATED BUT THE CERTIFICATE REVOCATION STATUS WILL NOT BE CHECKED.
[2024-11-25T20:41:17.573+0000] {cursor.py:1166} INFO - Number of results in first chunk: 1
[2024-11-25T20:41:17.576+0000] {sql.py:364} INFO - Running statement: PUT file:///opt/airflow/shared/all_data.parquet @~/stage/ AUTO_COMPRESS=FALSE, parameters: None
[2024-11-25T20:41:17.824+0000] {connection.py:789} INFO - closed
[2024-11-25T20:41:17.910+0000] {connection.py:795} INFO - No async queries seem to be running, deleting session
[2024-11-25T20:41:18.001+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/common/sql/operators/sql.py", line 255, in execute
    output = hook.run(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/snowflake/hooks/snowflake.py", line 388, in run
    self._run_command(cur, sql_statement, parameters)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/common/sql/hooks/sql.py", line 369, in _run_command
    cur.execute(sql_statement)
  File "/home/airflow/.local/lib/python3.8/site-packages/snowflake/connector/cursor.py", line 1058, in execute
    sf_file_transfer_agent.execute()
  File "/home/airflow/.local/lib/python3.8/site-packages/snowflake/connector/file_transfer_agent.py", line 390, in execute
    self._init_file_metadata()
  File "/home/airflow/.local/lib/python3.8/site-packages/snowflake/connector/file_transfer_agent.py", line 986, in _init_file_metadata
    Error.errorhandler_wrapper(
  File "/home/airflow/.local/lib/python3.8/site-packages/snowflake/connector/errors.py", line 284, in errorhandler_wrapper
    handed_over = Error.hand_to_other_handler(
  File "/home/airflow/.local/lib/python3.8/site-packages/snowflake/connector/errors.py", line 339, in hand_to_other_handler
    cursor.errorhandler(connection, cursor, error_class, error_value)
  File "/home/airflow/.local/lib/python3.8/site-packages/snowflake/connector/errors.py", line 215, in default_errorhandler
    raise error_class(
snowflake.connector.errors.ProgrammingError: 253006: 253006: File doesn't exist: ['/opt/airflow/shared/all_data.parquet']
[2024-11-25T20:41:18.056+0000] {taskinstance.py:1322} INFO - Marking task as FAILED. dag_id=kafka_to_snowflake_pipeline, task_id=upload_all_data_to_stage, execution_date=20241125T203516, start_date=20241125T204115, end_date=20241125T204118
[2024-11-25T20:41:18.137+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 39 for task upload_all_data_to_stage (253006: 253006: File doesn't exist: ['/opt/airflow/shared/all_data.parquet']; 835)
[2024-11-25T20:41:18.206+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2024-11-25T20:41:18.371+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
